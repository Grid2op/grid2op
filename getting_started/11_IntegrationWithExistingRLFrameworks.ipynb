{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid2op integration with existing frameworks\n",
    "\n",
    "Try me out interactively with: [![Binder](./img/badge_logo.svg)](https://mybinder.org/v2/gh/rte-france/Grid2Op/master)\n",
    "\n",
    "\n",
    "**objectives** This notebooks briefly explains how to use grid2op with commonly used RL frameworks.\n",
    "\n",
    "The structure is always very similar:\n",
    "1. Create a grid2op environment\n",
    "2. Convert it to a gym environment\n",
    "3. (optional) Customize the action space and observation space\n",
    "4. Use the framework to train an agent\n",
    "\n",
    "In this notebook, we will demonstrate its usage with 3 different framework. The code provided here are given as examples and we do not assume anything on their performance or fitness of use. More detailed example will be provided in the l2rpn-baselines repository in due time (work in progress at the time of writing this notebook). The 3 framework we will demonstrate an example of are:\n",
    "\n",
    "- ray (rllib): see [ray on github](https://github.com/ray-project/ray) or [rllib on github](https://github.com/ray-project/ray/blob/master/doc/source/rllib.rst)\n",
    "- stable-baselines3: see [stable-baselines3 on github](https://github.com/DLR-RM/stable-baselines3)\n",
    "- tf_agents: see [tf_agents on github](https://github.com/tensorflow/agents)\n",
    "\n",
    "Other RL frameworks are not cover here. If you already use them, let us know !\n",
    "- https://github.com/wau/keras-rl2\n",
    "- https://github.com/deepmind/acme\n",
    "\n",
    "\n",
    "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" width=\"200\">\n",
    "Execute the cell below by removing the `#` characters if you use google colab !\n",
    "\n",
    "Cell will look like:\n",
    "```python\n",
    "import sys\n",
    "!$sys.executable install grid2op[optional]  # for use with google colab (grid2Op is not installed by default)\n",
    "!$sys.executable install tensorflow pytorch stable-baselines3 'ray[rllib]' tf_agents\n",
    "```\n",
    "\n",
    "It might take a while\n",
    "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" width=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# !$sys.executable install grid2op[optional]  # for use with google colab (grid2Op is not installed by default)\n",
    "# !$sys.executable -m pip install stable-baselines3 'ray[rllib]' tf_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because this notebook is part of some tests, we train the agent for only a small number of steps\n",
    "nb_step_train = 0  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) Recommended initial steps\n",
    "\n",
    "### Create a grid2op environment\n",
    "\n",
    "This is a rather standard step, with lots of inspiration drawn from openAI gym framework, and there is absolutely no specificity here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benjamin/Documents/grid2op_dev/getting_started/grid2op/MakeEnv/Make.py:305: UserWarning: You are using a development environment. This environment is not intended for training agents. It might not be up to date and its primary use if for tests (hence the \"test=True\" you passed as argument). Use at your own risk.\n",
      "  warnings.warn(_MAKE_DEV_ENV_WARN)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<grid2op.Space.GridObjects.CompleteObservation_educ_case14_storage at 0x7f267cdac8b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import grid2op\n",
    "env_name = \"l2rpn_case14_sandbox\"\n",
    "env_name = \"educ_case14_storage\"\n",
    "env_glop = grid2op.make(env_name, test=True)  # NOTE: do not set the flag \"test=True\" for a real usage !\n",
    "# This flag is here for testing purpose !!!\n",
    "obs_glop = env_glop.reset()\n",
    "obs_glop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert it to a gym environment\n",
    "\n",
    "To that end, we recommend using the \"gym_compat\" module. More information is given in the [official grid2op documentation](https://grid2op.readthedocs.io/en/latest/gym.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The \"env_gym\" is a gym environment: True\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from grid2op.gym_compat import GymEnv\n",
    "env_gym = GymEnv(env_glop)\n",
    "print(f\"The \\\"env_gym\\\" is a gym environment: {isinstance(env_gym, gym.Env)}\")\n",
    "obs_gym = env_gym.reset()\n",
    "# obs_gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customize the action space and observation space\n",
    "\n",
    "This step is optional, but highly recommended.\n",
    "\n",
    "By default, grid2op actions and observations are huge. Even for this very simplistic example, you have really important sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the action space is : 28\n",
      "The size of the observation space is : 440\n"
     ]
    }
   ],
   "source": [
    "dim_act_space = np.sum([np.sum(env_gym.action_space[el].shape) for el in env_gym.action_space.spaces])\n",
    "print(f\"The size of the action space is : \"\n",
    "      f\"{dim_act_space}\")\n",
    "dim_obs_space = np.sum([np.sum(env_gym.observation_space[el].shape).astype(int) \n",
    "                        for el in env_gym.observation_space.spaces])\n",
    "print(f\"The size of the observation space is : \"\n",
    "      f\"{dim_obs_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Action space\n",
    "This is partly due because in grid2op, you can represent the same concept (*eg* reconnect a powerline) in different manners (in this case: either you \"toggle a switch\" - if the said powerline was connected, it will disconnect it, otherwise it will reconnect it- or you can say \"i want this line connected whatever its original state\"). This behaviour is detailed in the [official grid2op documentation](https://grid2op.readthedocs.io/en/latest/action.html#usage-examples).\n",
    "\n",
    "To (in general) reduce the action space by a factor of 2, you can represent these actions only using the change method (for example). You can do that with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new size of the action space is : 28\n"
     ]
    }
   ],
   "source": [
    "# example: ignore the \"set_status\" and \"set_bus\" type of actions, that are covered by the \"change_status\" and\n",
    "# \"change_bus\"\n",
    "\n",
    "env_gym.action_space = env_gym.action_space.ignore_attr(\"set_bus\").ignore_attr(\"set_line_status\")\n",
    "\n",
    "new_dim_act_space = np.sum([np.sum(env_gym.action_space[el].shape) for el in env_gym.action_space.spaces])\n",
    "print(f\"The new size of the action space is : {new_dim_act_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid2op environments allow for both continuous and discrete action. For the sake of the example, let's \"convert\" the continuous actions in discrete ones (this is done with \"binning\" the values as explained in more details [in the documentation](https://grid2op.readthedocs.io/en/latest/gym.html#grid2op.gym_compat.ContinuousToDiscreteConverter) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: convert the continuous action type \"redispatch\" to a discrete action type\n",
    "from grid2op.gym_compat import ContinuousToDiscreteConverter\n",
    "env_gym.action_space = env_gym.action_space.reencode_space(\"redispatch\",\n",
    "                                                           ContinuousToDiscreteConverter(nb_bins=11)\n",
    "                                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict(change_line_status:MultiBinary(20), redispatch:MultiDiscrete([11 11  1  1  1 11]), storage_power:Box(-inf, inf, (2,), float32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And now our action space looks like:\n",
    "env_gym.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation space\n",
    "\n",
    "For the obsevation space, we will remove lots of useless attributes (remember, it is for the sake of the example here, and rescale some other so that they have numbers between rougly 0. and 1., which stabilizes the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict(a_ex:Box(0.0, inf, (20,), float32), a_or:Box(0.0, inf, (20,), float32), actual_dispatch:Box(-140.0, 140.0, (6,), float32), curtailment:Box(0.0, 1.0, (6,), float32), curtailment_limit:Box(0.0, 1.0, (6,), float32), day:Discrete(32), day_of_week:Discrete(8), duration_next_maintenance:Box(-1, 2147483647, (20,), int32), gen_p:Box(0.0, 168.0, (6,), float32), gen_p_before_curtail:Box(0.0, 168.0, (6,), float32), gen_q:Box(-inf, inf, (6,), float32), gen_v:Box(0.0, inf, (6,), float32), hour_of_day:Discrete(24), line_status:MultiBinary(20), load_p:Box(-inf, inf, (11,), float32), load_q:Box(-inf, inf, (11,), float32), load_v:Box(0.0, inf, (11,), float32), minute_of_hour:Discrete(60), month:Discrete(13), p_ex:Box(-inf, inf, (20,), float32), p_or:Box(-inf, inf, (20,), float32), q_ex:Box(-inf, inf, (20,), float32), q_or:Box(-inf, inf, (20,), float32), rho:Box(0.0, inf, (20,), float32), storage_charge:Box(0.0, 15.0, (2,), float32), storage_power:Box(-10.0, 10.0, (2,), float32), storage_power_target:Box(-10.0, 10.0, (2,), float32), target_dispatch:Box(-140.0, 140.0, (6,), float32), time_before_cooldown_line:Box(0, 99999, (20,), int32), time_before_cooldown_sub:Box(0, 3, (14,), int32), time_next_maintenance:Box(-1, 2147483647, (20,), int32), timestep_overflow:Box(-2147483648, 2147483647, (20,), int32), topo_vect:Box(-1, 2, (59,), int32), v_ex:Box(0.0, inf, (20,), float32), v_or:Box(0.0, inf, (20,), float32), year:Discrete(2100))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first let's see which are the attributes in the observation space:\n",
    "# More information on\n",
    "# https://beta-grid2op.readthedocs.io/en/latest/observation.html#main-observation-attributes\n",
    "# and \n",
    "# https://grid2op.readthedocs.io/en/latest/gym.html#observation-space-and-action-space-customization\n",
    "env_gym.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep only the information about the flow on the powerlines: `rho`, the generation `gen_p`, the load `load_p` and the representation of the topology `topo_vect` (for the sake of the example, once again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new size of the observation space is : 102 (it was 440 before!)\n"
     ]
    }
   ],
   "source": [
    "env_gym.observation_space = env_gym.observation_space.keep_only_attr([\"rho\", \"gen_p\", \"load_p\", \"topo_vect\", \n",
    "                                                                      \"actual_dispatch\"])\n",
    "new_dim_obs_space = np.sum([np.sum(env_gym.observation_space[el].shape).astype(int) \n",
    "                        for el in env_gym.observation_space.spaces])\n",
    "print(f\"The new size of the observation space is : \"\n",
    "      f\"{new_dim_obs_space} (it was {dim_obs_space} before!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One other detail here, the generation and loads are not scaled (they are given in MW). We recommend to scale them to have number roughly between 0 and 1 for stability during learning.\n",
    "\n",
    "This can be done pretty easily with the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'obs_gym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-ed1037d79db4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m                                    )\n\u001b[1;32m     13\u001b[0m ob_space = ob_space.reencode_space(\"load_p\",\n\u001b[0;32m---> 14\u001b[0;31m                                   ScalerAttrConverter(substract=obs_gym[\"load_p\"],\n\u001b[0m\u001b[1;32m     15\u001b[0m                                                       \u001b[0mdivide\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mobs_gym\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"load_p\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                                                       )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'obs_gym' is not defined"
     ]
    }
   ],
   "source": [
    "from grid2op.gym_compat import ScalerAttrConverter\n",
    "ob_space = env_gym.observation_space\n",
    "ob_space = ob_space.reencode_space(\"actual_dispatch\",\n",
    "                                   ScalerAttrConverter(substract=0.,\n",
    "                                                       divide=env_glop.gen_pmax\n",
    "                                                       )\n",
    "                                   )\n",
    "ob_space = ob_space.reencode_space(\"gen_p\",\n",
    "                                   ScalerAttrConverter(substract=0.,\n",
    "                                                       divide=env_glop.gen_pmax\n",
    "                                                       )\n",
    "                                   )\n",
    "ob_space = ob_space.reencode_space(\"load_p\",\n",
    "                                  ScalerAttrConverter(substract=obs_gym[\"load_p\"],\n",
    "                                                      divide=0.5 * obs_gym[\"load_p\"]\n",
    "                                                      )\n",
    "                                  )\n",
    "\n",
    "env_gym.observation_space = ob_space\n",
    "env_gym.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) RLLIB\n",
    "\n",
    "This part is not a tutorial on how to use rllib. Please refer to [their documentation](https://docs.ray.io/en/master/rllib.html) for more detailed information.\n",
    "\n",
    "As explained in the header of this notebook, we will follow the recommended usage:\n",
    "1. Create a grid2op environment (see section [0) Recommended initial steps](#0\\)-Recommended-initial-steps))\n",
    "2. Convert it to a gym environment (see section [0) Recommended initial steps](#0\\)-Recommended-initial-steps))\n",
    "3. (optional) Customize the action space and observation space (see section [0) Recommended initial steps](#0\\)-Recommended-initial-steps))\n",
    "4. Use the framework to train an agent  **(only this part is framework specific)**\n",
    "\n",
    "\n",
    "The issue with rllib is that it does not take into account MultiBinary nor MultiDiscrete action space (see \n",
    "see https://github.com/ray-project/ray/issues/1519) so we need some way to encode these types of actions. This can be done automatically with the `MultiToTupleConverter` provided in grid2op (as always, more information [in the documentation](https://grid2op.readthedocs.io/en/latest/gym.html#grid2op.gym_compat.MultiToTupleConverter) ).\n",
    "\n",
    "We will then use this to customize our environment previously defined:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "env_rllib = copy.deepcopy(env_gym)\n",
    "from grid2op.gym_compat import MultiToTupleConverter\n",
    "env_rllib.action_space = env_rllib.action_space.reencode_space(\"change_bus\", MultiToTupleConverter())\n",
    "env_rllib.action_space = env_rllib.action_space.reencode_space(\"change_line_status\", MultiToTupleConverter())\n",
    "env_rllib.action_space = env_rllib.action_space.reencode_space(\"redispatch\", MultiToTupleConverter())\n",
    "env_rllib.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another specificity of RLLIB is that it handles creation of environments \"on its own\". This implies that you need to create a custom class representing an environment, rather a python object.\n",
    "\n",
    "And finally, you ask it to use this class, and learn a specific agent. This is really well explained in their documentation: https://docs.ray.io/en/master/rllib-env.html#configuring-environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gym specific, we simply do a copy paste of what we did in the previous cells, wrapping it in the\n",
    "# MyEnv class, and train a Proximal Policy Optimisation based agent\n",
    "import gym\n",
    "import ray\n",
    "from ray.rllib.agents import ppo\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "      \n",
    "class MyEnv(gym.Env):\n",
    "    def __init__(self, env_config):\n",
    "        import grid2op\n",
    "        from grid2op.gym_compat import GymEnv\n",
    "        from grid2op.gym_compat import ScalerAttrConverter, ContinuousToDiscreteConverter, MultiToTupleConverter\n",
    "\n",
    "        # 1. create the grid2op environment\n",
    "        if not \"env_name\" in env_config:\n",
    "            raise RuntimeError(\"The configuration for RLLIB should provide the env name\")\n",
    "        nm_env = env_config[\"env_name\"]\n",
    "        del env_config[\"env_name\"]\n",
    "        self.env_glop = grid2op.make(nm_env, **env_config)\n",
    "\n",
    "        # 2. create the gym environment\n",
    "        self.env_gym = GymEnv(self.env_glop)\n",
    "        obs_gym = self.env_gym.reset()\n",
    "\n",
    "        # 3. (optional) customize it (see section above for more information)\n",
    "        ## customize action space\n",
    "        self.env_gym.action_space = self.env_gym.action_space.ignore_attr(\"set_bus\").ignore_attr(\"set_line_status\")\n",
    "        self.env_gym.action_space = self.env_gym.action_space.reencode_space(\"redispatch\",\n",
    "                                                                             ContinuousToDiscreteConverter(nb_bins=11)\n",
    "                                                                             )\n",
    "        self.env_gym.action_space = self.env_gym.action_space.reencode_space(\"change_bus\", MultiToTupleConverter())\n",
    "        self.env_gym.action_space = self.env_gym.action_space.reencode_space(\"change_line_status\",\n",
    "                                                                             MultiToTupleConverter())\n",
    "        self.env_gym.action_space = self.env_gym.action_space.reencode_space(\"redispatch\", MultiToTupleConverter())\n",
    "        ## customize observation space\n",
    "        ob_space = self.env_gym.observation_space\n",
    "        ob_space = ob_space.keep_only_attr([\"rho\", \"gen_p\", \"load_p\", \"topo_vect\", \"actual_dispatch\"])\n",
    "        ob_space = ob_space.reencode_space(\"actual_dispatch\",\n",
    "                                           ScalerAttrConverter(substract=0.,\n",
    "                                                               divide=self.env_glop.gen_pmax\n",
    "                                                               )\n",
    "                                           )\n",
    "        ob_space = ob_space.reencode_space(\"gen_p\",\n",
    "                                           ScalerAttrConverter(substract=0.,\n",
    "                                                               divide=self.env_glop.gen_pmax\n",
    "                                                               )\n",
    "                                           )\n",
    "        ob_space = ob_space.reencode_space(\"load_p\",\n",
    "                                           ScalerAttrConverter(substract=obs_gym[\"load_p\"],\n",
    "                                                               divide=0.5 * obs_gym[\"load_p\"]\n",
    "                                                               )\n",
    "                                           )\n",
    "        self.env_gym.observation_space = ob_space\n",
    "\n",
    "        # 4. specific to rllib\n",
    "        self.action_space = self.env_gym.action_space\n",
    "        self.observation_space = self.env_gym.observation_space\n",
    "\n",
    "    def reset(self):\n",
    "        obs = self.env_gym.reset()\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env_gym.step(action)\n",
    "        return obs, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = MyEnv({\"env_name\": \"l2rpn_case14_sandbox\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now you can train it :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fist initialize ray\n",
    "ray.init()\n",
    "try:\n",
    "    # then define a \"trainer\"\n",
    "    trainer = ppo.PPOTrainer(env=MyEnv, config={\n",
    "        \"env_config\": {\"env_name\":\"l2rpn_case14_sandbox\"},  # config to pass to env class\n",
    "    })\n",
    "    # and then train it for a given number of iteration\n",
    "    for step in range(nb_step_train):\n",
    "        trainer.train()\n",
    "finally:   \n",
    "    # shutdown ray\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB** We want to emphasize here that:\n",
    "- This encoding is far from being suitable here. It is shown as an example, mainly to demonstrate the use of some of the gym_compat module\n",
    "- The actions in particular are not really suited here. Actions in grid2op are relatively complex and encoding them this way does not seem like a great idea. For example, with this encoding, the agent will have to learn that it cannot act on more than 2 lines or two substations at the same time...\n",
    "- The \"PPO\" agent shown here, with some default parameters is unlikely to lead to a good agent. You might want to read litterature on past L2RPN agents or draw some inspiration from L2RPN baselines packages for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_gym = env_rllib.action_space.sample()\n",
    "act_glop = env_rllib.action_space.from_gym(act_gym)\n",
    "act_gym2 = env_rllib.action_space.to_gym(act_glop)\n",
    "act_glop2 = env_rllib.action_space.from_gym(act_gym2)\n",
    "for k in act_gym.keys():\n",
    "    assert np.array_equal(act_gym[k], act_gym2[k]), f\"error for {k}\"\n",
    "for k in act_gym2.keys():\n",
    "    assert np.array_equal(act_gym[k], act_gym2[k]), f\"error for {k}\"\n",
    "assert act_glop == act_glop2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_gym2[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Stable baselines\n",
    "\n",
    "This part is not a tutorial on how to use stable baselines. Please refer to [their documentation](https://stable-baselines3.readthedocs.io/en/master/) for more detailed information.\n",
    "\n",
    "As explained in the header of this notebook, we will follow the recommended usage:\n",
    "1. Create a grid2op environment (see section [0) Recommended initial steps](#0\\)-Recommended-initial-steps))\n",
    "2. Convert it to a gym environment (see section [0) Recommended initial steps](#0\\)-Recommended-initial-steps))\n",
    "3. (optional) Customize the action space and observation space (see section [0) Recommended initial steps](#0\\)-Recommended-initial-steps))\n",
    "4. Use the framework to train an agent  **(only this part is framework specific)**\n",
    "\n",
    "\n",
    "The issue with stable beselines 3 is that it expects standard action / observation types as explained there:\n",
    "https://stable-baselines3.readthedocs.io/en/master/guide/algos.html#rl-algorithms\n",
    "\n",
    "> Non-array spaces such as Dict or Tuple are not currently supported by any algorithm.\n",
    "\n",
    "Unfortunately, it's not possible to convert without any \"loss of information\" an action space of dictionnary type to a vector.\n",
    "\n",
    "![](https://blog.planview.com/wp-content/uploads/2020/02/limiting-work-in-progress.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
