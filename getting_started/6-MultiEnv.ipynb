{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent, RL and MultiEnvironment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Disclaimer***: This file referenced some files in other directories. In order to have working cross referencing it's recommended to start the notebook server from the root directory (`Grid2Op`) of the package and __not__ in the `getting_started` sub directory:\n",
    "```bash\n",
    "cd Grid2Op\n",
    "jupyter notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***NB*** For more information about how to use the package, a general help can be built locally (provided that sphinx is installed on the machine) with:\n",
    "```bash\n",
    "cd Grid2Op\n",
    "make html\n",
    "```\n",
    "from the top directory of the package (usually `Grid2Op`).\n",
    "\n",
    "Once build, the help can be access from [here](../documentation/html/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is recommended to have a look at the [0_basic_functionalities](0_basic_functionalities.ipynb), [1_Observation_Agents](1_Observation_Agents.ipynb) and [2_Action_GridManipulation](2_Action_GridManipulation.ipynb) and especially [3_TrainingAnAgent](3_TrainingAnAgent.ipynb) notebooks before getting into this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objectives**\n",
    "\n",
    "In this notebook we will expose :\n",
    "* what is a \"MultiEnv\"\n",
    "* how can it be used with an agent\n",
    "* how can it be used to train a agent that uses different environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"my_id_menu_nb\">run previous cell, wait for 2 seconds</div>\n",
       "<script>\n",
       "function repeat_indent_string(n){\n",
       "    var a = \"\" ;\n",
       "    for ( ; n > 0 ; --n)\n",
       "        a += \"    \";\n",
       "    return a;\n",
       "}\n",
       "// look up into all sections and builds an automated menu //\n",
       "var update_menu_string = function(begin, lfirst, llast, sformat, send, keep_item, begin_format, end_format) {\n",
       "    var anchors = document.getElementsByClassName(\"section\");\n",
       "    if (anchors.length == 0) {\n",
       "        anchors = document.getElementsByClassName(\"text_cell_render rendered_html\");\n",
       "    }\n",
       "    var i,t;\n",
       "    var text_menu = begin;\n",
       "    var text_memo = \"<pre>\\nlength:\" + anchors.length + \"\\n\";\n",
       "    var ind = \"\";\n",
       "    var memo_level = 1;\n",
       "    var href;\n",
       "    var tags = [];\n",
       "    var main_item = 0;\n",
       "    var format_open = 0;\n",
       "    for (i = 0; i <= llast; i++)\n",
       "        tags.push(\"h\" + i);\n",
       "\n",
       "    for (i = 0; i < anchors.length; i++) {\n",
       "        text_memo += \"**\" + anchors[i].id + \"--\\n\";\n",
       "\n",
       "        var child = null;\n",
       "        for(t = 0; t < tags.length; t++) {\n",
       "            var r = anchors[i].getElementsByTagName(tags[t]);\n",
       "            if (r.length > 0) {\n",
       "child = r[0];\n",
       "break;\n",
       "            }\n",
       "        }\n",
       "        if (child == null) {\n",
       "            text_memo += \"null\\n\";\n",
       "            continue;\n",
       "        }\n",
       "        if (anchors[i].hasAttribute(\"id\")) {\n",
       "            // when converted in RST\n",
       "            href = anchors[i].id;\n",
       "            text_memo += \"#1-\" + href;\n",
       "            // passer à child suivant (le chercher)\n",
       "        }\n",
       "        else if (child.hasAttribute(\"id\")) {\n",
       "            // in a notebook\n",
       "            href = child.id;\n",
       "            text_memo += \"#2-\" + href;\n",
       "        }\n",
       "        else {\n",
       "            text_memo += \"#3-\" + \"*\" + \"\\n\";\n",
       "            continue;\n",
       "        }\n",
       "        var title = child.textContent;\n",
       "        var level = parseInt(child.tagName.substring(1,2));\n",
       "\n",
       "        text_memo += \"--\" + level + \"?\" + lfirst + \"--\" + title + \"\\n\";\n",
       "\n",
       "        if ((level < lfirst) || (level > llast)) {\n",
       "            continue ;\n",
       "        }\n",
       "        if (title.endsWith('¶')) {\n",
       "            title = title.substring(0,title.length-1).replace(\"<\", \"&lt;\")\n",
       "         .replace(\">\", \"&gt;\").replace(\"&\", \"&amp;\");\n",
       "        }\n",
       "        if (title.length == 0) {\n",
       "            continue;\n",
       "        }\n",
       "\n",
       "        while (level < memo_level) {\n",
       "            text_menu += end_format + \"</ul>\\n\";\n",
       "            format_open -= 1;\n",
       "            memo_level -= 1;\n",
       "        }\n",
       "        if (level == lfirst) {\n",
       "            main_item += 1;\n",
       "        }\n",
       "        if (keep_item != -1 && main_item != keep_item + 1) {\n",
       "            // alert(main_item + \" - \" + level + \" - \" + keep_item);\n",
       "            continue;\n",
       "        }\n",
       "        while (level > memo_level) {\n",
       "            text_menu += \"<ul>\\n\";\n",
       "            memo_level += 1;\n",
       "        }\n",
       "        text_menu += repeat_indent_string(level-2);\n",
       "        text_menu += begin_format + sformat.replace(\"__HREF__\", href).replace(\"__TITLE__\", title);\n",
       "        format_open += 1;\n",
       "    }\n",
       "    while (1 < memo_level) {\n",
       "        text_menu += end_format + \"</ul>\\n\";\n",
       "        memo_level -= 1;\n",
       "        format_open -= 1;\n",
       "    }\n",
       "    text_menu += send;\n",
       "    //text_menu += \"\\n\" + text_memo;\n",
       "\n",
       "    while (format_open > 0) {\n",
       "        text_menu += end_format;\n",
       "        format_open -= 1;\n",
       "    }\n",
       "    return text_menu;\n",
       "};\n",
       "var update_menu = function() {\n",
       "    var sbegin = \"\";\n",
       "    var sformat = '<a href=\"#__HREF__\">__TITLE__</a>';\n",
       "    var send = \"\";\n",
       "    var begin_format = '<li>';\n",
       "    var end_format = '</li>';\n",
       "    var keep_item = -1;\n",
       "    var text_menu = update_menu_string(sbegin, 2, 4, sformat, send, keep_item,\n",
       "       begin_format, end_format);\n",
       "    var menu = document.getElementById(\"my_id_menu_nb\");\n",
       "    menu.innerHTML=text_menu;\n",
       "};\n",
       "window.setTimeout(update_menu,2000);\n",
       "            </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = None\n",
    "try:\n",
    "    from jyquickhelper import add_notebook_menu\n",
    "    res = add_notebook_menu()\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Impossible to automatically add a menu / table of content to this notebook.\\nYou can download \\\"jyquickhelper\\\" package with: \\n\\\"pip install jyquickhelper\\\"\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/donnotben/.local/lib/python3.6/site-packages/pandapower/io_utils.py:8: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  from pandas.util.testing import assert_series_equal, assert_frame_equal\n"
     ]
    }
   ],
   "source": [
    "import grid2op\n",
    "from grid2op.Reward import ConstantReward, FlatReward\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "TRAINING_STEP = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I) Download more data for the default environment.\n",
    "\n",
    "A lot of data have been made available for the default \"case14_redisp\" environment. Including this data in the package is not convenient. We chose instead to release them and make them easily available with a utility. To download them in the default directory (\"~/data_grid2op/case14_redisp\") on linux based system you can do the following (uncomment the following command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !$sys.executable -m grid2op.download --name \"case14_realistic\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II) Make a regular environment and agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we downloaded the dataset, it is time to make an environment that will use all the data avaiable. You can execute the following command line. If you see any error or warning consider re downloading the data, or adapting the key-word argument \"chronics_path\" to match the path where the data have been downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/donnotben/.local/lib/python3.6/site-packages/grid2op/MakeEnv.py:370: UserWarning: Your are using only 2 chronics for this environment. More can be download by running, from a command line:\n",
      "python -m grid2op.download --name \"case14_realistic\" --path_save PATH\\WHERE\\YOU\\WANT\\TO\\DOWNLOAD\\DATA\n",
      "  warnings.warn(\"Your are using only 2 chronics for this environment. More can be download by running, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot create and instance of chronics_class with parameters \"<class 'grid2op.ChronicsHandler.ChronicsHandler'>\"\n",
      "Please read the above cell, it appears you don't have downloaded the dataset, or save it into an unknown repository. I will continue with only 2 sets.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    env = grid2op.make(name_env=\"case14_realistic\", chronics_path=os.path.expanduser(\"~/data_grid2op/case14_realistic\"))\n",
    "except Exception as exc :\n",
    "    print(\"Please read the above cell, it appears you don't have downloaded the dataset, \"\\\n",
    "          \"or save it into an unknown repository. \" \\\n",
    "          \"I will continue with only 2 sets.\")\n",
    "    env = grid2op.make(name_env=\"case14_realistic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III) Train a standard RL Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you are using a computer with at least 4 cores if you want to notice some speed-ups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grid2op.MultiEnv import MultiEnvironment\n",
    "from grid2op.Agent import DoNothingAgent\n",
    "NUM_CORE = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IIIa) Using the standard open AI gym loop\n",
    "\n",
    "Here we demonstrate how to use the multi environment class. First let's create a multi environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple agent\n",
    "agent = DoNothingAgent(env.action_space)\n",
    "\n",
    "# create the multi environment class\n",
    "multi_envs = MultiEnvironment(env=env, nb_env=NUM_CORE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A multienvironment is just like a regular environment but instead of dealing with one action, and one observation, is requires to be sent multiple actions, and returns a list of observations as well. \n",
    "\n",
    "It requires a grid2op environment to be initialized and creates some specific \"workers\", each a replication of the initial environment. None of the \"worker\" can be accessed directly. Supported methods are:\n",
    "- multi_env.reset\n",
    "- multi_env.step\n",
    "- multi_env.close\n",
    "\n",
    "That have similar behaviour to \"env.step\", \"env.close\" or \"env.reset\".\n",
    "\n",
    "\n",
    "It can be used the following manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<grid2op.Observation.CompleteObservation object at 0x7fa11b145fd0>,\n",
       "       <grid2op.Observation.CompleteObservation object at 0x7fa14bcc7278>,\n",
       "       <grid2op.Observation.CompleteObservation object at 0x7fa14bcc72e8>,\n",
       "       <grid2op.Observation.CompleteObservation object at 0x7fa14bcc7e48>],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initiliaze some variable with the proper dimension\n",
    "obss = multi_envs.reset()\n",
    "rews = [env.reward_range[0] for i in range(NUM_CORE)]\n",
    "dones = [False for i in range(NUM_CORE)]\n",
    "obss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False, False, False]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, obs is not a single obervation, but a list (numpy nd array to be precise) of 4 observations, each one being an observation of a given \"worker\" environment.\n",
    "\n",
    "Worker environments are always called in the same order. It means the first observation of this vector will always correspond to the first worker environment. \n",
    "\n",
    "\n",
    "Similarly to Observation, the \"step\" function of a multi_environment takes as input a list of multiple actions, each action will be implemented in its own environment. It returns a list of observations, a list of rewards, and boolean list of whether or not the worker environment suffer from a game over (in that case this worker environment is automatically restarted using the \"reset\" method.)\n",
    "\n",
    "Because orker environments are always called in the same order, the first action sent to the \"multi_env.step\" function will also be applied on this first environment.\n",
    "\n",
    "It is possible to use it as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<grid2op.Observation.CompleteObservation object at 0x7fa14bccdeb8>,\n",
       "       <grid2op.Observation.CompleteObservation object at 0x7fa14bccda20>,\n",
       "       <grid2op.Observation.CompleteObservation object at 0x7fa14bccde80>,\n",
       "       <grid2op.Observation.CompleteObservation object at 0x7fa14bccdc18>],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize the vector of actions that will be processed by each worker environment.\n",
    "acts = [None for _ in range(NUM_CORE)]\n",
    "for env_act_id in range(NUM_CORE):\n",
    "    acts[env_act_id] = agent.act(obss[env_act_id], rews[env_act_id], dones[env_act_id])\n",
    "    \n",
    "# feed them to the multi_env\n",
    "obss, rews, dones, infos = multi_envs.step(acts)\n",
    "\n",
    "# as explained, this is a vector of Observation (as many as NUM_CORE in this example)\n",
    "obss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multi environment loop is really close to the \"gym\" loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performs the appropriated steps\n",
    "for i in range(TRAINING_STEP):\n",
    "    acts = [None for _ in range(NUM_CORE)]\n",
    "    for env_act_id in range(NUM_CORE):\n",
    "        acts[env_act_id] = agent.act(obss[env_act_id], rews[env_act_id], dones[env_act_id])\n",
    "    obss, rews, dones, infos = multi_envs.step(acts)\n",
    "\n",
    "    # DO SOMETHING WITH THE AGENT IF YOU WANT\n",
    "    ## agent.train(obss, rews, dones)\n",
    "    \n",
    "\n",
    "# close the environments created by the multi_env\n",
    "multi_envs.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the above example, `TRAINING_STEP` steps are performed on `NUM_CORE` environments in parrallel. The agent has then acted `TRAINING_STEP * NUM_CORE` (=`10 * 4 = 40` by default) times on `NUM_CORE` different environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.b) Practical example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reuse the code of the Notebook [3_TrainingAnAgent](3_TrainingAnAgent.ipynb) to train a new agent, but this time using more than one process of the machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_agent import TrainingParam, ReplayBuffer, TrainAgent\n",
    "from ml_agent import DeepQ, DuelQ, SAC\n",
    "from grid2op.Agent import AgentWithConverter\n",
    "from grid2op.Reward import RedispReward\n",
    "from grid2op.Converters import IdToAct\n",
    "import numpy as np\n",
    "import random\n",
    "import warnings\n",
    "import pdb\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    import tensorflow.keras\n",
    "    import tensorflow.keras.backend as K\n",
    "    from tensorflow.keras.models import load_model, Sequential, Model\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, subtract, add\n",
    "    from tensorflow.keras.layers import Input, Lambda, Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAgentMultiEnv(TrainAgent):\n",
    "    def __init__(self, agent, nb_process, reward_fun=RedispReward, env=None):\n",
    "        TrainAgent.__init__(self, agent, reward_fun=RedispReward, env=env)\n",
    "        self.nb_process = nb_process\n",
    "        self.multi_envs = None\n",
    "        # TODO optimize that to have a numpy array\n",
    "        self.process_buffer = [[] for _ in range(self.nb_process)]\n",
    "        # self.process_buffer = np.zeros((self.nb_process, ))\n",
    "        \n",
    "    def convert_obs(self, observation):\n",
    "        return observation.rho\n",
    "    \n",
    "    def convert_process_buffer(self):\n",
    "        \"\"\"Converts the list of NUM_FRAMES images in the process buffer\n",
    "        into one training sample\"\"\"\n",
    "        # here i simply concatenate the action in case of multiple action in the \"buffer\"\n",
    "        # this function existed in the original implementation, bus has been adapted.\n",
    "        if self.training_param.NUM_FRAMES != 1:\n",
    "            raise RuntimeError(\"can only use self.training_param.NUM_FRAMES = 1 for now\")\n",
    "        return np.array([np.concatenate(el) for el in self.process_buffer])\n",
    "        # return np.concatenate(self.process_buffer)\n",
    "        # TODO fix cases where NUM_FRAMES is not 1 !!!!\n",
    "        \n",
    "    def _build_valid_env(self, training_param):\n",
    "        create_new = False\n",
    "        if self.multi_envs is None:\n",
    "#             create_new = super()._build_valid_env(training_param)\n",
    "            self.multi_envs = MultiEnvironment(env=env, nb_env=self.nb_process)\n",
    "            \n",
    "            # make sure the environment is reset\n",
    "            obss = self.multi_envs.reset()\n",
    "            for worker_id in range(self.nb_process):\n",
    "                self.process_buffer[worker_id].append(self.agent.convert_obs(obss[worker_id])) \n",
    "                # self.agent.process_buffer.append(self.agent.convert_obs(obss[worker_id]))\n",
    "            do_nothing = [self.env.action_space() for _ in range(self.nb_process)]\n",
    "            for _ in range(training_param.NUM_FRAMES-1):\n",
    "                # Initialize buffer with the first frames\n",
    "                s1, r1, _, _ = self.multi_envs.step(do_nothing)\n",
    "                for worker_id in range(self.nb_process):\n",
    "                    self.process_buffer[worker_id].append(self.agent.convert_obs(s1[worker_id])) \n",
    "        return create_new\n",
    "    \n",
    "    def train(self, num_frames, training_param=TrainingParam()):\n",
    "        # this function existed in the original implementation, but has been slightly adapted.\n",
    "        \n",
    "        # first we create an environment or make sure the given environment is valid\n",
    "        close_env = self._build_valid_env(training_param)\n",
    "        \n",
    "        # bellow that, only slight modification has been made. They are highlighted\n",
    "        observation_num = 0\n",
    "        curr_state = self.convert_process_buffer()\n",
    "        \n",
    "        # it's a bit less convenient that using the SpaceInvader environment.\n",
    "        # first we need to initiliaze the neural network\n",
    "        self.agent.init_deep_q(curr_state)\n",
    "        # TODO it's weird to use the process buffer for this purpose...\n",
    "            \n",
    "        epsilon = training_param.INITIAL_EPSILON\n",
    "        alive_frame = np.zeros(self.nb_process, dtype=np.int)\n",
    "        total_reward = np.zeros(self.nb_process, dtype=np.float)\n",
    "\n",
    "        while observation_num < num_frames:\n",
    "            if observation_num % 1000 == 999:\n",
    "                print((\"Executing loop %d\" %observation_num))\n",
    "\n",
    "            # Slowly decay the learning rate\n",
    "            if epsilon > training_param.FINAL_EPSILON:\n",
    "                epsilon -= (training_param.INITIAL_EPSILON-training_param.FINAL_EPSILON)/training_param.EPSILON_DECAY\n",
    "\n",
    "            initial_state = self.convert_process_buffer()\n",
    "            self.process_buffer = [[] for _ in range(self.nb_process)]\n",
    "            \n",
    "            # TODO vectorize that in the Agent directly\n",
    "            # ADDED\n",
    "            predict_movement_int = []\n",
    "            predict_q_value = []\n",
    "            acts = []\n",
    "            # then we need to predict the next moves\n",
    "            #pdb.set_trace()\n",
    "            pm_i, pq_v = self.agent.deep_q.predict_movement(curr_state, epsilon)\n",
    "            for p_id in range(self.nb_process):\n",
    "                predict_movement_int.append(pm_i[p_id])\n",
    "                predict_q_value.append(pq_v[p_id])\n",
    "                # and then we convert it to a valid action\n",
    "                acts.append(self.agent.convert_act(pm_i[p_id]))\n",
    "            \n",
    "            reward, done = np.zeros(self.nb_process), np.full(self.nb_process, fill_value=False, dtype=np.bool)\n",
    "            for i in range(training_param.NUM_FRAMES):\n",
    "                temp_observation_obj, temp_reward, temp_done, _ = self.multi_envs.step(acts)\n",
    "                # here it has been adapted too. The observation get from the environment is\n",
    "                # first converted to vector\n",
    "                \n",
    "                # below this line no changed have been made to the original implementation.\n",
    "                reward[~temp_done] += temp_reward[~temp_done]\n",
    "                \n",
    "                \n",
    "                for worker_id, obs in enumerate(temp_observation_obj):\n",
    "                    # ADDED\n",
    "                    self.process_buffer[worker_id].append(self.agent.convert_obs(temp_observation_obj[worker_id])) \n",
    "                    \n",
    "                done = done | temp_done\n",
    "\n",
    "                # TODO fix that too\n",
    "                alive_frame[~temp_done] += 1\n",
    "            \n",
    "                for env_done_idx in np.where(temp_done)[0]:\n",
    "                    print(\"For env with id {}\".format(env_done_idx))\n",
    "                    print(\"\\tLived with maximum time \", alive_frame[env_done_idx])\n",
    "                    print(\"\\tEarned a total of reward equal to \", total_reward[env_done_idx])\n",
    "                \n",
    "                reward[temp_done] = 0.\n",
    "                total_reward[temp_done] = 0.\n",
    "                total_reward += reward\n",
    "                alive_frame[temp_done] = 0\n",
    "            \n",
    "            new_state = self.convert_process_buffer()\n",
    "            for sub_env_id in range(self.nb_process):\n",
    "                # ADDED\n",
    "                self.agent.replay_buffer.add(initial_state[sub_env_id],\n",
    "                                             predict_movement_int[sub_env_id],\n",
    "                                             reward[sub_env_id],\n",
    "                                             done[sub_env_id],\n",
    "                                             new_state[sub_env_id])\n",
    "                \n",
    "            if self.agent.replay_buffer.size() > training_param.MIN_OBSERVATION:\n",
    "                s_batch, a_batch, r_batch, d_batch, s2_batch = self.agent.replay_buffer.sample(training_param.MINIBATCH_SIZE)\n",
    "                isfinite = self.agent.deep_q.train(s_batch, a_batch, r_batch, d_batch, s2_batch, observation_num)\n",
    "                self.agent.deep_q.target_train()\n",
    "            \n",
    "                if not isfinite:\n",
    "                    # if the loss is not finite\n",
    "                    print(\"E INFINITE LOSS\")\n",
    "                    break\n",
    "                \n",
    "\n",
    "            # Save the network every 100000 iterations\n",
    "            if observation_num % 10000 == 9999 or observation_num == num_frames-1:\n",
    "                print(\"Saving Network\")\n",
    "                self.agent.deep_q.save_network(\"saved_notebook6.h5\")\n",
    "                \n",
    "            observation_num += 1\n",
    "            \n",
    "        if close_env:\n",
    "            print(\"closing env\")\n",
    "            self.env.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We redifine the class used to train the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DeepQ(object):\n",
    "#     \"\"\"Constructs the desired deep q learning network\"\"\"\n",
    "#     def __init__(self, action_size, observation_size,\n",
    "#                  lr=1e-5,\n",
    "#                  training_param=TrainingParam()):\n",
    "#         # It is not modified from  Abhinav Sagar's code, except for adding the possibility to change the learning rate\n",
    "#         # in parameter is also present the size of the action space\n",
    "#         # (it used to be a global variable in the original code)\n",
    "#         self.action_size = action_size\n",
    "#         self.observation_size = observation_size\n",
    "#         self.model = None\n",
    "#         self.target_model = None\n",
    "#         self.lr_ = lr\n",
    "#         self.qvalue_evolution = np.zeros((0,))\n",
    "#         self.training_param = training_param\n",
    "#         self.construct_q_network()\n",
    "    \n",
    "#     def construct_q_network(self):\n",
    "#         # replacement of the Convolution layers by Dense layers, and change the size of the input space and output space\n",
    "        \n",
    "#         # Uses the network architecture found in DeepMind paper\n",
    "#         self.model = Sequential()\n",
    "#         input_layer = Input(shape=(self.observation_size * self.training_param.NUM_FRAMES,))\n",
    "#         layer1 = Dense(self.observation_size * self.training_param.NUM_FRAMES)(input_layer)\n",
    "#         layer1 = Activation('relu')(layer1)\n",
    "#         layer2 = Dense(self.observation_size)(layer1)\n",
    "#         layer2 = Activation('relu')(layer2)\n",
    "#         layer3 = Dense(self.observation_size)(layer2)\n",
    "#         layer3 = Activation('relu')(layer3)\n",
    "#         layer4 = Dense(2 * self.action_size)(layer3)\n",
    "#         layer4 = Activation('relu')(layer4)\n",
    "#         output = Dense(self.action_size)(layer4)\n",
    "\n",
    "#         self.model = Model(inputs=[input_layer], outputs=[output])\n",
    "#         self.model.compile(loss='mse', optimizer=Adam(lr=self.lr_))\n",
    "\n",
    "#         self.target_model = Model(inputs=[input_layer], outputs=[output])\n",
    "#         self.target_model.compile(loss='mse', optimizer=Adam(lr=self.lr_))\n",
    "#         self.target_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "#     def predict_movement(self, data, epsilon):\n",
    "#         \"\"\"Predict movement of game controler where is epsilon\n",
    "#         probability randomly move.\"\"\"\n",
    "#         rand_val = np.random.random(data.shape[0])\n",
    "#         q_actions = self.model.predict(data)\n",
    "#         opt_policy = np.argmax(np.abs(q_actions), axis=-1)\n",
    "#         opt_policy[rand_val < epsilon] = np.random.randint(0, self.action_size, size=(np.sum(rand_val < epsilon)))\n",
    "        \n",
    "#         self.qvalue_evolution = np.concatenate((self.qvalue_evolution , q_actions[0, opt_policy]))\n",
    "#         return opt_policy, q_actions[0, opt_policy]\n",
    "\n",
    "#     def train(self, s_batch, a_batch, r_batch, d_batch, s2_batch, observation_num):\n",
    "#         \"\"\"Trains network to fit given parameters\"\"\"\n",
    "#         targets = self.model.predict(s_batch)\n",
    "#         fut_action = self.target_model.predict(s2_batch)\n",
    "#         targets[:, a_batch] = r_batch\n",
    "#         targets[d_batch, a_batch[d_batch]] += self.training_param.DECAY_RATE * np.max(fut_action[d_batch], axis=-1)\n",
    "        \n",
    "#         loss = self.model.train_on_batch(s_batch, targets)\n",
    "#         # Print the loss every 100 iterations.\n",
    "#         if observation_num % 100 == 0:\n",
    "#             print(\"We had a loss equal to \", loss)\n",
    "#         return np.all(np.isfinite(loss))\n",
    "\n",
    "#     def save_network(self, path):\n",
    "#         # Saves model at specified path as h5 file\n",
    "#         # nothing has changed\n",
    "#         self.model.save(path)\n",
    "#         print(\"Successfully saved network.\")\n",
    "\n",
    "#     def load_network(self, path):\n",
    "#         # nothing has changed\n",
    "#         self.model = load_model(path)\n",
    "#         print(\"Succesfully loaded network.\")\n",
    "\n",
    "#     def target_train(self):\n",
    "#         # nothing has changed from the original implementation\n",
    "#         model_weights = self.model.get_weights()\n",
    "#         target_model_weights = self.target_model.get_weights()\n",
    "#         for i in range(len(model_weights)):\n",
    "#             target_model_weights[i] = self.training_param.TAU * model_weights[i] + (1 - self.training_param.TAU) * target_model_weights[i]\n",
    "#         self.target_model.set_weights(target_model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQAgent(AgentWithConverter):\n",
    "    # first change: An Agent must derived from grid2op.Agent (in this case MLAgent, because we manipulate vector instead\n",
    "    # of classes)\n",
    "\n",
    "    def convert_obs(self, observation):\n",
    "        return np.concatenate((observation.rho, observation.topo_vect))\n",
    "\n",
    "    def my_act(self, transformed_observation, reward, done=False):\n",
    "        if self.deep_q is None:\n",
    "            self.init_deep_q(transformed_observation)\n",
    "        predict_movement_int, *_ = self.deep_q.predict_movement(transformed_observation, epsilon=0.0)\n",
    "        # print(\"predict_movement_int: {}\".format(predict_movement_int))\n",
    "        return predict_movement_int\n",
    "\n",
    "    def init_deep_q(self, transformed_observation):\n",
    "        if self.deep_q is None:\n",
    "            # the first time an observation is observed, I set up the neural network with the proper dimensions.\n",
    "            if self.mode == \"DQN\":\n",
    "                cls = DeepQ\n",
    "            elif self.mode == \"DDQN\":\n",
    "                cls = DuelQ\n",
    "            # elif self.mode == \"RealQ\":\n",
    "            #     cls = RealQ\n",
    "            elif self.mode == \"SAC\":\n",
    "                cls = SAC\n",
    "            else:\n",
    "                raise RuntimeError(\"Unknown neural network named \\\"{}\\\"\".format(self.mode))\n",
    "            self.deep_q = cls(self.action_space.size(), observation_size=transformed_observation.shape[-1], lr=self.lr)\n",
    "\n",
    "    def __init__(self, action_space, mode=\"DDQN\", lr=1e-5, training_param=TrainingParam()):\n",
    "        # this function has been adapted.\n",
    "\n",
    "        # to built a AgentWithConverter, we need an action_space.\n",
    "        # No problem, we add it in the constructor.\n",
    "        AgentWithConverter.__init__(self, action_space, action_space_converter=IdToAct)\n",
    "\n",
    "        # and now back to the origin implementation\n",
    "        self.replay_buffer = ReplayBuffer(training_param.BUFFER_SIZE)\n",
    "\n",
    "        # compare to original implementation, i don't know the observation space size.\n",
    "        # Because it depends on the component of the observation we want to look at. So these neural network will\n",
    "        # be initialized the first time an observation is observe.\n",
    "        self.deep_q = None\n",
    "        self.mode = mode\n",
    "        self.lr = lr\n",
    "        self.training_param = training_param\n",
    "\n",
    "    def load_network(self, path):\n",
    "        # not modified compare to original implementation\n",
    "        self.deep_q.load_network(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/donnotben/.local/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/donnotben/.local/lib/python3.6/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Successfully constructed networks.\n",
      "For env with id 3\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 1\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  663.1549377834867\n",
      "For env with id 2\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 0\n",
      "\tLived with maximum time  7\n",
      "\tEarned a total of reward equal to  1119.0141611232095\n",
      "For env with id 1\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 2\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 3\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 0\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 3\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "WARNING:tensorflow:From /home/donnotben/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "For env with id 1\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 2\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.1051538807224\n",
      "For env with id 0\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 2\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 3\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 0\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 3\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 1\n",
      "\tLived with maximum time  8\n",
      "\tEarned a total of reward equal to  1340.738059939322\n",
      "For env with id 2\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 0\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  894.7650249874125\n",
      "For env with id 2\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 3\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 0\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 1\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 3\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 1\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 2\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 0\n",
      "\tLived with maximum time  6\n",
      "\tEarned a total of reward equal to  915.0065051670035\n",
      "For env with id 2\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 3\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 0\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 1\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 3\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 1\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 2\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 0\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 3\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  1140.2695266588664\n",
      "For env with id 0\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 1\n",
      "\tLived with maximum time  7\n",
      "\tEarned a total of reward equal to  886.555916273282\n",
      "For env with id 3\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  889.3287299169673\n",
      "For env with id 2\n",
      "\tLived with maximum time  9\n",
      "\tEarned a total of reward equal to  1568.9710548441935\n",
      "For env with id 1\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 0\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 0\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 2\n",
      "\tLived with maximum time  4\n",
      "\tEarned a total of reward equal to  917.7439746052744\n",
      "For env with id 3\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  1126.425636489143\n",
      "For env with id 2\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  219.68301715770022\n",
      "For env with id 3\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 1\n",
      "\tLived with maximum time  7\n",
      "\tEarned a total of reward equal to  1123.7535391830147\n",
      "For env with id 1\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 3\n",
      "\tLived with maximum time  3\n",
      "\tEarned a total of reward equal to  694.9462542270578\n",
      "For env with id 0\n",
      "\tLived with maximum time  7\n",
      "\tEarned a total of reward equal to  1118.13770283329\n",
      "For env with id 2\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 3\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 0\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 1\n",
      "\tLived with maximum time  3\n",
      "\tEarned a total of reward equal to  672.1510071208504\n",
      "For env with id 2\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 1\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 3\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 0\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 2\n",
      "\tLived with maximum time  4\n",
      "\tEarned a total of reward equal to  900.8270000500152\n",
      "For env with id 3\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 0\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 1\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 2\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 1\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 3\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 2\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 0\n",
      "\tLived with maximum time  7\n",
      "\tEarned a total of reward equal to  887.6388684788744\n",
      "For env with id 3\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 1\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 2\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 0\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 1\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  448.6448031317167\n",
      "For env with id 3\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  920.3227828534493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For env with id 2\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 0\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 3\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 1\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 2\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 0\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 1\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 3\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 2\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "We had a loss equal to  158.96332\n",
      "For env with id 0\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  1128.940801257686\n",
      "For env with id 3\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 1\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 2\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 0\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 1\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 3\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 2\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 3\n",
      "\tLived with maximum time  1\n",
      "\tEarned a total of reward equal to  225.69855698594938\n",
      "For env with id 0\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 1\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 2\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 0\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 1\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 3\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 2\n",
      "\tLived with maximum time  4\n",
      "\tEarned a total of reward equal to  899.5587135321497\n",
      "For env with id 3\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 0\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 2\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 1\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 0\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 1\n",
      "\tLived with maximum time  3\n",
      "\tEarned a total of reward equal to  681.120597465485\n",
      "For env with id 2\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 3\n",
      "\tLived with maximum time  7\n",
      "\tEarned a total of reward equal to  1117.7479548117778\n",
      "For env with id 0\n",
      "\tLived with maximum time  4\n",
      "\tEarned a total of reward equal to  947.0927337608713\n",
      "For env with id 2\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 3\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 0\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 1\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 1\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 2\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 3\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 0\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 2\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 3\n",
      "\tLived with maximum time  1\n",
      "\tEarned a total of reward equal to  237.76975531488168\n",
      "For env with id 0\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  219.68301715770022\n",
      "For env with id 1\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 1\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 2\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  1128.66475782728\n",
      "For env with id 3\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 0\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 2\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 3\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 0\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 1\n",
      "\tLived with maximum time  6\n",
      "\tEarned a total of reward equal to  888.9851245633683\n",
      "For env with id 1\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 2\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 3\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 0\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 2\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 3\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 0\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 1\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 1\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 2\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 3\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 0\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 2\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 0\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 3\n",
      "\tLived with maximum time  4\n",
      "\tEarned a total of reward equal to  680.1145726191411\n",
      "For env with id 1\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 1\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 2\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 0\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 3\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 2\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 0\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 3\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 1\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 1\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 2\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 3\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 2\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For env with id 0\n",
      "\tLived with maximum time  7\n",
      "\tEarned a total of reward equal to  886.555916273282\n",
      "For env with id 3\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 1\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 0\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 1\n",
      "\tLived with maximum time  2\n",
      "\tEarned a total of reward equal to  447.7991706748635\n",
      "For env with id 2\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 3\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n",
      "For env with id 0\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  896.2686016287104\n"
     ]
    }
   ],
   "source": [
    "TRAINING_STEP = 1000\n",
    "my_agent = DeepQAgent(env.action_space, mode=\"DDQN\", training_param=TrainingParam())\n",
    "trainer = TrainAgentMultiEnv(agent=my_agent, env=env, nb_process=NUM_CORE)\n",
    "# trainer = TrainAgent(agent=my_agent, env=env)\n",
    "trainer.train(TRAINING_STEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(30,20))\n",
    "plt.plot(my_agent.deep_q.qvalue_evolution)\n",
    "plt.axhline(y=0, linewidth=3, color='red')\n",
    "_ = plt.xlim(0, len(my_agent.deep_q.qvalue_evolution))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_agent.deep_q.qvalue_evolution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
