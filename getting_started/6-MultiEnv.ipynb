{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benjamin/.local/lib/python3.6/site-packages/pandapower/io_utils.py:8: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  from pandas.util.testing import assert_series_equal, assert_frame_equal\n"
     ]
    }
   ],
   "source": [
    "import grid2op\n",
    "from grid2op.Reward import ConstantReward, FlatReward\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "TRAINING_STEP = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download more data for the default environment.\n",
    "\n",
    "A lot of data have been made available for the default \"case14_redisp\" environment. Including this data in the package is not convenient. We chose instead to release them and make them easily available with a utility. To download them in the default directory (\"~/data_grid2op/case14_redisp\") on linux based system you can do the following (uncomment the following command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !$sys.executable -m grid2op.download --name \"case14_redisp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a regular environment and agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we downloaded the dataset, it is time to make an environment that will use all the data avaiable. You can execute the following command line. If you see any error or warning consider re downloading the data, or adapting the key-word argument \"chronics_path\" to match the path where the data have been downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    env = grid2op.make(name_env=\"case14_redisp\", chronics_path=os.path.expanduser(\"~/data_grid2op/case14_redisp\"))\n",
    "except Exception as exc :\n",
    "    print(\"Please read the above cell, it appears you don't have downloaded the dataset,\"\\\n",
    "          \"or save it into an unknown repository.\" \\\n",
    "          \"I will continue with only 2 sets.\")\n",
    "    env = grid2op.make(name_env=\"case14_redisp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a standard RL Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you are using a computer with at least 4 cores if you want to notice some speed-ups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grid2op.MultiEnv import MultiEnvironment\n",
    "from grid2op.Agent import DoNothingAgent\n",
    "NUM_CORE = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the standard open AI gym loop\n",
    "\n",
    "Here we demonstrate how to use the multi environment class. First let's create a multi environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple agent\n",
    "agent = DoNothingAgent(env.action_space)\n",
    "\n",
    "# create the multi environment class\n",
    "multi_envs = MultiEnvironment(env=env, nb_env=NUM_CORE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A multienvironment is just like a regular environment but instead of dealing with one action, and one observation, is requires to be sent multiple actions, and returns a list of observations as well. \n",
    "\n",
    "It requires a grid2op environment to be initialized and creates some specific \"workers\", each a replication of the initial environment. None of the \"worker\" can be accessed directly. Supported methods are:\n",
    "- multi_env.reset\n",
    "- multi_env.step\n",
    "- multi_env.close\n",
    "\n",
    "That have similar behaviour to \"env.step\", \"env.close\" or \"env.reset\".\n",
    "\n",
    "\n",
    "It can be used the following manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<grid2op.Observation.CompleteObservation object at 0x7fa4d523b8d0>,\n",
       "       <grid2op.Observation.CompleteObservation object at 0x7fa4d523e5f8>,\n",
       "       <grid2op.Observation.CompleteObservation object at 0x7fa4d523b9e8>,\n",
       "       <grid2op.Observation.CompleteObservation object at 0x7fa4d523e6a0>],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initiliaze some variable with the proper dimension\n",
    "obss = multi_envs.reset()\n",
    "rews = [env.reward_range[0] for i in range(NUM_CORE)]\n",
    "dones = [False for i in range(NUM_CORE)]\n",
    "obss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, obs is not a single obervation, but a list (numpy nd array to be precise) of 4 observations, each one being an observation of a given \"worker\" environment.\n",
    "\n",
    "Worker environments are always called in the same order. It means the first observation of this vector will always correspond to the first worker environment. \n",
    "\n",
    "\n",
    "Similarly to Observation, the \"step\" function of a multi_environment takes as input a list of multiple actions, each action will be implemented in its own environment. It returns a list of observations, a list of rewards, and boolean list of whether or not the worker environment suffer from a game over (in that case this worker environment is automatically restarted using the \"reset\" method.)\n",
    "\n",
    "Because orker environments are always called in the same order, the first action sent to the \"multi_env.step\" function will also be applied on this first environment.\n",
    "\n",
    "It is possible to use it as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<grid2op.Observation.CompleteObservation object at 0x7fa4d5246be0>,\n",
       "       <grid2op.Observation.CompleteObservation object at 0x7fa4d5246978>,\n",
       "       <grid2op.Observation.CompleteObservation object at 0x7fa4d52469e8>,\n",
       "       <grid2op.Observation.CompleteObservation object at 0x7fa4d5246b38>],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize the vector of actions that will be processed by each worker environment.\n",
    "acts = [None for _ in range(NUM_CORE)]\n",
    "for env_act_id in range(NUM_CORE):\n",
    "    acts[env_act_id] = agent.act(obss[env_act_id], rews[env_act_id], dones[env_act_id])\n",
    "    \n",
    "# feed them to the multi_env\n",
    "obss, rews, dones, infos = multi_envs.step(acts)\n",
    "\n",
    "# as explained, this is a vector of Observation (as many as NUM_CORE in this example)\n",
    "obss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multi environment loop is really close to the \"gym\" loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performs the appropriated steps\n",
    "for i in range(TRAINING_STEP):\n",
    "    acts = [None for _ in range(NUM_CORE)]\n",
    "    for env_act_id in range(NUM_CORE):\n",
    "        acts[env_act_id] = agent.act(obss[env_act_id], rews[env_act_id], dones[env_act_id])\n",
    "    obss, rews, dones, infos = multi_envs.step(acts)\n",
    "\n",
    "    # DO SOMETHING WITH THE AGENT IF YOU WANT\n",
    "    ## agent.train(obss, rews, dones)\n",
    "    \n",
    "\n",
    "# close the environments created by the multi_env\n",
    "multi_envs.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the above example, `TRAINING_STEP` steps are performed on `NUM_CORE` environments in parrallel. The agent has then acted `TRAINING_STEP * NUM_CORE` (=`10 * 4 = 40` by default) times on `NUM_CORE` different environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the training agent class from the previous notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reuse the code of the Notebook [3_TrainingAnAgent](3_TrainingAnAgent.ipynb) to train a new agent, but this time using more than one process of the machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from ml_agent import TrainAgent, DeepQAgent, TrainingParam\n",
    "from grid2op.Reward import RedispReward\n",
    "import numpy as np\n",
    "import random\n",
    "import warnings\n",
    "import pdb\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    import keras\n",
    "    import keras.backend as K\n",
    "    from keras.models import load_model, Sequential, Model\n",
    "    from keras.optimizers import Adam\n",
    "    from keras.layers.core import Activation, Dropout, Flatten, Dense\n",
    "    from keras.layers import Input, Lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We redifine the class used to train the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAgentMultiEnv(TrainAgent):\n",
    "    def __init__(self, agent, nb_process, reward_fun=RedispReward, env=None):\n",
    "        TrainAgent.__init__(self, agent, reward_fun=RedispReward, env=env)\n",
    "        self.nb_process = nb_process\n",
    "        self.multi_envs = None\n",
    "        \n",
    "    def _build_valid_env(self, training_param):\n",
    "        create_new = False\n",
    "        if self.multi_envs is None:\n",
    "#             create_new = super()._build_valid_env(training_param)\n",
    "            self.multi_envs = MultiEnvironment(env=env, nb_env=self.nb_process)\n",
    "            \n",
    "            # make sure the environment is reset\n",
    "            obss = self.multi_envs.reset()\n",
    "            for worker_id in range(self.nb_process):\n",
    "                self.agent.process_buffer.append(self.agent.convert_obs(obss[worker_id]))\n",
    "            do_nothing = [self.env.action_space() for _ in range(self.nb_process)]\n",
    "            for _ in range(training_param.NUM_FRAMES-1):\n",
    "                # Initialize buffer with the first frames\n",
    "                s1, r1, _, _ = self.multi_envs.step(do_nothing)\n",
    "                for worker_id in range(self.nb_process):\n",
    "                    self.agent.process_buffer.append(self.agent.convert_obs(s1[worker_id])) \n",
    "        return create_new\n",
    "    \n",
    "    def train(self, num_frames, training_param=TrainingParam()):\n",
    "        # this function existed in the original implementation, but has been slightly adapted.\n",
    "        \n",
    "        # first we create an environment or make sure the given environment is valid\n",
    "        close_env = self._build_valid_env(training_param)\n",
    "        \n",
    "        # bellow that, only slight modification has been made. They are highlighted\n",
    "        observation_num = 0\n",
    "        curr_state = self.agent.convert_process_buffer()\n",
    "        \n",
    "        # it's a bit less convenient that using the SpaceInvader environment.\n",
    "        # first we need to initiliaze the neural network\n",
    "        self.agent.init_deep_q(curr_state[0])\n",
    "        # TODO it's weird to use the process buffer for this purpose...\n",
    "            \n",
    "        epsilon = training_param.INITIAL_EPSILON\n",
    "        alive_frame = np.zeros(self.nb_process, dtype=np.int)\n",
    "        total_reward = np.zeros(self.nb_process, dtype=np.float)\n",
    "\n",
    "        while observation_num < num_frames:\n",
    "            if observation_num % 1000 == 999:\n",
    "                print((\"Executing loop %d\" %observation_num))\n",
    "\n",
    "            # Slowly decay the learning rate\n",
    "            if epsilon > training_param.FINAL_EPSILON:\n",
    "                epsilon -= (training_param.INITIAL_EPSILON-training_param.FINAL_EPSILON)/training_param.EPSILON_DECAY\n",
    "\n",
    "            initial_state = self.agent.convert_process_buffer()\n",
    "#             pdb.set_trace()\n",
    "            self.agent.process_buffer = []\n",
    "            \n",
    "            # TODO vectorize that in the Agent directly\n",
    "            # ADDED\n",
    "            predict_movement_int = []\n",
    "            predict_q_value = []\n",
    "            acts = []\n",
    "            # then we need to predict the next moves\n",
    "            for p_id in range(self.nb_process):\n",
    "                pm_i, pq_v = self.agent.deep_q.predict_movement(curr_state[p_id], epsilon)\n",
    "                predict_movement_int.append(pm_i)\n",
    "                predict_q_value.append(pq_v)\n",
    "                \n",
    "                # and then we convert it to a valid action\n",
    "                acts.append(self.agent.convert_act(pm_i))\n",
    "            \n",
    "            reward, done = np.zeros(self.nb_process), np.full(self.nb_process, fill_value=False, dtype=np.bool)\n",
    "            for i in range(training_param.NUM_FRAMES):\n",
    "                temp_observation_obj, temp_reward, temp_done, _ = self.multi_envs.step(acts)\n",
    "                # here it has been adapted too. The observation get from the environment is\n",
    "                # first converted to vector\n",
    "                \n",
    "                # below this line no changed have been made to the original implementation.\n",
    "                reward[~temp_done] += temp_reward[~temp_done]\n",
    "                \n",
    "                \n",
    "                for obs in temp_observation_obj:\n",
    "                    # ADDED\n",
    "                    self.agent.process_buffer.append(self.agent.convert_obs(obs))\n",
    "                    \n",
    "                done = done | temp_done\n",
    "\n",
    "                # TODO fix that too\n",
    "                alive_frame[~temp_done] += 1\n",
    "            \n",
    "                for env_done_idx in np.where(temp_done)[0]:\n",
    "                    print(\"For env with id {}\".format(env_done_idx))\n",
    "                    print(\"\\tLived with maximum time \", alive_frame[env_done_idx])\n",
    "                    print(\"\\tEarned a total of reward equal to \", total_reward[env_done_idx])\n",
    "                \n",
    "                reward[temp_done] = 0.\n",
    "                alive_frame[temp_done] = 0\n",
    "            \n",
    "            new_state = self.agent.convert_process_buffer()\n",
    "            for sub_env_id in range(self.nb_process):\n",
    "                # ADDED\n",
    "                self.agent.replay_buffer.add(initial_state[sub_env_id],\n",
    "                                             predict_movement_int[sub_env_id],\n",
    "                                             reward[sub_env_id],\n",
    "                                             done[sub_env_id],\n",
    "                                             new_state[sub_env_id])\n",
    "            total_reward += reward\n",
    "            if self.agent.replay_buffer.size() > training_param.MIN_OBSERVATION:\n",
    "                s_batch, a_batch, r_batch, d_batch, s2_batch = self.agent.replay_buffer.sample(training_param.MINIBATCH_SIZE)\n",
    "                self.agent.deep_q.train(s_batch, a_batch, r_batch, d_batch, s2_batch, observation_num)\n",
    "                self.agent.deep_q.target_train()\n",
    "\n",
    "            # Save the network every 100000 iterations\n",
    "            if observation_num % 10000 == 9999 or observation_num == num_frames-1:\n",
    "                print(\"Saving Network\")\n",
    "                self.agent.deep_q.save_network(\"saved_notebook6.h5\")\n",
    "\n",
    "#             alive_frame += 1\n",
    "            observation_num += 1\n",
    "            \n",
    "        if close_env:\n",
    "            print(\"closing env\")\n",
    "            self.env.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/benjamin/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "For env with id 1\n",
      "\tLived with maximum time  5\n",
      "\tEarned a total of reward equal to  -25.0\n",
      "For env with id 2\n",
      "\tLived with maximum time  20\n",
      "\tEarned a total of reward equal to  867.1499305135851\n",
      "For env with id 0\n",
      "\tLived with maximum time  21\n",
      "\tEarned a total of reward equal to  147.57217847769024\n",
      "For env with id 1\n",
      "\tLived with maximum time  30\n",
      "\tEarned a total of reward equal to  585.8699453090342\n",
      "For env with id 2\n",
      "\tLived with maximum time  22\n",
      "\tEarned a total of reward equal to  2468.932234905286\n",
      "For env with id 3\n",
      "\tLived with maximum time  46\n",
      "\tEarned a total of reward equal to  3564.6394229154744\n",
      "For env with id 1\n",
      "\tLived with maximum time  16\n",
      "\tEarned a total of reward equal to  4079.7780691596645\n",
      "For env with id 2\n",
      "\tLived with maximum time  10\n",
      "\tEarned a total of reward equal to  4530.986762016142\n",
      "For env with id 0\n",
      "\tLived with maximum time  59\n",
      "\tEarned a total of reward equal to  1849.4143333089123\n",
      "For env with id 0\n",
      "\tLived with maximum time  7\n",
      "\tEarned a total of reward equal to  3005.5015134067844\n",
      "For env with id 3\n",
      "\tLived with maximum time  50\n",
      "\tEarned a total of reward equal to  14974.436810532696\n",
      "For env with id 0\n",
      "\tLived with maximum time  9\n",
      "\tEarned a total of reward equal to  5074.249670261919\n",
      "Saving Network\n",
      "Successfully saved network.\n"
     ]
    }
   ],
   "source": [
    "nb_frame = 100\n",
    "my_agent = DeepQAgent(env.action_space, mode=\"DDQN\", training_param=TrainingParam())\n",
    "trainer = TrainAgentMultiEnv(agent=my_agent, env=env, nb_process=NUM_CORE)\n",
    "trainer.train(nb_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TODO fix the reward, this is weird now, i believe!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
