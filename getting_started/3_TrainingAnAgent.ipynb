{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Agent, action converters and l2rpn_baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is recommended to have a look at the [0_basic_functionalities](0_basic_functionalities.ipynb), [1_Observation_Agents](1_Observation_Agents.ipynb) and [2_Action_GridManipulation](2_Action_GridManipulation.ipynb) notebooks before getting into this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objectives**\n",
    "\n",
    "In this notebook we will expose :\n",
    "* how to use the \"converters\": some specific action_space that allows to manipulate a specific action representation\n",
    "* how to train a (stupid) Agent using reinforcement learning.\n",
    "* how to inspect (rapidly) the action taken by the Agent\n",
    "\n",
    "**NB** for this tutorial we train an Agent inspired from this blog post: [deep-reinforcement-learning-tutorial-with-open-ai-gym](https://towardsdatascience.com/deep-reinforcement-learning-tutorial-with-open-ai-gym-c0de4471f368). Many other different reinforcement learning tutorial exist. The code showed in this notebook has no pretention except to demonstrate how to use Grid2Op functionality to train a Deep Reinforcement learning Agent and inspect its behaviour. There are absolutely nothing implied about the performance, training strategy, type of Agent, meta parameters etc. All of them are purely \"random\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import grid2op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Impossible to automatically add a menu / table of content to this notebook.\n",
      "You can download \"jyquickhelper\" package with: \n",
      "\"pip install jyquickhelper\"\n"
     ]
    }
   ],
   "source": [
    "res = None\n",
    "try:\n",
    "    from jyquickhelper import add_notebook_menu\n",
    "    res = add_notebook_menu()\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Impossible to automatically add a menu / table of content to this notebook.\\nYou can download \\\"jyquickhelper\\\" package with: \\n\\\"pip install jyquickhelper\\\"\")\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I) Manipulating action representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid2op package has been built with an \"object oriented\" perspective: almost everything is encapsulated in a dedicated `class`. This allows for more customization of the plateform.\n",
    "\n",
    "The downside of this approach is that machine learning method, and especially deep learning, often prefers to deal with vectors rather than with `complex` objects. Indeed, as we covered in the previous tutorials on the platform, building our own actions can be tedious and can sometime require knowledge of the powergrid.\n",
    "\n",
    "On the contrary, in most of standard Reinforcement Learning environment, actions have an higher representation. For example in pacman, there are 4 different types of actions: turn left, turn right, go up or do down. This allows for easy sampling (you need to achieve a uniform sampling you simply need to sample a number between 0 and 3 included) and an easy representation: each action is a different component of a vector of dimension 4 [because there are 4 actions]. \n",
    "\n",
    "On the other hand this representation is not \"human friendly\". It is quite convenient in the case of pacman because the action space is rather small making it possible to remember which action corresponds to which component, but in the case of the grid2op package, there are hundreds, sometimes thousands of actions, making it impossible to remember which component corresponds to which actions. We suppose we don't really care about this fact here, as tutorials on Reinforcement Learning with discrete action space often assume that actions are labelled with integer (such as in pacman for example).\n",
    "\n",
    "Howerever, to allow the training of RL agent more easily, we allows to make some \"[Converters](https://grid2op.readthedocs.io/en/latest/converters.html)\" whose roles are to allow an agent to deal with a custom representation of the action space. The class [AgentWithConverter](https://grid2op.readthedocs.io/en/latest/agent.html#grid2op.Agent.AgentWithConverter) is perfect for such usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benjamin/Documents/grid2op_dev/getting_started/grid2op/MakeEnv/Make.py:224: UserWarning: You are using a development environment. This environment is not intended for training agents.\n",
      "  warnings.warn(_MAKE_DEV_ENV_WARN)\n"
     ]
    }
   ],
   "source": [
    "# import the usefull class\n",
    "import numpy as np\n",
    "\n",
    "from grid2op import make\n",
    "from grid2op.Agent import RandomAgent \n",
    "max_iter = 100 # to make computation much faster we will only consider 50 time steps instead of 287\n",
    "env_name = \"rte_case14_redisp\"\n",
    "env = make(env_name, test=True)\n",
    "env.seed(0)  # this is to ensure the same action are taken by the \"RandomAgent\".\n",
    "my_agent =  RandomAgent(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it. This agent will be able to perform any action, but instead of going through the description of the actions from a powersystem point of view (ie setting what is connected to what, what is disconnected etc.) it will simply choose an integer with the method `my_act` this integer will then be converter back to a proper valid action.\n",
    "\n",
    "Here we have an example on the action representation as seen by the Agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172\n",
      "47\n",
      "117\n"
     ]
    }
   ],
   "source": [
    "for el in range(3):\n",
    "    print(my_agent.my_act(None, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And below you can see the \"`act`\" functions behaves as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This action will:\n",
      "\t - NOT change anything to the injections\n",
      "\t - NOT perform any redispatching action\n",
      "\t - NOT force any line status\n",
      "\t - NOT switch any line status\n",
      "\t - NOT switch anything in the topology\n",
      "\t - Set the bus of the following element:\n",
      "\t \t - assign bus 2 to line (origin) 12 [on substation 5]\n",
      "\t \t - assign bus 2 to line (origin) 13 [on substation 5]\n",
      "\t \t - assign bus 1 to line (origin) 14 [on substation 5]\n",
      "\t \t - assign bus 2 to line (extremity) 17 [on substation 5]\n",
      "\t \t - assign bus 1 to generator 2 [on substation 5]\n",
      "\t \t - assign bus 1 to load 5 [on substation 5]\n",
      "This action will:\n",
      "\t - NOT change anything to the injections\n",
      "\t - NOT perform any redispatching action\n",
      "\t - NOT force any line status\n",
      "\t - NOT switch any line status\n",
      "\t - Change the bus of the following element:\n",
      "\t \t - switch bus of line (origin) 3 [on substation 8]\n",
      "\t \t - switch bus of line (extremity) 16 [on substation 8]\n",
      "\t \t - switch bus of line (extremity) 19 [on substation 8]\n",
      "\t - NOT force any particular bus configuration\n",
      "This action will:\n",
      "\t - NOT change anything to the injections\n",
      "\t - NOT perform any redispatching action\n",
      "\t - NOT force any line status\n",
      "\t - NOT switch any line status\n",
      "\t - Change the bus of the following element:\n",
      "\t \t - switch bus of generator 1 [on substation 2]\n",
      "\t \t - switch bus of load 1 [on substation 2]\n",
      "\t - NOT force any particular bus configuration\n"
     ]
    }
   ],
   "source": [
    "for el in range(3):\n",
    "    print(my_agent.act(None, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB** lots of these actions are equivalent to the \"do nothing\" action at some point. For example, when trying to reconnect a powerline that is already connected will do nothing. Same for the topology. If everything is already connected to bus 1, then the action to connect things to bus 1 on the same substation will not affect the powergrid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II) Training an Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial, we will expose to built a Q-learning Agent. Most of the code originated from this blog post (today deleted) [https://towardsdatascience.com/deep-reinforcement-learning-tutorial-with-open-ai-gym-c0de4471f368](https://towardsdatascience.com/deep-reinforcement-learning-tutorial-with-open-ai-gym-c0de4471f368). \n",
    "\n",
    "The goal of this notebook is to emphasize the possibility to train agent using grid2op framework. The key message is: as grid2op fully implement the gym API it is rather easy to do. We will use the [l2rpn baselines](https://github.com/rte-france/l2rpn-baselines) repository and implement a Double Duelling Deep Q learning Algorithm. For more information, you can consult the code in the dedicated repository [here](https://github.com/rte-france/l2rpn-baselines/tree/master/l2rpn_baselines/DoubleDuelingDQN).\n",
    "\n",
    "**Requirements** This notebook require to have `keras` installed on your machine as well as the `l2rpn_baselines` repository\n",
    "\n",
    "As always in these notebook, we will use the `rte_case14_realistic` test Environment. More data are available if you don't pass the `test=True` parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.A) Defining some \"helpers\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type of Agent were are using require a bit of set up, independantly of Grid2Op. We will reuse the code showed in \n",
    "[https://towardsdatascience.com/deep-reinforcement-learning-tutorial-with-open-ai-gym-c0de4471f368](https://towardsdatascience.com/deep-reinforcement-learning-tutorial-with-open-ai-gym-c0de4471f368) and in [Reinforcement-Learning-Tutorial](https://github.com/abhinavsagar/Reinforcement-Learning-Tutorial) from Abhinav Sagar code under a *MIT license* found here: [MIT License](https://github.com/abhinavsagar/Reinforcement-Learning-Tutorial/blob/master/LICENSE).\n",
    "\n",
    "This first section is here to define these classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first let's import the necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf2.0 friendly\n",
    "import numpy as np\n",
    "import random\n",
    "import warnings\n",
    "import l2rpn_baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Meta parameters of the methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DECAY_RATE = 0.9\n",
    "BUFFER_SIZE = 40000\n",
    "MINIBATCH_SIZE = 64\n",
    "TOT_FRAME = 3000000\n",
    "EPSILON_DECAY = 10000\n",
    "MIN_OBSERVATION = 42 #5000\n",
    "FINAL_EPSILON = 1/300  # have on average 1 random action per scenario of approx 287 time steps\n",
    "INITIAL_EPSILON = 0.1\n",
    "TAU = 0.01\n",
    "ALPHA = 1\n",
    "# Number of frames to \"throw\" into network\n",
    "NUM_FRAMES = 1 ## this has been changed compared to the original implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.B) Adapatation of the inputs\n",
    "\n",
    "For most of the Deep Reinforcement Learning (for example on model used to play an Atari games) the inputs are images and output are integers that encodes for different action types (typically \"move up\" or \"move down\" in Atari). For our system (the powergrid) it is rather different. We did our best effort to simply the task of transforming to / from complex structures. Indeed the use of converters such as ([IdToAct](https://grid2op.readthedocs.io/en/latest/converter.html#grid2op.Converter.IdToAct)) allows easily to:\n",
    "- convert the class \"Observation\" automatically into vectors\n",
    "- map the action from integer to complete action type define in the previous notebook.\n",
    "\n",
    "In essence, a converter substitue to the \"action space\" of the Agent and is such that:\n",
    "- Agent manipulates simple structure\n",
    "- Converter ensure the mapping from this structure to  complex grid2op class Action / Observation\n",
    "- So that outside of the Agent, it is \"as if\" the Agent manipulated the original Action / Observation.\n",
    "\n",
    "\n",
    "#### A note on the converter\n",
    "To use this converter, Agent must inherit the class [`grid2op.Agent.AgentWithConverter`](https://grid2op.readthedocs.io/en/latest/agent.html#grid2op.Agent.AgentWithConverter) and implement the following interface (showed here as an example):\n",
    "\n",
    "\n",
    "```python\n",
    "from grid2op.Agent import AgentWithConverter\n",
    "class MyAgent(AgentWithConverter):\n",
    "    def __init__(self, action_space, action_space_converter=None):\n",
    "        super(MyAgent, self).__init__(action_space=action_space, action_space_converter=action_space_converter)\n",
    "        # for example you can define here all the actions you will consider\n",
    "        self.my_actions = [action_space(),\n",
    "                           action_space({\"redispatching\": [0,+1]}),\n",
    "                           action_space({\"set_line_status\": [(0,-1)]}),\n",
    "                           action_space({\"change_bus\": {\"lines_or_id\": [12]}}),\n",
    "                          ...\n",
    "                          ]\n",
    "        # or load them from a file for example...\n",
    "        # self.my_action = np.load(\"my_action_pre_selected.npy\")\n",
    "        \n",
    "        # you can also in this agent load a neural network...\n",
    "        self.my_nn_model = model.load(\"my_saved_neural_network_weights.h5\")\n",
    "    \n",
    "    def convert_obs(self, observation):\n",
    "        \"\"\"\n",
    "        This method is used to convert the observation, represented as a class Observation in input\n",
    "        into a \"transformed_observation\" that will be manipulated by the agent\n",
    "        An example here will transform the observation into a numpy array.\n",
    "        \n",
    "        It is recommended to modify it to suit your needs.\n",
    "        \n",
    "        \"\"\"\n",
    "        return observation.to_vect()\n",
    "    \n",
    "    def convert_act(self, encoded_act):\n",
    "        \"\"\"\n",
    "        This method will take an \"encoded_act\" (for example a integer) into a valid grid2op action.\n",
    "        \"\"\"\n",
    "        if encoded_act < 0 or encoded_act > len(self.my_action):\n",
    "            raise RuntimeError(\"Invalid action with id {}\".format(encoded_act))\n",
    "        return self.my_actions[encoded_act]\n",
    "    \n",
    "    def my_act(self, transformed_observation, reward, done=False):\n",
    "        \"\"\"\n",
    "        This is the main function where you can take your decision.\n",
    "        \n",
    "        Instead of:\n",
    "        - calling \"act(observation, reward, done)\" you implement \n",
    "          \"my_act(transformed_observation, reward, done)\"\n",
    "        - this manipulates only \"transformed_observation\" fully flexible as you defined \"convert_obs\"\n",
    "        - and returns \"encoded_action\" that are then digest automatically by \n",
    "          \"convert_act(encoded_act)\" and to return valid actions.\n",
    "        \n",
    "        Here we suppose, as many dqn agent, that `my_nn_model` return a vector of size \n",
    "        nb_actions filled with number between 0 and 1 and we take the action given the highest score\n",
    "        \"\"\"\n",
    "        pred_score = self.my_nn_model.predict(transformed_observation, reward, done)\n",
    "        res = np.argmax(pred_score)\n",
    "        return res\n",
    "```\n",
    "And that's it. Nothing else to do, your agent is ready to learn to control powergrid using this only 3 functions.\n",
    "\n",
    "\n",
    "**NB** A few things are worth noting:\n",
    "- if you use an agent with converter, do not modify the method **act** but rather change the method **my_act** this is really important !\n",
    "- some automatic functions can compute the set of all possible actions, so no need to do \"self.my_actions = ...\" This was done as an example\n",
    "- if the converter is properly set up, you don't even need to modify \"convert_obs(self, observation)\" and \"convert_act(self, encoded_act)\" as this is already performed by the default implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.C) Using the code of the Agent and train it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Code of the agent\n",
    "\n",
    "Here we show the most interesting part (for this tutorial) part of the code that are implemented into the baseline. For a full description of the code, you can check [here](https://github.com/rte-france/l2rpn-baselines/tree/master/l2rpn_baselines/DoubleDuelingDQN)\n",
    "\n",
    "This is the `DoubleDuelingDQN_NN.py` file:\n",
    "\n",
    "```python\n",
    "import tensorflow.keras as tfk\n",
    "class DoubleDuelingDQN_NN(object):\n",
    "    \"\"\"Constructs the desired deep q learning network\"\"\"\n",
    "    def __init__(self,\n",
    "                 action_size,\n",
    "                 observation_size,                 \n",
    "                 HIDDEN_FOR_SIMPLICITY\n",
    "                ):\n",
    "        self.action_size = action_size\n",
    "        self.observation_size = observation_size\n",
    "        HIDDEN_FOR_SIMPLICITY\n",
    "\n",
    "    def construct_q_network(self):\n",
    "        \"\"\"\n",
    "        we showed this here to tell you it was exactly like any keras implementation\n",
    "        \"\"\"\n",
    "        input_layer = tfk.Input(shape = (self.observation_size * self.num_frames,), name=\"input_obs\")\n",
    "        lay1 = tfkl.Dense(self.observation_size * 2, name=\"fc_1\")(input_layer)\n",
    "        lay1 = tfka.relu(lay1, alpha=0.01) #leaky_relu\n",
    "        ...\n",
    "        HIDDEN_FOR_SIMPLICITY\n",
    "        ...\n",
    "        self.model = tfk.Model(...)\n",
    "\n",
    "    def random_move(self):\n",
    "        \"\"\"\n",
    "        Moves are encoded by a random number between 0 and the total number of actions.\n",
    "        Easy to do a random move isn't it ? :-)\n",
    "        \"\"\"\n",
    "        opt_policy = np.random.randint(0, self.action_size)\n",
    "        return opt_policy\n",
    "        \n",
    "    def predict_move(self, data):\n",
    "        \"\"\"\n",
    "        in this example we decided to show \n",
    "        \"\"\"\n",
    "        model_input = data.reshape(1, self.observation_size * self.num_frames)\n",
    "        q_actions = self.model.predict(model_input, batch_size = 1)     \n",
    "        opt_policy = np.argmax(q_actions)\n",
    "        return opt_policy, q_actions[0]\n",
    "```\n",
    "\n",
    "This is the `DoubleDuelingDQN.py` file:\n",
    "```python\n",
    "from grid2op.Agent import AgentWithConverter  # all converter agent should inherit this\n",
    "from grid2op.Converter import IdToAct  # this is the automatic converter to convert action given as ID (integer)\n",
    "# to valid grid2op action (in particular it is able to compute all actions).\n",
    "\n",
    "from l2rpn_baselines.DoubleDuelingDQN.DoubleDuelingDQN_NN import DoubleDuelingDQN_NN\n",
    "class DoubleDuelingDQN(AgentWithConverter):\n",
    "    def __init__(self,\n",
    "                 observation_space,\n",
    "                 action_space,\n",
    "                 HIDDEN_FOR_SIMPLICITY\n",
    "                ):\n",
    "        ...\n",
    "        HIDDEN_FOR_SIMPLICITY\n",
    "        ...\n",
    "        # Load network graph\n",
    "        self.Qmain = DoubleDuelingDQN_NN(self.action_size,\n",
    "                                         self.observation_size,\n",
    "                                         HIDDEN_FOR_SIMPLICITY)\n",
    "    ## Agent Interface\n",
    "    def convert_obs(self, observation):\n",
    "        # Made a custom version to normalize per attribute\n",
    "        # return observation.to_vect() - like object scaled accordingly\n",
    "        li_vect=  []\n",
    "        for el in observation.attr_list_vect:\n",
    "            v = observation._get_array_from_attr_name(el).astype(np.float)\n",
    "            v_fix = np.nan_to_num(v)\n",
    "            v_norm = np.linalg.norm(v_fix)\n",
    "            if v_norm > 1e8:\n",
    "                v_res = (v_fix / v_norm) * 10.0\n",
    "            else:\n",
    "                v_res = v_fix\n",
    "            li_vect.append(v_res)\n",
    "        return np.concatenate(li_vect)\n",
    "\n",
    "    def convert_act(self, action):\n",
    "        \"\"\"\n",
    "        calling the convert_act method of the base class.\n",
    "        This is not mandatory as this is the standard behaviour in OOP (object oriented programming)\n",
    "        \"\"\"\n",
    "        return super().convert_act(action)\n",
    "   \n",
    "    def my_act(self, state, reward, done=False):\n",
    "        \"\"\"\n",
    "        The complete implementation of the my_act function\n",
    "        \"\"\"\n",
    "        # Register current state to stacking buffer\n",
    "        self._save_current_frame(state)\n",
    "        # We need at least num frames to predict\n",
    "        if len(self.frames) < self.num_frames:\n",
    "            return 0 # Do nothing\n",
    "        # Infer with the last num_frames states\n",
    "        a, _ = self.Qmain.predict_move(np.array(self.frames))  # self.Qmain is of type 'DoubleDuelingDQN_NN' previously defined\n",
    "        return a\n",
    "```\n",
    "\n",
    "#### b) Training the model\n",
    "\n",
    "Now we can define the model (agent), and then train it.\n",
    "\n",
    "For that we will use the \"train\" method provided in the `l2rpn_baselines` repository.\n",
    "\n",
    "**NB** The code bellow can take a few minutes to run. It's training a Deep Reinforcement Learning Agent afterall. It this takes too long on your machine, you can always decrease the \"nb_frame\", and set it to 1000 for example. In this case, the Agent will probably not be really good.\n",
    "\n",
    "**NB** For a real Agent, it would take much longer to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benjamin/Documents/grid2op_dev/getting_started/grid2op/MakeEnv/Make.py:224: UserWarning: You are using a development environment. This environment is not intended for training agents.\n",
      "  warnings.warn(_MAKE_DEV_ENV_WARN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [0] -- Random [0.99]\n",
      "31 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "56 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "Survived [5] steps\n",
      "Total reward [3244.8480224609375]\n",
      "90 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "Survived [2] steps\n",
      "Total reward [1058.5562744140625]\n",
      "61 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "Survived [1] steps\n",
      "Total reward [-20.0]\n",
      "30 {'disc_lines': None, 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal'), Grid2OpException DivergingPowerFlow DivergingPowerFlow('Powerflow has diverged during computation.')], 'rewards': {}}\n",
      "Survived [3] steps\n",
      "Total reward [3209.047607421875]\n",
      "Survived [5] steps\n",
      "Total reward [5451.7432861328125]\n",
      "74 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "26 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "Survived [3] steps\n",
      "Total reward [1048.84814453125]\n",
      "41 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "48 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "52 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "        True, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "Survived [4] steps\n",
      "Total reward [1044.8372802734375]\n",
      "62 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "Survived [4] steps\n",
      "Total reward [3219.994140625]\n",
      "97 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "71 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "        True, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "71 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "Survived [7] steps\n",
      "Total reward [4348.4609375]\n",
      "30 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "88 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "65 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "Survived [7] steps\n",
      "Total reward [4198.181396484375]\n",
      "34 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "Survived [5] steps\n",
      "Total reward [4356.4600830078125]\n",
      "69 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "48 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "Survived [6] steps\n",
      "Total reward [4233.656982421875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "31 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "45 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "Survived [4] steps\n",
      "Total reward [1048.1866455078125]\n",
      "Survived [0] steps\n",
      "Total reward [-10.0]\n",
      "56 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "45 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "Survived [5] steps\n",
      "Total reward [3244.678955078125]\n",
      "Survived [6] steps\n",
      "Total reward [6464.846923828125]\n",
      "66 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "79 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "99 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "48 {'disc_lines': None, 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal'), Grid2OpException DivergingPowerFlow DivergingPowerFlow('Powerflow has diverged during computation.')], 'rewards': {}}\n",
      "Survived [11] steps\n",
      "Total reward [8716.031860351562]\n",
      "33 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "99 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "Survived [8] steps\n",
      "Total reward [6413.8114013671875]\n",
      "82 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "Survived [6] steps\n",
      "Total reward [5391.1849365234375]\n",
      "48 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "25 {'disc_lines': array([False, False, False, False, False,  True, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "Survived [6] steps\n",
      "Total reward [4290.169677734375]\n",
      "Survived [0] steps\n",
      "Total reward [-10.0]\n",
      "61 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "84 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "Survived [3] steps\n",
      "Total reward [1044.3125]\n",
      "67 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "69 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "37 {'disc_lines': None, 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal'), Grid2OpException DivergingPowerFlow DivergingPowerFlow('Powerflow has diverged during computation.')], 'rewards': {}}\n",
      "Survived [5] steps\n",
      "Total reward [3222.377685546875]\n",
      "42 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "Survived [1] steps\n",
      "Total reward [-20.0]\n",
      "37 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "Survived [7] steps\n",
      "Total reward [6494.12548828125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "28 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "Survived [4] steps\n",
      "Total reward [2132.1236572265625]\n",
      "88 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "77 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "Survived [8] steps\n",
      "Total reward [6605.39013671875]\n",
      "89 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "91 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "        True, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "Survived [7] steps\n",
      "Total reward [5380.2347412109375]\n",
      "33 {'disc_lines': None, 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal'), Grid2OpException DivergingPowerFlow DivergingPowerFlow('Powerflow has diverged during computation.')], 'rewards': {}}\n",
      "Survived [4] steps\n",
      "Total reward [4349.8685302734375]\n",
      "62 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "Survived [5] steps\n",
      "Total reward [4278.356689453125]\n",
      "Survived [1] steps\n",
      "Total reward [1078.072998046875]\n",
      "Survived [0] steps\n",
      "Total reward [-10.0]\n",
      "Survived [2] steps\n",
      "Total reward [2161.4300537109375]\n",
      "Survived [2] steps\n",
      "Total reward [2111.1539306640625]\n",
      "Survived [1] steps\n",
      "Total reward [1075.765625]\n",
      "Survived [1] steps\n",
      "Total reward [1068.5426025390625]\n",
      "Survived [6] steps\n",
      "Total reward [6484.6883544921875]\n",
      "Survived [0] steps\n",
      "Total reward [-10.0]\n",
      "Survived [1] steps\n",
      "Total reward [1074.8907470703125]\n",
      "77 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "96 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "Survived [8] steps\n",
      "Total reward [6476.5052490234375]\n",
      "90 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "Survived [4] steps\n",
      "Total reward [3239.291015625]\n",
      "36 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "        True, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "41 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "Survived [5] steps\n",
      "Total reward [3185.23388671875]\n",
      "39 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "52 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "        True, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "96 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "Survived [6] steps\n",
      "Total reward [3233.19091796875]\n",
      "100 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "Survived [2] steps\n",
      "Total reward [1058.6549072265625]\n",
      "91 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "69 {'disc_lines': None, 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal'), Grid2OpException DivergingPowerFlow DivergingPowerFlow('Powerflow has diverged during computation.')], 'rewards': {}}\n",
      "Survived [3] steps\n",
      "Total reward [2149.69970703125]\n",
      "23 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "Survived [5] steps\n",
      "Total reward [4269.8126220703125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "Survived [5] steps\n",
      "Total reward [4344.7838134765625]\n",
      "75 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "        True, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "38 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "86 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "85 {'disc_lines': array([False, False, False, False, False,  True, False, False, False,\n",
      "       False, False, False, False, False,  True, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "Survived [8] steps\n",
      "Total reward [4250.845458984375]\n",
      "96 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "86 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "60 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n",
      "45 {'disc_lines': array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False]), 'is_illegal': True, 'is_ambiguous': False, 'is_dispatching_illegal': False, 'is_illegal_reco': False, 'exception': [Grid2OpException IllegalAction IllegalAction('BaseAction illegal')], 'rewards': {}}\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "No gradient defined for operation 'IteratorGetNext' (op type: IteratorGetNext)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_GradientsHelper\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\u001b[0m\n\u001b[1;32m    606\u001b[0m           \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m             \u001b[0mgrad_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_gradient_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_gradient_function\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2496\u001b[0m     \u001b[0mop_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2497\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_gradient_registry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/registry.py\u001b[0m in \u001b[0;36mlookup\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       raise LookupError(\n\u001b[0m\u001b[1;32m     97\u001b[0m           \"%s registry has no entry for: %s\" % (self._name, name))\n",
      "\u001b[0;31mLookupError\u001b[0m: gradient registry has no entry for: IteratorGetNext",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e826c3db6ea8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# import the train function and train your agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0ml2rpn_baselines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDoubleDuelingDQN\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m train(env,\n\u001b[0m\u001b[1;32m      8\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test_agent\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/l2rpn-baselines/l2rpn_baselines/DoubleDuelingDQN/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(env, name, iterations, save_path, load_path, logs_path, num_pre_training_steps, num_frames, batch_size, learning_rate)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     agent.train(env,\n\u001b[0m\u001b[1;32m     92\u001b[0m                 \u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/l2rpn-baselines/l2rpn_baselines/DoubleDuelingDQN/DoubleDuelingDQN.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, env, iterations, save_path, num_pre_training_steps, logdir)\u001b[0m\n\u001b[1;32m    269\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mUPDATE_FREQ\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mper_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                     \u001b[0;31m# Perform training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mUPDATE_TARGET_SOFT_TAU\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/l2rpn-baselines/l2rpn_baselines/DoubleDuelingDQN/DoubleDuelingDQN.py\u001b[0m in \u001b[0;36m_batch_train\u001b[0;34m(self, training_step, step)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;31m# Batch train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQmain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;31m# Update PER buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/l2rpn-baselines/l2rpn_baselines/DoubleDuelingDQN/DoubleDuelingDQN_NN.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y_true, sample_weight)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# Get y_pred for batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;31m# Compute loss for each sample in the batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict_on_batch\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1439\u001b[0m       \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_batch_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m       \u001b[0mpredict_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_predict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1441\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1442\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    616\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1659\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \"\"\"\n\u001b[0;32m-> 1661\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1662\u001b[0m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mpossible_gradient_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m         executing_eagerly)\n\u001b[0;32m-> 1751\u001b[0;31m     \u001b[0mforward_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs_with_tangents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_backward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1752\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1753\u001b[0m       flat_outputs = forward_function.call(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1474\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m     \u001b[0;34m\"\"\"Builds or retrieves a forward function for this call.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1476\u001b[0;31m     forward_function = self._functions.forward(\n\u001b[0m\u001b[1;32m   1477\u001b[0m         self._inference_args, self._input_tangents)\n\u001b[1;32m   1478\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_tangents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inference_args, input_tangents)\u001b[0m\n\u001b[1;32m   1231\u001b[0m       (self._forward, self._forward_graph, self._backward,\n\u001b[1;32m   1232\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forwardprop_output_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_forwardprop_outputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1233\u001b[0;31m            self._forward_and_backward_functions(inference_args, input_tangents))\n\u001b[0m\u001b[1;32m   1234\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_forward_and_backward_functions\u001b[0;34m(self, inference_args, input_tangents)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     \"\"\"\n\u001b[1;32m   1383\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_inference_outputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m     return self._build_functions_for_outputs(\n\u001b[0m\u001b[1;32m   1385\u001b[0m         outputs, inference_args, input_tangents)\n\u001b[1;32m   1386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_build_functions_for_outputs\u001b[0;34m(self, outputs, inference_args, input_tangents)\u001b[0m\n\u001b[1;32m    937\u001b[0m             graph_placeholder(gradient_dtype, gradient_shape))\n\u001b[1;32m    938\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 939\u001b[0;31m         gradients_wrt_inputs = gradients_util._GradientsHelper(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    940\u001b[0m             \u001b[0mtrainable_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_GradientsHelper\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\u001b[0m\n\u001b[1;32m    619\u001b[0m               \u001b[0mgrad_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_grad_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m               raise LookupError(\n\u001b[0m\u001b[1;32m    622\u001b[0m                   \u001b[0;34m\"No gradient defined for operation '%s' (op type: %s)\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m                   (op.name, op.type))\n",
      "\u001b[0;31mLookupError\u001b[0m: No gradient defined for operation 'IteratorGetNext' (op type: IteratorGetNext)"
     ]
    }
   ],
   "source": [
    "# create an environment\n",
    "env = make(env_name, test=True)  \n",
    "# don't forget to set \"test=False\" (or remove it, as False is the default value) for \"real\" training\n",
    "\n",
    "# import the train function and train your agent\n",
    "from l2rpn_baselines.DoubleDuelingDQN import train\n",
    "train(env,\n",
    "      name=\"test_agent\",\n",
    "      iterations=max_iter,\n",
    "      save_path=\"saved_agent_DDDQN_{}\".format(max_iter),\n",
    "      load_path=None, # put something else if you want to reload an agent instead of creating a new one\n",
    "      logs_path=\"tf_logs_DDDQN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To see whether the agent learnt something during training: lets plot the learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(30,20))\n",
    "plt.plot(my_agent.deep_q.qvalue_evolution)\n",
    "plt.axhline(y=0, linewidth=3, color='red')\n",
    "plt.xlim(0, len(my_agent.deep_q.qvalue_evolution))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning curve shown above is really poor. It's because the agent has not been trained for a long time (and because it uses a very poor input data). If you train this agent for approximately 10-12 hours, using only the relative flow (`obs.rho` see the last section of this notebook for an example) you will get the following:\n",
    "\n",
    "![](img/trained_agent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III) Evaluating the Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, time to test this trained agent.\n",
    "\n",
    "To do that, we have multiple choices.\n",
    "\n",
    "Either we recode the \"DeepQAgent\" class to load the stored weights (that have been saved during trainig) when it is initialized (not covered in this notebook), or we can also directly specify the \"instance\" of the Agent to use in the Grid2Op Runner.\n",
    "\n",
    "To do that, it's fairly simple. First, you need to specify that you won't use the \"*agentClass*\" argument, by setting it to ``None``, and secondly you simply provide the agent to use in the *agentInstance* argument.\n",
    "\n",
    "**NB** If you don't do that, the Runner will be created (the constructor will raise an exception). And if you choose to use the \"*agentClass*\" argument, your agent will be reloaded from scratch. So **if it doesn't load the weights** it will behave as a non trained agent, unlikely to perform well on the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.A) Evaluate the Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have \"successfully\" trained our Agent, we will evaluating it. As opposed to the trainining, the evaluation is done classically using a standard Runner.\n",
    "\n",
    "Note that the Runner will use a \"scoring function\" that might be different from the \"reward function\" used during training. In our case, it's not. We use the `L2RPNReward` in both cases.\n",
    "\n",
    "In the code bellow, we commented on what can be different and what must be identical for training and evaluation of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grid2op.Runner import Runner\n",
    "from grid2op.Chronics import GridStateFromFileWithForecasts, Multifolder\n",
    "scoring_function = L2RPNReward\n",
    "\n",
    "dict_params = env.get_params_for_runner()\n",
    "dict_params[\"gridStateclass_kwargs\"][\"max_iter\"] =  max_iter\n",
    "# make a runner from an intialized environment\n",
    "runner = Runner(**dict_params, agentClass=None, agentInstance=my_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the Agent and save the results. As opposed to the multiple times we exposed the \"runner.run\" call, we never really dive into the \"path_save\" argument. This path allows you to save lots of information about your Agent behaviour. Please All the informations present are shown on the documentation [here](https://grid2op.readthedocs.io/en/latest/runner.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "path_save=\"trained_agent_log\"\n",
    "\n",
    "# delete the previous stored results\n",
    "if os.path.exists(path_save):\n",
    "    shutil.rmtree(path_save)\n",
    "\n",
    "# run the episode\n",
    "res = runner.run(nb_episode=2, path_save=path_save)\n",
    "print(\"The results for the trained agent are:\")\n",
    "for _, chron_name, cum_reward, nb_time_step, max_ts in res:\n",
    "    msg_tmp = \"\\tFor chronics located at {}\\n\".format(chron_name)\n",
    "    msg_tmp += \"\\t\\t - cumulative reward: {:.6f}\\n\".format(cum_reward)\n",
    "    msg_tmp += \"\\t\\t - number of time steps completed: {:.0f} / {:.0f}\".format(nb_time_step, max_ts)\n",
    "    print(msg_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.B) Inspect the Agent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please refer to the official document for more information about the content of the directory where the data are saved. Note that the saving of the information is triggered by the \"path_save\" argument sent to the \"runner.run\" function.\n",
    "\n",
    "Some information that will be present in this repository are:\n",
    "If enabled, the :class:`Runner` will save the information in a structured way. For each episode there will be a folder\n",
    "with:\n",
    "\n",
    "  - \"episode_meta.json\" that represents some meta information about:\n",
    "\n",
    "    - \"backend_type\": the name of the `grid2op.Backend` class used\n",
    "    - \"chronics_max_timestep\": the **maximum** number of timestep for the chronics used\n",
    "    - \"chronics_path\": the path where the temporal data (chronics) are located\n",
    "    - \"env_type\": the name of the `grid2op.Environment` class used.\n",
    "    - \"grid_path\": the path where the powergrid has been loaded from\n",
    "\n",
    "  - \"episode_times.json\": gives some information about the total time spend in multiple part of the runner, mainly the\n",
    "    `grid2op.Agent` (and especially its method `grid2op.Agent.act`) and amount of time spent in the\n",
    "    `grid2op.Environment`\n",
    "\n",
    "  - \"_parameters.json\": is a representation as json of a the `grid2op.Parameters` used for this episode\n",
    "  - \"rewards.npy\" is a numpy 1d array giving the rewards at each time step. We adopted the convention that the stored\n",
    "    reward at index `i` is the one observed by the agent at time `i` and **NOT** the reward sent by the\n",
    "    `grid2op.Environment` after the action has been implemented.\n",
    "  - \"exec_times.npy\" is a numpy 1d array giving the execution time of each time step of the episode\n",
    "  - \"actions.npy\" gives the actions that has been taken by the `grid2op.Agent`. At row `i` of \"actions.npy\" is a\n",
    "    vectorized representation of the action performed by the agent at timestep `i` *ie.* **after** having observed\n",
    "    the observation present at row `i` of \"observation.npy\" and the reward showed in row `i` of \"rewards.npy\".\n",
    "  - \"disc_lines.npy\" gives which lines have been disconnected during the simulation of the cascading failure at each\n",
    "    time step. The same convention as for \"rewards.npy\" has been adopted. This means that the powerlines are\n",
    "    disconnected when the `grid2op.Agent` takes the `grid2op.Action` at time step `i`.\n",
    "  - \"observations.npy\" is a numpy 2d array reprensenting the `grid2op.Observation` at the disposal of the\n",
    "    `grid2op.Agent` when he took his action.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can first look at the repository were the data are stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir(path_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there is only one folder there. It's named \"1\" because, in the original data, this came from the folder named \"1\" (the original data are located at \"/home/donnotben/.local/lib/python3.6/site-packages/grid2op/data/test_multi_chronics/\")\n",
    "\n",
    "If there were multiple episode, each episode would have it's own folder, with a name as resemblant as possible to the origin name of the data. This is done to ease the studying of the results.\n",
    "\n",
    "Now let's see what is inside this folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(os.path.join(path_save,\"0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can for example load the \"actions\" performed by the Agent, and have a look at them.\n",
    "\n",
    "To do that we will load the action array (represented as vector) and use the action_space to convert it back into valid action class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_actions = np.load(os.path.join(\"trained_agent_log\", \"0\", \"actions.npy\"))\n",
    "li_actions = []\n",
    "for i in range(all_actions.shape[0]):\n",
    "    try:\n",
    "        tmp = runner.env.action_space.from_vect(all_actions[i,:])\n",
    "        li_actions.append(tmp)\n",
    "    except:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows us to have a deeper look at the action, and their effect. Note that here, we used action that can only **set** the line status, so looking at their effect is pretty straightforward.\n",
    "\n",
    "Also, note that as oppose to \"change\", if a powerline is already connected, trying to **set** it as connected has absolutely no impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_disc = 0\n",
    "line_reco = 0\n",
    "for act in li_actions:\n",
    "    dict_ = act.as_dict()\n",
    "    if \"set_line_status\" in dict_:\n",
    "        line_reco +=  dict_[\"set_line_status\"][\"nb_connected\"]\n",
    "        line_disc +=  dict_[\"set_line_status\"][\"nb_disconnected\"]\n",
    "line_reco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As wa can see for our event, the agent always try to reconnect a powerline. As all lines are alway reconnected, this Agent does basically nothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do the same kind of post analysis for the observation, even though here, as the observations come from files, it's probably not particularly intersting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_observations = np.load(os.path.join(\"trained_agent_log\", \"0\", \"observations.npy\"))\n",
    "li_observations = []\n",
    "nb_real_disc = 0\n",
    "for i in range(all_observations.shape[0]):\n",
    "    try:\n",
    "        tmp = runner.env.observation_space.from_vect(all_observations[i,:])\n",
    "        li_observations.append(tmp)\n",
    "        nb_real_disc += (np.sum(tmp.line_status == False))\n",
    "    except:\n",
    "        break\n",
    "nb_real_disc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the type of action the agent did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_count = {}\n",
    "for act in li_actions:\n",
    "    act_as_vect = tuple(act.to_vect())\n",
    "    if not act_as_vect in actions_count:\n",
    "        actions_count[act_as_vect] = 0\n",
    "    actions_count[act_as_vect] += 1\n",
    "print(\"Number of different actions played: {}\".format(len(actions_count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(runner.env.action_space.from_vect(np.array(list(actions_count.keys())[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent only did one action (note that this number can really vary on the number of training step and the . This is not really good, the agent didn't learn anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV) Improve your Agent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw, the agent we develop was not really interesting. To improve it, we could think about:\n",
    "\n",
    "- a better encoding of the observation. For now everything is fed to the neural network, without any normalization of any kind. This is a real problem for learning algorithm.\n",
    "- a better neural network architecture (as said, we didn't pay any attention to it in our model)\n",
    "- train it for a longer time\n",
    "- adapt the learning rate and all the meta parameters of the learning algorithm.\n",
    "- etc.\n",
    "\n",
    "In this notebook, we will focus on changing the observation representation, by only feeding the agent only some informations.\n",
    "\n",
    "To do so, the only modification we need to do is to modify the way the observation are converted. So the \"*convert_obs*\" method, and that is it. Nothing else need to be changed. Here for example we could think of only using the flow ratio (*i.e.,* the current flows divided by the thermal limits) as part of the observation (named rho) instead of feeding the whole observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQAgent_Improved(DeepQAgent):\n",
    "    def __init__(self, action_space, mode=\"DDQN\"):\n",
    "        DeepQAgent.__init__(self, action_space, mode=mode)\n",
    "    \n",
    "    def convert_obs(self, observation):\n",
    "        return observation.rho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can reuse exactly the same code to train it :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_agent = DeepQAgent_Improved(env.action_space, mode=\"DDQN\")\n",
    "trainer = TrainAgent(agent=my_agent, env=env)\n",
    "trainer.train(nb_frame)\n",
    "\n",
    "plt.figure(figsize=(30,20))\n",
    "plt.plot(my_agent.deep_q.qvalue_evolution)\n",
    "plt.axhline(y=0, linewidth=3, color='red')\n",
    "_ = plt.xlim(0, len(my_agent.deep_q.qvalue_evolution))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
