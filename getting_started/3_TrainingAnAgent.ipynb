{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actions as vector, and RL agent training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Disclaimer***: This file referenced some files in other directories. In order to have working cross referencing it's recommended to start the notebook server from the root directory (`Grid2Op`) of the package and __not__ in the `getting_started` sub directory:\n",
    "```bash\n",
    "cd Grid2Op\n",
    "jupyter notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***NB*** For more information about how to use the package, a general help can be built locally (provided that sphinx is installed on the machine) with:\n",
    "```bash\n",
    "cd Grid2Op\n",
    "make html\n",
    "```\n",
    "from the top directory of the package (usually `Grid2Op`).\n",
    "\n",
    "Once build, the help can be access from [here](../documentation/html/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is recommended to have a look at the [0_basic_functionalities](0_basic_functionalities.ipynb), [1_Observation_Agents](1_Observation_Agents.ipynb) and [2_Action_GridManipulation](2_Action_GridManipulation.ipynb) notebooks before getting into this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objectives**\n",
    "\n",
    "In this notebook we will expose :\n",
    "* how to use the \"vectorized\" verions of Observations and Actions\n",
    "* how to train a (stupid) Agent using reinforcement learning.\n",
    "* how to inspect (rapidly) the action taken by the Agent\n",
    "\n",
    "**NB** for this tutorial we train an Agent inspired from this blog post: [deep-reinforcement-learning-tutorial-with-open-ai-gym](https://towardsdatascience.com/deep-reinforcement-learning-tutorial-with-open-ai-gym-c0de4471f368). Many other different reinforcement learning tutorial exist. The code showed in this notebook has no pretention except to demonstrate how to use Grid2Op functionality to train a Deep Reinforcement learning Agent and inspect its behaviour. There are absolutely nothing implied about the performance, training strategy, type of Agent etc, meta parameters etc. All of them are purely \"random\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import grid2op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"my_id_menu_nb\">run previous cell, wait for 2 seconds</div>\n",
       "<script>\n",
       "function repeat_indent_string(n){\n",
       "    var a = \"\" ;\n",
       "    for ( ; n > 0 ; --n)\n",
       "        a += \"    \";\n",
       "    return a;\n",
       "}\n",
       "// look up into all sections and builds an automated menu //\n",
       "var update_menu_string = function(begin, lfirst, llast, sformat, send, keep_item, begin_format, end_format) {\n",
       "    var anchors = document.getElementsByClassName(\"section\");\n",
       "    if (anchors.length == 0) {\n",
       "        anchors = document.getElementsByClassName(\"text_cell_render rendered_html\");\n",
       "    }\n",
       "    var i,t;\n",
       "    var text_menu = begin;\n",
       "    var text_memo = \"<pre>\\nlength:\" + anchors.length + \"\\n\";\n",
       "    var ind = \"\";\n",
       "    var memo_level = 1;\n",
       "    var href;\n",
       "    var tags = [];\n",
       "    var main_item = 0;\n",
       "    var format_open = 0;\n",
       "    for (i = 0; i <= llast; i++)\n",
       "        tags.push(\"h\" + i);\n",
       "\n",
       "    for (i = 0; i < anchors.length; i++) {\n",
       "        text_memo += \"**\" + anchors[i].id + \"--\\n\";\n",
       "\n",
       "        var child = null;\n",
       "        for(t = 0; t < tags.length; t++) {\n",
       "            var r = anchors[i].getElementsByTagName(tags[t]);\n",
       "            if (r.length > 0) {\n",
       "child = r[0];\n",
       "break;\n",
       "            }\n",
       "        }\n",
       "        if (child == null) {\n",
       "            text_memo += \"null\\n\";\n",
       "            continue;\n",
       "        }\n",
       "        if (anchors[i].hasAttribute(\"id\")) {\n",
       "            // when converted in RST\n",
       "            href = anchors[i].id;\n",
       "            text_memo += \"#1-\" + href;\n",
       "            // passer à child suivant (le chercher)\n",
       "        }\n",
       "        else if (child.hasAttribute(\"id\")) {\n",
       "            // in a notebook\n",
       "            href = child.id;\n",
       "            text_memo += \"#2-\" + href;\n",
       "        }\n",
       "        else {\n",
       "            text_memo += \"#3-\" + \"*\" + \"\\n\";\n",
       "            continue;\n",
       "        }\n",
       "        var title = child.textContent;\n",
       "        var level = parseInt(child.tagName.substring(1,2));\n",
       "\n",
       "        text_memo += \"--\" + level + \"?\" + lfirst + \"--\" + title + \"\\n\";\n",
       "\n",
       "        if ((level < lfirst) || (level > llast)) {\n",
       "            continue ;\n",
       "        }\n",
       "        if (title.endsWith('¶')) {\n",
       "            title = title.substring(0,title.length-1).replace(\"<\", \"&lt;\")\n",
       "         .replace(\">\", \"&gt;\").replace(\"&\", \"&amp;\");\n",
       "        }\n",
       "        if (title.length == 0) {\n",
       "            continue;\n",
       "        }\n",
       "\n",
       "        while (level < memo_level) {\n",
       "            text_menu += end_format + \"</ul>\\n\";\n",
       "            format_open -= 1;\n",
       "            memo_level -= 1;\n",
       "        }\n",
       "        if (level == lfirst) {\n",
       "            main_item += 1;\n",
       "        }\n",
       "        if (keep_item != -1 && main_item != keep_item + 1) {\n",
       "            // alert(main_item + \" - \" + level + \" - \" + keep_item);\n",
       "            continue;\n",
       "        }\n",
       "        while (level > memo_level) {\n",
       "            text_menu += \"<ul>\\n\";\n",
       "            memo_level += 1;\n",
       "        }\n",
       "        text_menu += repeat_indent_string(level-2);\n",
       "        text_menu += begin_format + sformat.replace(\"__HREF__\", href).replace(\"__TITLE__\", title);\n",
       "        format_open += 1;\n",
       "    }\n",
       "    while (1 < memo_level) {\n",
       "        text_menu += end_format + \"</ul>\\n\";\n",
       "        memo_level -= 1;\n",
       "        format_open -= 1;\n",
       "    }\n",
       "    text_menu += send;\n",
       "    //text_menu += \"\\n\" + text_memo;\n",
       "\n",
       "    while (format_open > 0) {\n",
       "        text_menu += end_format;\n",
       "        format_open -= 1;\n",
       "    }\n",
       "    return text_menu;\n",
       "};\n",
       "var update_menu = function() {\n",
       "    var sbegin = \"\";\n",
       "    var sformat = '<a href=\"#__HREF__\">__TITLE__</a>';\n",
       "    var send = \"\";\n",
       "    var begin_format = '<li>';\n",
       "    var end_format = '</li>';\n",
       "    var keep_item = -1;\n",
       "    var text_menu = update_menu_string(sbegin, 2, 4, sformat, send, keep_item,\n",
       "       begin_format, end_format);\n",
       "    var menu = document.getElementById(\"my_id_menu_nb\");\n",
       "    menu.innerHTML=text_menu;\n",
       "};\n",
       "window.setTimeout(update_menu,2000);\n",
       "            </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = None\n",
    "try:\n",
    "    from jyquickhelper import add_notebook_menu\n",
    "    res = add_notebook_menu()\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Impossible to automatically add a menu / table of content to this notebook.\\nYou can download \\\"jyquickhelper\\\" package with: \\n\\\"pip install jyquickhelper\\\"\")\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I) Manipulating vectors instead of class in Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid2op package has been built with an \"object oriented\" perspective: almost everything is encapsulated in a dedicated `class`. This allows for more customization of the plateform.\n",
    "\n",
    "The downside of this approach is that machine learning method, and especially deep learning, often prefers to deal with vectors rather than with `complex` objects. \n",
    "\n",
    "To have the best of both worlds, we provided a `MLAgent` class that handles the convertion to / from these classes to numpy vector. In this notebook, we will see how this class can be used and overidden to develop a Agent that learns how to perform action on the grid. By default, this class does nothing, and it's possible (and encouraged) to override the `MLAgent._ml_act` method to build smarter Agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the DoNothing agent are:\n",
      "\tFor chronics located at /home/donnotben/.local/lib/python3.6/site-packages/grid2op/data/test_multi_chronics/1\n",
      "\t\t - cumulative reward: 5739.951023\n",
      "\t\t - number of time steps completed: 287 / 287\n"
     ]
    }
   ],
   "source": [
    "# import the usefull class\n",
    "import numpy as np\n",
    "\n",
    "from grid2op.Runner import Runner\n",
    "from grid2op.ChronicsHandler import Multifolder, GridStateFromFileWithForecasts\n",
    "from grid2op.Agent import MLAgent\n",
    "from grid2op.Reward import L2RPNReward\n",
    "from grid2op.Action import PowerLineSet\n",
    "# make a runner\n",
    "runner = Runner(init_grid_path=grid2op.CASE_14_FILE,\n",
    "               path_chron=grid2op.CHRONICS_MLUTIEPISODE,\n",
    "               gridStateclass=Multifolder,\n",
    "               gridStateclass_kwargs={\"gridvalueClass\": GridStateFromFileWithForecasts},\n",
    "               names_chronics_to_backend = grid2op.NAMES_CHRONICS_TO_BACKEND,\n",
    "                agentClass=MLAgent,\n",
    "               rewardClass=L2RPNReward,\n",
    "               actionClass=PowerLineSet)\n",
    "# initialize it\n",
    "res = runner.run(nb_episode=1)\n",
    "print(\"The results for the DoNothing agent are:\")\n",
    "for chron_name, cum_reward, nb_time_step, max_ts in res:\n",
    "    msg_tmp = \"\\tFor chronics located at {}\\n\".format(chron_name)\n",
    "    msg_tmp += \"\\t\\t - cumulative reward: {:.6f}\\n\".format(cum_reward)\n",
    "    msg_tmp += \"\\t\\t - number of time steps completed: {:.0f} / {:.0f}\".format(nb_time_step, max_ts)\n",
    "    print(msg_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that is it. It is as simple as changing the \"agentClass\". Now we will provide an example on how this class can be overidden to train a \"real\" Agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II) Training an Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial, we will expose to built a Q-learning Agent. Most of the code (except the neural network architecture) are inspired from this blog post: [https://towardsdatascience.com/deep-reinforcement-learning-tutorial-with-open-ai-gym-c0de4471f368](https://towardsdatascience.com/deep-reinforcement-learning-tutorial-with-open-ai-gym-c0de4471f368).\n",
    "\n",
    "**Requirements** This notebook require to have `keras` installed on your machine.\n",
    "\n",
    "The Agent we will train will:\n",
    "* only act on the topology\n",
    "* only try to set the status of the powerline (he won't try to modify the \"change\" them, nor try to perform any kind of bus splitting merging)\n",
    "\n",
    "This is achieved by passing the \"*PowerLineSet*\" class in the actionClass of the Runner. For more information, one can consult the [PowerLineSet](../documentation/html/action.html#grid2op.Action.PowerLineSet) help page (if built locally, which is recommended) or the [PowerLineSet](../grid2op/Action.py) class definition in the [Action.py](../grid2op/Action.py) file.\n",
    "\n",
    "Note that this agent is unlikely to perform well in a L2RPN competition, as it uses only a small subset of the action. It is exposed here as an example.\n",
    "\n",
    "Also, note that we use a specific class of `Action` in this notebook. This is unlikely this class will be use in a competition. The exercise in this notebook is then purely for demonstrating \"how to\".\n",
    "\n",
    "As always in these notebook, we will use the `case14_fromfile` Environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.A) Defining some \"helpers\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type of Agent were are using require a bit of set up, independantly of Grid2Op. We will reuse the code showed in \n",
    "[https://towardsdatascience.com/deep-reinforcement-learning-tutorial-with-open-ai-gym-c0de4471f368](https://towardsdatascience.com/deep-reinforcement-learning-tutorial-with-open-ai-gym-c0de4471f368) and in [Reinforcement-Learning-Tutorial](https://github.com/abhinavsagar/Reinforcement-Learning-Tutorial) from Abhinav Sagar code under a *MIT license* found here: [MIT License](https://github.com/abhinavsagar/Reinforcement-Learning-Tutorial/blob/master/LICENSE).\n",
    "\n",
    "This first section is here to define these classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first let's import the necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    import keras\n",
    "    import keras.backend as K\n",
    "    from keras.models import load_model, Sequential, Model\n",
    "    from keras.optimizers import Adam\n",
    "    from keras.layers.core import Activation, Dropout, Flatten, Dense\n",
    "    from keras.layers import Input, Lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Replay buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " First we define a \"replay buffer\" necessary to train the Agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit Abhinav Sagar: \n",
    "# https://github.com/abhinavsagar/Reinforcement-Learning-Tutorial\n",
    "# Code under MIT license, available at:\n",
    "# https://github.com/abhinavsagar/Reinforcement-Learning-Tutorial/blob/master/LICENSE\n",
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Constructs a buffer object that stores the past moves\n",
    "    and samples a set of subsamples\"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.count = 0\n",
    "        self.buffer = deque()\n",
    "\n",
    "    def add(self, s, a, r, d, s2):\n",
    "        \"\"\"Add an experience to the buffer\"\"\"\n",
    "        # S represents current state, a is action,\n",
    "        # r is reward, d is whether it is the end, \n",
    "        # and s2 is next state\n",
    "        experience = (s, a, r, d, s2)\n",
    "        if self.count < self.buffer_size:\n",
    "            self.buffer.append(experience)\n",
    "            self.count += 1\n",
    "        else:\n",
    "            self.buffer.popleft()\n",
    "            self.buffer.append(experience)\n",
    "\n",
    "    def size(self):\n",
    "        return self.count\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Samples a total of elements equal to batch_size from buffer\n",
    "        if buffer contains enough elements. Otherwise return all elements\"\"\"\n",
    "\n",
    "        batch = []\n",
    "\n",
    "        if self.count < batch_size:\n",
    "            batch = random.sample(self.buffer, self.count)\n",
    "        else:\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        # Maps each experience in batch in batches of states, actions, rewards\n",
    "        # and new states\n",
    "        s_batch, a_batch, r_batch, d_batch, s2_batch = list(map(np.array, list(zip(*batch))))\n",
    "\n",
    "        return s_batch, a_batch, r_batch, d_batch, s2_batch\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "        self.count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Meta parameters of the methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we re-use the default parameters, note that these can be optimized. Nothing has been changed for this example.\n",
    "\n",
    "For more information about them, please refer to the blog post of Abhinav Sagar [available here](https://towardsdatascience.com/deep-reinforcement-learning-tutorial-with-open-ai-gym-c0de4471f368)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DECAY_RATE = 0.99\n",
    "BUFFER_SIZE = 40000\n",
    "MINIBATCH_SIZE = 64\n",
    "TOT_FRAME = 3000000\n",
    "EPSILON_DECAY = 1000000\n",
    "MIN_OBSERVATION = 1000 #5000\n",
    "FINAL_EPSILON = 1/300  # have on average 1 random action per scenario of approx 287 time steps\n",
    "INITIAL_EPSILON = 0.1\n",
    "TAU = 0.01\n",
    "# Number of frames to \"throw\" into network\n",
    "NUM_FRAMES = 1 ## this has been changed compared to the original implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.B) Adapatation of the inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original code, the models were used to play an Atari game and the inputs were images. For our system, the inputs are \"Observation\" converted as vector.\n",
    "\n",
    "For a more detailed description of the code used, please check:\n",
    "* [https://towardsdatascience.com/deep-reinforcement-learning-tutorial-with-open-ai-gym-c0de4471f368](https://towardsdatascience.com/deep-reinforcement-learning-tutorial-with-open-ai-gym-c0de4471f368)\n",
    "* and [Reinforcement-Learning-Tutorial](https://github.com/abhinavsagar/Reinforcement-Learning-Tutorial)\n",
    "\n",
    "\n",
    "This is why we adapted the original code from Abhinav Sagar:\n",
    "* We replaced convolutional layers with fully connected (dense) layers\n",
    "* We made sure not to look at all the observations, but rather at only some part of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) extracting relevant information of observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we extract relevant information about the dimension of the observation space, and the action space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = grid2op.make(action_class=PowerLineSet)\n",
    "observation_size_init = env.observation_space.size()\n",
    "topo_vect_size = env.observation_space.dim_topo\n",
    "action_size = env.action_space.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Agent we will train will be able to act one only one powerline, and to either reconnect it, or disconnect it. It has a choice then bewteen 2 * `number of powerlines`.\n",
    "\n",
    "To train our agent more easily, we will only use part of the ***observation space***. We will not inform the agent about the state of the loads or generators, nor about the flows on the powerline, or the topology vector. Only the part of the observation that will concerned:\n",
    "* the relative powerflow $\\rho$ (current flow divided by thermal limit)\n",
    "* the powerline status (1 powerline is connected, 0 it's disconnected)\n",
    "\n",
    "The ***action space*** will be represented by a \"one hot\" vector, as followed:\n",
    "* if the first component (index 0) is set to 1, then the Agent does nothing\n",
    "* if the component equal to \"1\" has its index `i` between 1 and the `number of powerlines`, it will consist in reconnecting powerline `i-1`\n",
    "* if the component equal to \"1\" has its index `i` between the `number of powerlines` + 1 and twice \"the `number of powerlines`\", it will consist in disconnecting powerline `i-nb_powerline-1`\n",
    "\n",
    "It has then a dimension of 2 * `number of powerlines` + 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the subspace (represented as index) of the action space used by the agent.\n",
    "obs_to_model = np.full(observation_size_init, fill_value=False, dtype=np.bool)\n",
    "obs_to_model[(-3*action_size-topo_vect_size):(-action_size-topo_vect_size)] = True\n",
    "observation_size = np.sum(obs_to_model)\n",
    "\n",
    "# define the size of the action space\n",
    "NUM_ACTIONS = 2* action_size + 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Code the neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code of the neural networks used have been impacted only slightly to adapt them to our problem. The biggest changes comes from removing the convolutional layers, as well as adapting the input and output size.\n",
    "\n",
    "For each of the method bellow, we specify what have been adapted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit Abhinav Sagar: \n",
    "# https://github.com/abhinavsagar/Reinforcement-Learning-Tutorial\n",
    "# Code under MIT license, available at:\n",
    "# https://github.com/abhinavsagar/Reinforcement-Learning-Tutorial/blob/master/LICENSE\n",
    "\n",
    "class DeepQ(object):\n",
    "    \"\"\"Constructs the desired deep q learning network\"\"\"\n",
    "    def __init__(self, action_size, lr=0.00001):\n",
    "        # It is not modified from  Abhinav Sagar's code, except for adding the possibility to change the learning rate\n",
    "        # in parameter is also present the size of the action space\n",
    "        # (it used to be a global variable in the original code)\n",
    "        self.action_size = action_size\n",
    "        self.model = None\n",
    "        self.target_model = None\n",
    "        self.lr_ = lr\n",
    "        self.construct_q_network()\n",
    "    \n",
    "    def construct_q_network(self):\n",
    "        # replacement of the Convolution layers by Dense layers, and change the size of the input space and output space\n",
    "        \n",
    "        # Uses the network architecture found in DeepMind paper\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(observation_size*NUM_FRAMES))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Dense(observation_size))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Dense(observation_size))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Dense(2*NUM_ACTIONS))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Dense(NUM_ACTIONS))\n",
    "        self.model.compile(loss='mse', optimizer=Adam(lr=self.lr_))\n",
    "\n",
    "        # Creates a target network as described in DeepMind paper\n",
    "        self.target_model = Sequential()\n",
    "        self.target_model.add(Dense(observation_size*NUM_FRAMES))\n",
    "        self.target_model.add(Activation('relu'))\n",
    "        self.target_model.add(Dense(observation_size))\n",
    "        self.target_model.add(Activation('relu'))\n",
    "        self.target_model.add(Dense(observation_size))\n",
    "        self.target_model.add(Activation('relu'))\n",
    "        self.target_model.add(Dense(2*NUM_ACTIONS))\n",
    "        self.target_model.add(Activation('relu'))\n",
    "        self.target_model.add(Dense(NUM_ACTIONS))\n",
    "        self.target_model.compile(loss='mse', optimizer=Adam(lr=self.lr_))\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "    def predict_movement(self, data, epsilon):\n",
    "        \"\"\"Predict movement of game controler where is epsilon\n",
    "        probability randomly move.\"\"\"\n",
    "        # nothing has changed from the original implementation\n",
    "        rand_val = np.random.random()\n",
    "        q_actions = self.model.predict(data.reshape(1, observation_size*NUM_FRAMES), batch_size = 1)\n",
    "        \n",
    "        if rand_val < epsilon:\n",
    "            opt_policy = np.random.randint(0, NUM_ACTIONS)\n",
    "        else:\n",
    "            opt_policy = np.argmax(np.abs(q_actions))\n",
    "        return opt_policy, q_actions[0, opt_policy]\n",
    "\n",
    "    def train(self, s_batch, a_batch, r_batch, d_batch, s2_batch, observation_num):\n",
    "        \"\"\"Trains network to fit given parameters\"\"\"\n",
    "        # nothing has changed from the original implementation, except for changing the input dimension 'reshape'\n",
    "        batch_size = s_batch.shape[0]\n",
    "        targets = np.zeros((batch_size, NUM_ACTIONS))\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            targets[i] = self.model.predict(s_batch[i].reshape(1, observation_size*NUM_FRAMES), batch_size = 1)\n",
    "            fut_action = self.target_model.predict(s2_batch[i].reshape(1, observation_size*NUM_FRAMES), batch_size = 1)\n",
    "            targets[i, a_batch[i]] = r_batch[i]\n",
    "            if d_batch[i] == False:\n",
    "                targets[i, a_batch[i]] += DECAY_RATE * np.max(fut_action)\n",
    "        loss = self.model.train_on_batch(s_batch, targets)\n",
    "        # Print the loss every 100 iterations.\n",
    "        if observation_num % 100 == 0:\n",
    "            print(\"We had a loss equal to \", loss)\n",
    "\n",
    "    def save_network(self, path):\n",
    "        # Saves model at specified path as h5 file\n",
    "        # nothing has changed\n",
    "        self.model.save(path)\n",
    "        print(\"Successfully saved network.\")\n",
    "\n",
    "    def load_network(self, path):\n",
    "        # nothing has changed\n",
    "        self.model = load_model(path)\n",
    "        print(\"Succesfully loaded network.\")\n",
    "\n",
    "    def target_train(self):\n",
    "        # nothing has changed from the original implementation\n",
    "        model_weights = self.model.get_weights()\n",
    "        target_model_weights = self.target_model.get_weights()\n",
    "        for i in range(len(model_weights)):\n",
    "            target_model_weights[i] = TAU * model_weights[i] + (1 - TAU) * target_model_weights[i]\n",
    "        self.target_model.set_weights(target_model_weights)\n",
    "        \n",
    "class DuelQ(object):\n",
    "    \"\"\"Constructs the desired deep q learning network\"\"\"\n",
    "    def __init__(self, action_size, lr=0.00001):\n",
    "        # It is not modified from  Abhinav Sagar's code, except for adding the possibility to change the learning rate\n",
    "        # in parameter is also present the size of the action space\n",
    "        # (it used to be a global variable in the original code)\n",
    "        self.action_size = action_size\n",
    "        self.lr_ = lr\n",
    "        self.model = None\n",
    "        self.construct_q_network()\n",
    "\n",
    "    def construct_q_network(self):\n",
    "        # Uses the network architecture found in DeepMind paper\n",
    "        # The inputs and outputs size have changed, as well as replacing the convolution by dense layers.\n",
    "        self.model = Sequential()\n",
    "        \n",
    "        input_layer = Input(shape = (observation_size*NUM_FRAMES,))\n",
    "        lay1 = Dense(observation_size*NUM_FRAMES)(input_layer)\n",
    "        lay1 = Activation('relu')(lay1)\n",
    "        \n",
    "        lay2 = Dense(observation_size)(lay1)\n",
    "        lay2 = Activation('relu')(lay2)\n",
    "        \n",
    "        lay3 = Dense(2*NUM_ACTIONS)(lay2)\n",
    "        lay3 = Activation('relu')(lay3)\n",
    "        \n",
    "        fc1 = Dense(NUM_ACTIONS)(lay3)\n",
    "        advantage = Dense(NUM_ACTIONS)(fc1)\n",
    "        fc2 = Dense(NUM_ACTIONS)(lay3)\n",
    "        value = Dense(1)(fc2)\n",
    "        \n",
    "        meaner = Lambda(lambda x: K.mean(x, axis=1) )\n",
    "        mn_ = meaner(advantage)  \n",
    "        tmp = keras.layers.subtract([advantage, mn_])  # keras doesn't like this part...\n",
    "        policy = keras.layers.add([tmp, value])\n",
    "\n",
    "        self.model = Model(inputs=[input_layer], outputs=[policy])\n",
    "        self.model.compile(loss='mse', optimizer=Adam(lr=self.lr_))\n",
    "\n",
    "        self.target_model = Model(inputs=[input_layer], outputs=[policy])\n",
    "        self.target_model.compile(loss='mse', optimizer=Adam(lr=self.lr_))\n",
    "        print(\"Successfully constructed networks.\")\n",
    "    \n",
    "    def predict_movement(self, data, epsilon):\n",
    "        \"\"\"Predict movement of game controler where is epsilon\n",
    "        probability randomly move.\"\"\"\n",
    "        # only changes lie in adapting the input shape\n",
    "        q_actions = self.model.predict(data.reshape(1, observation_size*NUM_FRAMES), batch_size = 1)\n",
    "        opt_policy = np.argmax(q_actions)\n",
    "        rand_val = np.random.random()\n",
    "        if rand_val < epsilon:\n",
    "            opt_policy = np.random.randint(0, NUM_ACTIONS)\n",
    "        return opt_policy, q_actions[0, opt_policy]\n",
    "\n",
    "    def train(self, s_batch, a_batch, r_batch, d_batch, s2_batch, observation_num):\n",
    "        \"\"\"Trains network to fit given parameters\"\"\"\n",
    "        # nothing has changed except adapting the input shapes\n",
    "        batch_size = s_batch.shape[0]\n",
    "        targets = np.zeros((batch_size, NUM_ACTIONS))\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            targets[i] = self.model.predict(s_batch[i].reshape(1, observation_size*NUM_FRAMES), batch_size = 1)\n",
    "            fut_action = self.target_model.predict(s2_batch[i].reshape(1, observation_size*NUM_FRAMES), batch_size = 1)\n",
    "            targets[i, a_batch[i]] = r_batch[i]\n",
    "            if d_batch[i] == False:\n",
    "                targets[i, a_batch[i]] += DECAY_RATE * np.max(fut_action)\n",
    "\n",
    "        loss = self.model.train_on_batch(s_batch, targets)\n",
    "\n",
    "        # Print the loss every 100 iterations.\n",
    "        if observation_num % 100 == 0:\n",
    "            print(\"We had a loss equal to \", loss)\n",
    "\n",
    "    def save_network(self, path):\n",
    "        # Saves model at specified path as h5 file\n",
    "        # nothing has changed\n",
    "        self.model.save(path)\n",
    "        print(\"Successfully saved network.\")\n",
    "\n",
    "    def load_network(self, path):\n",
    "        # nothing has changed\n",
    "        self.model.load_weights(path)\n",
    "        self.target_model.load_weights(path)\n",
    "        print(\"Succesfully loaded network.\")\n",
    "\n",
    "    def target_train(self):\n",
    "        # nothing has changed\n",
    "        model_weights = self.model.get_weights()\n",
    "        self.target_model.set_weights(model_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.C) Making the code of the Agent and train it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the \"reference\" article [https://towardsdatascience.com/deep-reinforcement-learning-tutorial-with-open-ai-gym-c0de4471f368](https://towardsdatascience.com/deep-reinforcement-learning-tutorial-with-open-ai-gym-c0de4471f368), the author Abhinav Sagar made a dedicated environment based on SpaceInvader in the gym repository. We proceed here on a similar way, but with a the grid2op environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Adapated code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first expose the modify code, for each function we highlight what has changed and what has not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit Abhinav Sagar: \n",
    "# https://github.com/abhinavsagar/Reinforcement-Learning-Tutorial\n",
    "# Code under MIT license, available at:\n",
    "# https://github.com/abhinavsagar/Reinforcement-Learning-Tutorial/blob/master/LICENSE\n",
    "\n",
    "from grid2op.Parameters import Parameters\n",
    "from grid2op.Action import PowerLineSet\n",
    "from grid2op.Reward import L2RPNReward\n",
    "from grid2op.Agent import MLAgent\n",
    "import pdb\n",
    "\n",
    "class DeepQAgent(MLAgent):\n",
    "    # first change: An Agent must derived from grid2op.Agent (in this case MLAgent, because we manipulate vector instead\n",
    "    # of classes)\n",
    "    \n",
    "    def __init__(self, action_space, mode=\"DDQN\", reward_fun=L2RPNReward):\n",
    "        # this function has been adapted.\n",
    "        # no environment is created here, but it's rather created in \"train\" method.\n",
    "        \n",
    "        # to built a MLAgent, we need an action_space. No problem, we add it in the constructor.\n",
    "        MLAgent.__init__(self, action_space)\n",
    "        \n",
    "        # easier to access\n",
    "        self.action_space = action_space\n",
    "        self.action_size = action_space.size()\n",
    "        self.do_nothing_act = action_space({})\n",
    "        \n",
    "        # this is the reward function used to train the Agent. It can be change when building it.\n",
    "        # note that a \"reward function\" must be a class, that is a subclass of grid2op.Reward\n",
    "        self.reward_fun = reward_fun\n",
    "        \n",
    "        # the scales of my inputs varies. I add a \"scaling vector\"\n",
    "        self.scale_vect = np.ones(observation_size)\n",
    "        self.scale_vect[:action_size] = 150\n",
    "        \n",
    "        # and now back to the origin implementation\n",
    "        self.replay_buffer = ReplayBuffer(BUFFER_SIZE)\n",
    "        \n",
    "        # Construct appropriate network based on flags\n",
    "        if mode == \"DDQN\":\n",
    "            self.deep_q = DeepQ(action_space.size())\n",
    "        elif mode == \"DQN\":\n",
    "            self.deep_q = DuelQ(action_space.size())\n",
    "\n",
    "    def _ml_act(self, observation, reward, done=False):\n",
    "        # method dedicated to grid2op, to use the MLAgent.\n",
    "        # i predict the action using the internal neural network\n",
    "        predict_movement_int, *_ = self.deep_q.predict_movement(observation[obs_to_model]*self.scale_vect, epsilon=0.)\n",
    "        # in the line above, we set the \"exploration\" parameter epsilon to \"0\" when using the agent.\n",
    "        # then convert it to a proper vector with the dedicated method (see bellow)\n",
    "        predict_movement_vect = self.opt_policy_to_action_vect(predict_movement_int)\n",
    "        return predict_movement_vect\n",
    "\n",
    "    def opt_policy_to_action_vect(self, opt_policy):\n",
    "        # helper to convert action, as returned as integer, to vector.\n",
    "        # DQN or DDQN will output a number (which is the choosen action)\n",
    "        # i need to convert it to a proper representation of an action.\n",
    "        # please refer to cell where  \"action space\" is defined, section II.B.a\n",
    "        res = np.zeros(self.action_size)\n",
    "        if opt_policy == 0:\n",
    "            # hard encode \"do nothing\"\n",
    "            pass\n",
    "        elif opt_policy < self.action_space.size():\n",
    "            # reconnect a powerline\n",
    "            res[opt_policy-1] = 1\n",
    "        else:\n",
    "            # disconnect a powerline\n",
    "            res[opt_policy-self.action_space.size()-1] = -1\n",
    "        return res\n",
    "    \n",
    "    def load_network(self, path):\n",
    "        # not modified compare to original implementation\n",
    "        self.deep_q.load_network(path)\n",
    "    \n",
    "    def convert_process_buffer(self):\n",
    "        \"\"\"Converts the list of NUM_FRAMES images in the process buffer\n",
    "        into one training sample\"\"\"\n",
    "        # here i simply concatenate the action in case of multiple action in the \"buffer\"\n",
    "        # this function existed in the original implementation, bus has been adapted.\n",
    "        return np.concatenate(self.process_buffer)\n",
    "    \n",
    "    def _build_valid_env(self, env=None):\n",
    "        # now we are creating a valid Environment\n",
    "        # it's mandatory because no environment are created when the agent is \n",
    "        # an Agent should not get direct access to the environment, but can interact with it only by:\n",
    "        # * receiving reward\n",
    "        # * receiving observation\n",
    "        # * sending action\n",
    "        \n",
    "        close_env = False\n",
    "        \n",
    "        if env is None:\n",
    "            env = grid2op.make(action_class=type(self.action_space({})),\n",
    "                              reward_class=self.reward_fun)\n",
    "            close_env = True\n",
    "                               \n",
    "        # I make sure the action space of the user and the environment are the same.\n",
    "        if not isinstance(self.action_space, type(env.action_space)):\n",
    "            raise RuntimeError(\"Imposssible to build an agent with 2 different action space\")\n",
    "        if not isinstance(env.action_space, type(self.action_space)):\n",
    "            raise RuntimeError(\"Imposssible to build an agent with 2 different action space\")\n",
    "            \n",
    "        # make sure the environment is reset\n",
    "        env.reset() \n",
    "        \n",
    "        # A buffer that keeps the last `NUM_FRAMES` images\n",
    "        self.replay_buffer.clear()\n",
    "        self.process_buffer = []\n",
    "        for _ in range(NUM_FRAMES):\n",
    "            # Initialize buffer with the first frames\n",
    "            s1, r1, _, _ = env.step(self.do_nothing_act)\n",
    "            s1_vect = s1.to_vect()\n",
    "            # all observation will that will be used by the agent will be\n",
    "            # of the shape vect_[obs_to_modl]*self.scale_vect\n",
    "            self.process_buffer.append(s1_vect[obs_to_model]*self.scale_vect)\n",
    "            \n",
    "        return env, close_env\n",
    "    \n",
    "    def train(self, num_frames, env=None):\n",
    "        # this function existed in the original implementation, but has been slightly adapted.\n",
    "        \n",
    "        # first we create an environment or make sure the given environment is valid\n",
    "        env, close_env = self._build_valid_env(env)\n",
    "        \n",
    "        # bellow that, only slight modification has been made. They are highlighted\n",
    "        observation_num = 0\n",
    "        curr_state = self.convert_process_buffer()\n",
    "        epsilon = INITIAL_EPSILON\n",
    "        alive_frame = 0\n",
    "        total_reward = 0\n",
    "\n",
    "        while observation_num < num_frames:\n",
    "            if observation_num % 1000 == 999:\n",
    "                print((\"Executing loop %d\" %observation_num))\n",
    "\n",
    "            # Slowly decay the learning rate\n",
    "            if epsilon > FINAL_EPSILON:\n",
    "                epsilon -= (INITIAL_EPSILON-FINAL_EPSILON)/EPSILON_DECAY\n",
    "\n",
    "            initial_state = self.convert_process_buffer()\n",
    "            self.process_buffer = []\n",
    "\n",
    "            # it's a bit less convenient that using the SpaceInvader environment.\n",
    "            # first we need to predict which actions to do (represented as an integer)\n",
    "            predict_movement_int, predict_q_value = self.deep_q.predict_movement(curr_state, epsilon)\n",
    "            # then we need to convert it to a valid vector that can represent a grid2op action\n",
    "            predict_movement_vect = self.opt_policy_to_action_vect(predict_movement_int)\n",
    "            # then we need to convert it to a proper action\n",
    "            predict_movement = self.convert_from_vect(predict_movement_vect)\n",
    "            \n",
    "            reward, done = 0, False\n",
    "            for i in range(NUM_FRAMES):\n",
    "                temp_observation_obj, temp_reward, temp_done, _ = env.step(predict_movement)\n",
    "                # here it has been adapted too. The observation get from the environment is\n",
    "                # first converted to vector\n",
    "                temp_observation = temp_observation_obj.to_vect()\n",
    "                # then only a subpart of it is used, and it is scaled to have proper values\n",
    "                temp_observation = temp_observation[obs_to_model]*self.scale_vect\n",
    "                \n",
    "                # below this line no changed have been made to the original implementation.\n",
    "                reward += temp_reward\n",
    "                self.process_buffer.append(temp_observation)\n",
    "                done = done | temp_done\n",
    "\n",
    "            if done:\n",
    "                print(\"Lived with maximum time \", alive_frame)\n",
    "                print(\"Earned a total of reward equal to \", total_reward)\n",
    "                # reset the environment\n",
    "                env.reset()\n",
    "                \n",
    "                alive_frame = 0\n",
    "                total_reward = 0\n",
    "\n",
    "            new_state = self.convert_process_buffer()\n",
    "            self.replay_buffer.add(initial_state, predict_movement_int, reward, done, new_state)\n",
    "            total_reward += reward\n",
    "            if self.replay_buffer.size() > MIN_OBSERVATION:\n",
    "                s_batch, a_batch, r_batch, d_batch, s2_batch = self.replay_buffer.sample(MINIBATCH_SIZE)\n",
    "                self.deep_q.train(s_batch, a_batch, r_batch, d_batch, s2_batch, observation_num)\n",
    "                self.deep_q.target_train()\n",
    "\n",
    "            # Save the network every 100000 iterations\n",
    "            if observation_num % 10000 == 9999 or observation_num == num_frames-1:\n",
    "                print(\"Saving Network\")\n",
    "                self.deep_q.save_network(\"saved.h5\")\n",
    "\n",
    "            alive_frame += 1\n",
    "            observation_num += 1\n",
    "        if close_env:\n",
    "            env.close()\n",
    "            \n",
    "    def calculate_mean(self, num_episode = 100, env=None):\n",
    "        # this method has been only slightly adapted from the original implementation\n",
    "        \n",
    "        # Note that it is NOT the recommended method to evaluate an Agent. Please use \"Grid2Op.Runner\" instead\n",
    "        \n",
    "        # first we create an environment or make sure the given environment is valid\n",
    "        env, close_env = self._build_valid_env(env)\n",
    "        \n",
    "        reward_list = []\n",
    "        print(\"Printing scores of each trial\")\n",
    "        for i in range(num_episode):\n",
    "            done = False\n",
    "            tot_award = 0\n",
    "            self.env.reset()\n",
    "            while not done:\n",
    "                state = self.convert_process_buffer()\n",
    "                \n",
    "                # same adapation as in \"train\" function. \n",
    "                predict_movement_int = self.deep_q.predict_movement(state, 0.0)[0]\n",
    "                predict_movement_vect = self.opt_policy_to_action_vect(predict_movement_int)\n",
    "                predict_movement = self.convert_from_vect(predict_movement_vect)\n",
    "                \n",
    "                # same adapation as in the \"train\" funciton\n",
    "                observation_obj, reward, done, _ = self.env.step(predict_movement)\n",
    "                observation_vect_full = observation_obj.to_vect()\n",
    "                observation = observation_vect_full[obs_to_model]*self.scale_vect\n",
    "                \n",
    "                tot_award += reward\n",
    "                self.process_buffer.append(observation)\n",
    "                self.process_buffer = self.process_buffer[1:]\n",
    "            print(tot_award)\n",
    "            reward_list.append(tot_award)\n",
    "            \n",
    "        if close_env:\n",
    "            env.close()\n",
    "        return np.mean(reward_list), np.std(reward_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define the model (agent), and then train it.\n",
    "\n",
    "This is done exactly the same way as in the Abhinav Sagar implementation.\n",
    "\n",
    "**NB** The code bellow can take a few minutes to run. It's training a Deep Reinforcement Learning Agent afterall. It this takes too long on your machine, you can always decrease the \"nb_frame\", and set it to 1000 for example. In this case, the Agent will probably not be really good.\n",
    "\n",
    "**NB** For a real Agent, it would take much longer to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/donnotben/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Successfully constructed networks.\n",
      "Lived with maximum time  139\n",
      "Earned a total of reward equal to  2779.9432466330645\n",
      "Lived with maximum time  147\n",
      "Earned a total of reward equal to  2919.9435985214477\n",
      "Lived with maximum time  60\n",
      "Earned a total of reward equal to  1179.9815756418086\n",
      "Lived with maximum time  66\n",
      "Earned a total of reward equal to  1299.9797465166648\n",
      "Lived with maximum time  60\n",
      "Earned a total of reward equal to  1179.9763611449916\n",
      "Lived with maximum time  122\n",
      "Earned a total of reward equal to  2419.958479722687\n",
      "Lived with maximum time  28\n",
      "Earned a total of reward equal to  539.9901606307824\n",
      "Lived with maximum time  56\n",
      "Earned a total of reward equal to  1099.9828651013831\n",
      "Lived with maximum time  119\n",
      "Earned a total of reward equal to  2359.9471366440293\n",
      "Lived with maximum time  91\n",
      "Earned a total of reward equal to  1799.964909821738\n",
      "Lived with maximum time  81\n",
      "Earned a total of reward equal to  1599.9727088733944\n",
      "Lived with maximum time  27\n",
      "Earned a total of reward equal to  519.9908356955004\n",
      "Executing loop 999\n",
      "WARNING:tensorflow:From /home/donnotben/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "We had a loss equal to  9.70562\n",
      "We had a loss equal to  9.64893\n",
      "We had a loss equal to  9.811851\n",
      "Lived with maximum time  217\n",
      "Earned a total of reward equal to  4319.9196249651\n",
      "We had a loss equal to  9.813609\n",
      "We had a loss equal to  9.629164\n",
      "Lived with maximum time  213\n",
      "Earned a total of reward equal to  4239.926335160655\n",
      "Lived with maximum time  51\n",
      "Earned a total of reward equal to  999.9846934794565\n",
      "We had a loss equal to  9.495363\n",
      "Lived with maximum time  56\n",
      "Earned a total of reward equal to  1099.9809528624476\n",
      "Lived with maximum time  47\n",
      "Earned a total of reward equal to  919.983254460504\n",
      "We had a loss equal to  9.7183695\n",
      "We had a loss equal to  9.941818\n",
      "Lived with maximum time  157\n",
      "Earned a total of reward equal to  3119.9396209011616\n",
      "We had a loss equal to  9.826441\n",
      "We had a loss equal to  9.931171\n",
      "Lived with maximum time  204\n",
      "Earned a total of reward equal to  4059.8747274397588\n",
      "Lived with maximum time  17\n",
      "Earned a total of reward equal to  319.9787741999127\n",
      "Executing loop 1999\n",
      "We had a loss equal to  9.752821\n",
      "Lived with maximum time  84\n",
      "Earned a total of reward equal to  1659.9740099549538\n",
      "Lived with maximum time  56\n",
      "Earned a total of reward equal to  1099.9763474888032\n",
      "We had a loss equal to  9.866148\n",
      "We had a loss equal to  10.287828\n",
      "Lived with maximum time  143\n",
      "Earned a total of reward equal to  2839.948373263176\n",
      "We had a loss equal to  9.920788\n",
      "Lived with maximum time  150\n",
      "Earned a total of reward equal to  2979.946757146769\n",
      "We had a loss equal to  10.44532\n",
      "We had a loss equal to  10.452843\n",
      "Lived with maximum time  201\n",
      "Earned a total of reward equal to  3999.9298755099508\n",
      "We had a loss equal to  11.019562\n",
      "Lived with maximum time  86\n",
      "Earned a total of reward equal to  1699.9737430358648\n",
      "We had a loss equal to  12.667879\n",
      "Lived with maximum time  41\n",
      "Earned a total of reward equal to  799.9846423472285\n",
      "We had a loss equal to  12.501457\n",
      "Lived with maximum time  103\n",
      "Earned a total of reward equal to  2039.9569782258109\n",
      "Lived with maximum time  33\n",
      "Earned a total of reward equal to  639.9890566068824\n",
      "We had a loss equal to  14.757476\n",
      "Lived with maximum time  102\n",
      "Earned a total of reward equal to  2019.9613015244367\n",
      "Executing loop 2999\n",
      "We had a loss equal to  10.100644\n",
      "Lived with maximum time  104\n",
      "Earned a total of reward equal to  2059.963037123253\n",
      "We had a loss equal to  14.506433\n",
      "Lived with maximum time  89\n",
      "Earned a total of reward equal to  1759.9668440350317\n",
      "We had a loss equal to  12.747721\n",
      "Lived with maximum time  108\n",
      "Earned a total of reward equal to  2139.9637303786035\n",
      "Lived with maximum time  31\n",
      "Earned a total of reward equal to  599.9896067307733\n",
      "We had a loss equal to  19.100836\n",
      "Lived with maximum time  89\n",
      "Earned a total of reward equal to  1759.9622049558081\n",
      "We had a loss equal to  26.381603\n",
      "We had a loss equal to  25.575556\n",
      "Lived with maximum time  187\n",
      "Earned a total of reward equal to  3719.935531285973\n",
      "We had a loss equal to  17.434925\n",
      "We had a loss equal to  50.19129\n",
      "Lived with maximum time  141\n",
      "Earned a total of reward equal to  2799.9339603195062\n",
      "We had a loss equal to  34.472965\n",
      "Lived with maximum time  116\n",
      "Earned a total of reward equal to  2299.931014868216\n",
      "Lived with maximum time  76\n",
      "Earned a total of reward equal to  1499.975514744526\n",
      "We had a loss equal to  34.451157\n",
      "Lived with maximum time  28\n",
      "Earned a total of reward equal to  539.9904872924848\n",
      "Lived with maximum time  47\n",
      "Earned a total of reward equal to  919.9852647578108\n",
      "Executing loop 3999\n",
      "We had a loss equal to  74.06401\n",
      "We had a loss equal to  44.328827\n",
      "Lived with maximum time  177\n",
      "Earned a total of reward equal to  3519.9337709555607\n",
      "We had a loss equal to  79.247246\n",
      "We had a loss equal to  105.56279\n",
      "Lived with maximum time  187\n",
      "Earned a total of reward equal to  3719.935602289031\n",
      "We had a loss equal to  122.38235\n",
      "We had a loss equal to  127.295746\n",
      "Lived with maximum time  217\n",
      "Earned a total of reward equal to  4319.921445529076\n",
      "We had a loss equal to  222.52023\n",
      "We had a loss equal to  248.97638\n",
      "Lived with maximum time  170\n",
      "Earned a total of reward equal to  3379.9394498478164\n",
      "We had a loss equal to  335.4291\n",
      "Lived with maximum time  135\n",
      "Earned a total of reward equal to  2679.9515872083834\n",
      "Lived with maximum time  22\n",
      "Earned a total of reward equal to  419.9922068417052\n",
      "We had a loss equal to  327.8947\n",
      "Lived with maximum time  73\n",
      "Earned a total of reward equal to  1439.9781841204726\n",
      "Executing loop 4999\n",
      "We had a loss equal to  258.729\n",
      "Lived with maximum time  87\n",
      "Earned a total of reward equal to  1719.9712480110222\n",
      "Lived with maximum time  27\n",
      "Earned a total of reward equal to  519.9897169472694\n",
      "We had a loss equal to  461.7373\n",
      "Lived with maximum time  117\n",
      "Earned a total of reward equal to  2319.956955503627\n",
      "We had a loss equal to  553.7923\n",
      "Lived with maximum time  28\n",
      "Earned a total of reward equal to  539.9905281735684\n",
      "Lived with maximum time  4\n",
      "Earned a total of reward equal to  59.99867695262013\n",
      "Lived with maximum time  18\n",
      "Earned a total of reward equal to  339.9936641921465\n",
      "We had a loss equal to  283.60696\n",
      "Lived with maximum time  153\n",
      "Earned a total of reward equal to  3039.913880926808\n",
      "We had a loss equal to  594.0502\n",
      "Lived with maximum time  92\n",
      "Earned a total of reward equal to  1819.962325528607\n",
      "Lived with maximum time  2\n",
      "Earned a total of reward equal to  19.999689945257252\n",
      "We had a loss equal to  867.5221\n",
      "We had a loss equal to  645.9403\n",
      "Lived with maximum time  125\n",
      "Earned a total of reward equal to  2479.9547753407596\n",
      "We had a loss equal to  537.8093\n",
      "Lived with maximum time  141\n",
      "Earned a total of reward equal to  2799.9437571323638\n",
      "We had a loss equal to  1101.7216\n",
      "We had a loss equal to  1643.8054\n",
      "Lived with maximum time  184\n",
      "Earned a total of reward equal to  3659.93657430912\n",
      "Executing loop 5999\n",
      "We had a loss equal to  1578.308\n",
      "Lived with maximum time  95\n",
      "Earned a total of reward equal to  1879.9557726081641\n",
      "Lived with maximum time  49\n",
      "Earned a total of reward equal to  959.9834871169006\n",
      "We had a loss equal to  400.9493\n",
      "We had a loss equal to  2461.928\n",
      "Lived with maximum time  129\n",
      "Earned a total of reward equal to  2559.9560929850372\n",
      "We had a loss equal to  1151.351\n",
      "Lived with maximum time  130\n",
      "Earned a total of reward equal to  2579.9554688893604\n",
      "We had a loss equal to  1786.9766\n",
      "We had a loss equal to  2471.042\n",
      "Lived with maximum time  212\n",
      "Earned a total of reward equal to  4219.895008471498\n",
      "Lived with maximum time  52\n",
      "Earned a total of reward equal to  1019.9838863480833\n",
      "We had a loss equal to  2413.9756\n",
      "Lived with maximum time  12\n",
      "Earned a total of reward equal to  219.99562810185262\n",
      "We had a loss equal to  714.01636\n",
      "We had a loss equal to  3019.5964\n",
      "Lived with maximum time  287\n",
      "Earned a total of reward equal to  5719.8673774301815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We had a loss equal to  2838.6738\n",
      "Lived with maximum time  21\n",
      "Earned a total of reward equal to  419.9924386821733\n",
      "Lived with maximum time  35\n",
      "Earned a total of reward equal to  679.9876148737703\n",
      "Executing loop 6999\n",
      "We had a loss equal to  1574.3474\n",
      "We had a loss equal to  2279.1033\n",
      "Lived with maximum time  245\n",
      "Earned a total of reward equal to  4879.907859242132\n",
      "We had a loss equal to  2771.8103\n",
      "We had a loss equal to  2869.1582\n",
      "Lived with maximum time  171\n",
      "Earned a total of reward equal to  3399.9423603286746\n",
      "We had a loss equal to  2616.048\n",
      "We had a loss equal to  4314.7754\n",
      "Lived with maximum time  216\n",
      "Earned a total of reward equal to  4299.895433504523\n",
      "We had a loss equal to  1699.2465\n",
      "We had a loss equal to  3021.687\n",
      "Lived with maximum time  133\n",
      "Earned a total of reward equal to  2639.956721026433\n",
      "Lived with maximum time  11\n",
      "Earned a total of reward equal to  199.9959546476586\n",
      "Lived with maximum time  31\n",
      "Earned a total of reward equal to  599.9890503036517\n",
      "We had a loss equal to  4523.053\n",
      "We had a loss equal to  4112.4033\n",
      "Lived with maximum time  156\n",
      "Earned a total of reward equal to  3099.9480470910507\n",
      "Lived with maximum time  21\n",
      "Earned a total of reward equal to  399.99270313566376\n",
      "Executing loop 7999\n",
      "We had a loss equal to  4390.7817\n",
      "Lived with maximum time  73\n",
      "Earned a total of reward equal to  1439.9749805597032\n",
      "We had a loss equal to  4619.464\n",
      "Lived with maximum time  145\n",
      "Earned a total of reward equal to  2879.9432205644475\n",
      "We had a loss equal to  3391.7993\n",
      "Lived with maximum time  90\n",
      "Earned a total of reward equal to  1779.9366124160047\n",
      "Lived with maximum time  19\n",
      "Earned a total of reward equal to  359.99328422690644\n",
      "We had a loss equal to  2437.307\n",
      "Lived with maximum time  84\n",
      "Earned a total of reward equal to  1659.9751312036435\n",
      "We had a loss equal to  576.9976\n",
      "Lived with maximum time  110\n",
      "Earned a total of reward equal to  2179.962842803299\n",
      "We had a loss equal to  3293.1821\n",
      "Lived with maximum time  88\n",
      "Earned a total of reward equal to  1739.9699671438386\n",
      "We had a loss equal to  3127.5757\n",
      "Lived with maximum time  128\n",
      "Earned a total of reward equal to  2539.9540186394884\n",
      "We had a loss equal to  6436.4746\n",
      "Lived with maximum time  34\n",
      "Earned a total of reward equal to  659.9889063282436\n",
      "Lived with maximum time  20\n",
      "Earned a total of reward equal to  379.9931276114705\n",
      "We had a loss equal to  8655.135\n",
      "Lived with maximum time  96\n",
      "Earned a total of reward equal to  1899.9680262802422\n",
      "Lived with maximum time  8\n",
      "Earned a total of reward equal to  139.99704070640226\n",
      "We had a loss equal to  4867.8784\n",
      "Lived with maximum time  98\n",
      "Earned a total of reward equal to  1939.9678270851039\n",
      "Executing loop 8999\n",
      "We had a loss equal to  9254.59\n",
      "Lived with maximum time  138\n",
      "Earned a total of reward equal to  2739.933462118355\n",
      "We had a loss equal to  5162.329\n",
      "We had a loss equal to  2712.2627\n",
      "Lived with maximum time  136\n",
      "Earned a total of reward equal to  2699.9532415873646\n",
      "Lived with maximum time  40\n",
      "Earned a total of reward equal to  779.9834535153088\n",
      "We had a loss equal to  3873.4253\n",
      "Lived with maximum time  121\n",
      "Earned a total of reward equal to  2399.9533190752254\n",
      "We had a loss equal to  3891.5444\n",
      "Lived with maximum time  116\n",
      "Earned a total of reward equal to  2299.9587142509135\n",
      "Lived with maximum time  2\n",
      "Earned a total of reward equal to  19.99955762695355\n",
      "We had a loss equal to  26738.77\n",
      "Lived with maximum time  60\n",
      "Earned a total of reward equal to  1179.9818772612477\n",
      "We had a loss equal to  5505.7686\n",
      "Lived with maximum time  108\n",
      "Earned a total of reward equal to  2139.9630248928243\n",
      "We had a loss equal to  8300.616\n",
      "We had a loss equal to  2323.268\n",
      "Lived with maximum time  186\n",
      "Earned a total of reward equal to  3699.9214459127616\n",
      "We had a loss equal to  3808.2893\n",
      "Lived with maximum time  109\n",
      "Earned a total of reward equal to  2159.9564916758486\n",
      "Lived with maximum time  14\n",
      "Earned a total of reward equal to  259.99522221443897\n",
      "Lived with maximum time  17\n",
      "Earned a total of reward equal to  319.99398931834\n",
      "Executing loop 9999\n",
      "Saving Network\n",
      "Successfully saved network.\n"
     ]
    }
   ],
   "source": [
    "nb_frame = 10000\n",
    "my_agent = DeepQAgent(runner.env.action_space, mode=\"DQN\")\n",
    "my_agent.train(nb_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III) Evaluating the Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, time to test this trained agent.\n",
    "\n",
    "To do that, we have multiple choices.\n",
    "\n",
    "Either we recode the \"DeepQAgent\" class to load the stored weights (that have been saved during trainig) when it is initialized (not covered in this notebook), or we can also directly specified the \"instance\" of the Agent to use in the Grid2Op Runner.\n",
    "\n",
    "To do that, it's fairly simple. First, you need to specify that you won't use the \"*agentClass*\" argument, by setting it to ``None``, and secondly you simply provide the agent to use in the *agentInstance* argument.\n",
    "\n",
    "**NB** If you don't do that, the Runner will be created (the constructor will raise an exception). And if you choose to use the \"*agentClass*\" argument, your agent will be reloaded from scratch. So **if it doesn't load the weights** it will behave as a non trained agent, unlikely to perform well on the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.A) Evaluate the Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have \"successfully\" trained our Agent, we will evaluating it. As opposed to the trainining, the evaluation is done classically using a standard Runner.\n",
    "\n",
    "Note that the Runner will use a \"scoring function\" that might be different from the \"reward function\" used during training. In our case, it's not. We use the `L2RPNReward` in both cases.\n",
    "\n",
    "In the code bellow, we commented on what can be different and what must be identical for training and evaluation of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_function = L2RPNReward\n",
    "# make a runner\n",
    "runner = Runner(init_grid_path=grid2op.CASE_14_FILE, # this should be the same grid as the one the agent is trained one\n",
    "                path_chron=grid2op.CHRONICS_MLUTIEPISODE,  # chronics can changed of course\n",
    "                gridStateclass=Multifolder, # the class of chronics can changed too\n",
    "                gridStateclass_kwargs={\"gridvalueClass\": GridStateFromFileWithForecasts},  # so this can changed too\n",
    "                names_chronics_to_backend = grid2op.NAMES_CHRONICS_TO_BACKEND,  # this also can changed\n",
    "                agentInstance=my_agent,  # here i pass a trained agent, no need to read it from the \n",
    "                agentClass=None,  # if i use an instance of Agent, i cannot provide a class\n",
    "                rewardClass=scoring_function,  # this can be anything, not necessarily the same for training\n",
    "                actionClass=PowerLineSet  # this must be the same as the one used for training.\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the Agent and save the results. As opposed to the multiple times we exposed the \"runner.run\" call, we never really dive into the \"path_save\" argument. This path allows you to save lots of information about your Agent behaviour. Please All the informations present are shown on the documentation [here](file:///home/donnotben/Documents/Grid2Op/documentation/html/runner.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the trained agent are:\n",
      "\tFor chronics located at /home/donnotben/.local/lib/python3.6/site-packages/grid2op/data/test_multi_chronics/1\n",
      "\t\t - cumulative reward: 5739.905567\n",
      "\t\t - number of time steps completed: 287 / 287\n"
     ]
    }
   ],
   "source": [
    "# initialize it\n",
    "res = runner.run(nb_episode=1, path_save=\"trained_agent_log\")\n",
    "print(\"The results for the trained agent are:\")\n",
    "for chron_name, cum_reward, nb_time_step, max_ts in res:\n",
    "    msg_tmp = \"\\tFor chronics located at {}\\n\".format(chron_name)\n",
    "    msg_tmp += \"\\t\\t - cumulative reward: {:.6f}\\n\".format(cum_reward)\n",
    "    msg_tmp += \"\\t\\t - number of time steps completed: {:.0f} / {:.0f}\".format(nb_time_step, max_ts)\n",
    "    print(msg_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.B) Inspect the Agent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please refer to the official document for more information about the content of the directory where the data are saved. Note that the saving of the information is triggered by the \"path_save\" argument sent to the \"runner.run\" function.\n",
    "\n",
    "Some information that will be present in this repository are:\n",
    "If enabled, the :class:`Runner` will save the information in a structured way. For each episode there will be a folder\n",
    "with:\n",
    "\n",
    "  - \"episode_meta.json\" that represents some meta information about:\n",
    "\n",
    "    - \"backend_type\": the name of the `grid2op.Backend` class used\n",
    "    - \"chronics_max_timestep\": the **maximum** number of timestep for the chronics used\n",
    "    - \"chronics_path\": the path where the temporal data (chronics) are located\n",
    "    - \"env_type\": the name of the `grid2op.Environment` class used.\n",
    "    - \"grid_path\": the path where the powergrid has been loaded from\n",
    "\n",
    "  - \"episode_times.json\": gives some information about the total time spend in multiple part of the runner, mainly the\n",
    "    `grid2op.Agent` (and especially its method `grid2op.Agent.act`) and amount of time spent in the\n",
    "    `grid2op.Environment`\n",
    "\n",
    "  - \"_parameters.json\": is a representation as json of a the `grid2op.Parameters` used for this episode\n",
    "  - \"rewards.npy\" is a numpy 1d array giving the rewards at each time step. We adopted the convention that the stored\n",
    "    reward at index `i` is the one observed by the agent at time `i` and **NOT** the reward sent by the\n",
    "    `grid2op.Environment` after the action has been implemented.\n",
    "  - \"exec_times.npy\" is a numpy 1d array giving the execution time of each time step of the episode\n",
    "  - \"actions.npy\" gives the actions that has been taken by the `grid2op.Agent`. At row `i` of \"actions.npy\" is a\n",
    "    vectorized representation of the action performed by the agent at timestep `i` *ie.* **after** having observed\n",
    "    the observation present at row `i` of \"observation.npy\" and the reward showed in row `i` of \"rewards.npy\".\n",
    "  - \"disc_lines.npy\" gives which lines have been disconnected during the simulation of the cascading failure at each\n",
    "    time step. The same convention as for \"rewards.npy\" has been adopted. This means that the powerlines are\n",
    "    disconnected when the `grid2op.Agent` takes the `grid2op.Action` at time step `i`.\n",
    "  - \"observations.npy\" is a numpy 2d array reprensenting the `grid2op.Observation` at the disposal of the\n",
    "    `grid2op.Agent` when he took his action.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can first look at the repository were the data are stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\r\n"
     ]
    }
   ],
   "source": [
    "!ls trained_agent_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there is only one folder there. It's named \"1\" because, in the original data, this came from the folder named \"1\" (the original data are located at \"/home/donnotben/.local/lib/python3.6/site-packages/grid2op/data/test_multi_chronics/\")\n",
    "\n",
    "If there were multiple episode, each episode would have it's own folder, with a name as resemblant as possible to the origin name of the data. This is done to ease the studying of the results.\n",
    "\n",
    "Now let's see what is inside this folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions.npy\t\t\t  episode_meta.json   _parameters.json\r\n",
      "agent_exec_times.npy\t\t  episode_times.json  rewards.npy\r\n",
      "disc_lines_cascading_failure.npy  observations.npy\r\n"
     ]
    }
   ],
   "source": [
    "!ls trained_agent_log/1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can for example load the \"actions\" performed by the Agent, and have a look at them.\n",
    "\n",
    "To do that we will load the action array (represented as vector) and use the action_space to convert it back into valid action class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_actions = np.load(os.path.join(\"trained_agent_log\", \"1\", \"actions.npy\"))\n",
    "li_actions = []\n",
    "for i in range(all_actions.shape[0]):\n",
    "    tmp = runner.env.action_space.from_vect(all_actions[i,:])\n",
    "    li_actions.append(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows us to have a deeper look at the action, and their effect. Note that here, we used action that can only **set** the line status, so looking at their effect is pretty straightforward.\n",
    "\n",
    "Also, note that as oppose to \"change\", if a powerline is already connected, trying to **set** it as connected has absolutely no impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_disc = 0\n",
    "line_reco = 0\n",
    "for act in li_actions:\n",
    "    dict_ = act.as_dict()\n",
    "    if \"set_line_status\" in dict_:\n",
    "        line_reco +=  dict_[\"set_line_status\"][\"nb_connected\"]\n",
    "        line_disc +=  dict_[\"set_line_status\"][\"nb_disconnected\"]\n",
    "line_reco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As wa can see for our event, the agent always try to reconnect a powerline. As all lines are alway reconnected, this Agent does basically nothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do the same kind of post analysis for the observation, even though here, as the observations come from files, it's probably not particularly intersting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-286"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_observations = np.load(os.path.join(\"trained_agent_log\", \"1\", \"observations.npy\"))\n",
    "li_observations = []\n",
    "nb_real_disc = 0\n",
    "for i in range(all_observations.shape[0]):\n",
    "    tmp = runner.env.observation_space.from_vect(all_observations[i,:])\n",
    "    li_observations.append(tmp)\n",
    "    nb_real_disc += (np.sum(tmp.line_status) - tmp.line_status.shape[0])\n",
    "nb_real_disc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
