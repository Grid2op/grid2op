{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actions as vector, and RL agent training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Disclaimer***: This file referenced some files in other directories. In order to have working cross referencing it's recommended to start the notebook server from the root directory (`Grid2Op`) of the package and __not__ in the `getting_started` sub directory:\n",
    "```bash\n",
    "cd Grid2Op\n",
    "jupyter notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***NB*** For more information about how to use the package, a general help can be built locally (provided that sphinx is installed on the machine) with:\n",
    "```bash\n",
    "cd Grid2Op\n",
    "make html\n",
    "```\n",
    "from the top directory of the package (usually `Grid2Op`).\n",
    "\n",
    "Once build, the help can be access from [here](../documentation/html/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is recommended to have a look at the [0_basic_functionalities](0_basic_functionalities.ipynb), [1_Observation_Agents](1_Observation_Agents.ipynb) and [2_Action_GridManipulation](2_Action_GridManipulation.ipynb) notebooks before getting into this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objectives**\n",
    "\n",
    "In this notebook we will expose :\n",
    "* how to use the \"vectorized\" verions of Observations and Actions\n",
    "* how to train a (stupid) Agent using reinforcement learning.\n",
    "* how to inspect (rapidly) the action taken by the Agent\n",
    "\n",
    "**NB** for this tutorial we train an Agent inspired from this blog post: [deep-reinforcement-learning-tutorial-with-open-ai-gym](https://towardsdatascience.com/deep-reinforcement-learning-tutorial-with-open-ai-gym-c0de4471f368). Many other different reinforcement learning tutorial exist. The code showed in this notebook has no pretention except to demonstrate how to use Grid2Op functionality to train a Deep Reinforcement learning Agent and inspect its behaviour. There are absolutely nothing implied about the performance, training strategy, type of Agent etc, meta parameters etc. All of them are purely \"random\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import grid2op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"my_id_menu_nb\">run previous cell, wait for 2 seconds</div>\n",
       "<script>\n",
       "function repeat_indent_string(n){\n",
       "    var a = \"\" ;\n",
       "    for ( ; n > 0 ; --n)\n",
       "        a += \"    \";\n",
       "    return a;\n",
       "}\n",
       "// look up into all sections and builds an automated menu //\n",
       "var update_menu_string = function(begin, lfirst, llast, sformat, send, keep_item, begin_format, end_format) {\n",
       "    var anchors = document.getElementsByClassName(\"section\");\n",
       "    if (anchors.length == 0) {\n",
       "        anchors = document.getElementsByClassName(\"text_cell_render rendered_html\");\n",
       "    }\n",
       "    var i,t;\n",
       "    var text_menu = begin;\n",
       "    var text_memo = \"<pre>\\nlength:\" + anchors.length + \"\\n\";\n",
       "    var ind = \"\";\n",
       "    var memo_level = 1;\n",
       "    var href;\n",
       "    var tags = [];\n",
       "    var main_item = 0;\n",
       "    var format_open = 0;\n",
       "    for (i = 0; i <= llast; i++)\n",
       "        tags.push(\"h\" + i);\n",
       "\n",
       "    for (i = 0; i < anchors.length; i++) {\n",
       "        text_memo += \"**\" + anchors[i].id + \"--\\n\";\n",
       "\n",
       "        var child = null;\n",
       "        for(t = 0; t < tags.length; t++) {\n",
       "            var r = anchors[i].getElementsByTagName(tags[t]);\n",
       "            if (r.length > 0) {\n",
       "child = r[0];\n",
       "break;\n",
       "            }\n",
       "        }\n",
       "        if (child == null) {\n",
       "            text_memo += \"null\\n\";\n",
       "            continue;\n",
       "        }\n",
       "        if (anchors[i].hasAttribute(\"id\")) {\n",
       "            // when converted in RST\n",
       "            href = anchors[i].id;\n",
       "            text_memo += \"#1-\" + href;\n",
       "            // passer à child suivant (le chercher)\n",
       "        }\n",
       "        else if (child.hasAttribute(\"id\")) {\n",
       "            // in a notebook\n",
       "            href = child.id;\n",
       "            text_memo += \"#2-\" + href;\n",
       "        }\n",
       "        else {\n",
       "            text_memo += \"#3-\" + \"*\" + \"\\n\";\n",
       "            continue;\n",
       "        }\n",
       "        var title = child.textContent;\n",
       "        var level = parseInt(child.tagName.substring(1,2));\n",
       "\n",
       "        text_memo += \"--\" + level + \"?\" + lfirst + \"--\" + title + \"\\n\";\n",
       "\n",
       "        if ((level < lfirst) || (level > llast)) {\n",
       "            continue ;\n",
       "        }\n",
       "        if (title.endsWith('¶')) {\n",
       "            title = title.substring(0,title.length-1).replace(\"<\", \"&lt;\")\n",
       "         .replace(\">\", \"&gt;\").replace(\"&\", \"&amp;\");\n",
       "        }\n",
       "        if (title.length == 0) {\n",
       "            continue;\n",
       "        }\n",
       "\n",
       "        while (level < memo_level) {\n",
       "            text_menu += end_format + \"</ul>\\n\";\n",
       "            format_open -= 1;\n",
       "            memo_level -= 1;\n",
       "        }\n",
       "        if (level == lfirst) {\n",
       "            main_item += 1;\n",
       "        }\n",
       "        if (keep_item != -1 && main_item != keep_item + 1) {\n",
       "            // alert(main_item + \" - \" + level + \" - \" + keep_item);\n",
       "            continue;\n",
       "        }\n",
       "        while (level > memo_level) {\n",
       "            text_menu += \"<ul>\\n\";\n",
       "            memo_level += 1;\n",
       "        }\n",
       "        text_menu += repeat_indent_string(level-2);\n",
       "        text_menu += begin_format + sformat.replace(\"__HREF__\", href).replace(\"__TITLE__\", title);\n",
       "        format_open += 1;\n",
       "    }\n",
       "    while (1 < memo_level) {\n",
       "        text_menu += end_format + \"</ul>\\n\";\n",
       "        memo_level -= 1;\n",
       "        format_open -= 1;\n",
       "    }\n",
       "    text_menu += send;\n",
       "    //text_menu += \"\\n\" + text_memo;\n",
       "\n",
       "    while (format_open > 0) {\n",
       "        text_menu += end_format;\n",
       "        format_open -= 1;\n",
       "    }\n",
       "    return text_menu;\n",
       "};\n",
       "var update_menu = function() {\n",
       "    var sbegin = \"\";\n",
       "    var sformat = '<a href=\"#__HREF__\">__TITLE__</a>';\n",
       "    var send = \"\";\n",
       "    var begin_format = '<li>';\n",
       "    var end_format = '</li>';\n",
       "    var keep_item = -1;\n",
       "    var text_menu = update_menu_string(sbegin, 2, 4, sformat, send, keep_item,\n",
       "       begin_format, end_format);\n",
       "    var menu = document.getElementById(\"my_id_menu_nb\");\n",
       "    menu.innerHTML=text_menu;\n",
       "};\n",
       "window.setTimeout(update_menu,2000);\n",
       "            </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = None\n",
    "try:\n",
    "    from jyquickhelper import add_notebook_menu\n",
    "    res = add_notebook_menu()\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Impossible to automatically add a menu / table of content to this notebook.\\nYou can download \\\"jyquickhelper\\\" package with: \\n\\\"pip install jyquickhelper\\\"\")\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I) Manipulating vectors instead of class in Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid2op package has been built with an \"object oriented\" perspective: almost everything is encapsulated in a dedicated `class`. This allows for more customization of the plateform.\n",
    "\n",
    "The downside of this approach is that machine learning method, and especially deep learning, often prefers to deal with vectors rather than with `complex` objects. \n",
    "\n",
    "To have the best of both worlds, we provided a `MLAgent` class that handles the convertion to / from these classes to numpy vector. In this notebook, we will see how this class can be used and overidden to develop a Agent that learns how to perform action on the grid. By default, this class does nothing, and it's possible (and encouraged) to override the `MLAgent._ml_act` method to build smarter Agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the DoNothing agent are:\n",
      "\tFor chronics located at /home/donnotben/.local/lib/python3.6/site-packages/grid2op/data/test_multi_chronics/1\n",
      "\t\t - cumulative reward: 5739.951023\n",
      "\t\t - number of time steps completed: 287 / 287\n"
     ]
    }
   ],
   "source": [
    "# import the usefull class\n",
    "import numpy as np\n",
    "\n",
    "from grid2op.Runner import Runner\n",
    "from grid2op.ChronicsHandler import Multifolder, GridStateFromFileWithForecasts\n",
    "from grid2op.Agent import MLAgent\n",
    "from grid2op.Reward import L2RPNReward\n",
    "from grid2op.Action import PowerLineSet\n",
    "# make a runner\n",
    "runner = Runner(init_grid_path=grid2op.CASE_14_FILE,\n",
    "               path_chron=grid2op.CHRONICS_MLUTIEPISODE,\n",
    "               gridStateclass=Multifolder,\n",
    "               gridStateclass_kwargs={\"gridvalueClass\": GridStateFromFileWithForecasts},\n",
    "               names_chronics_to_backend = grid2op.NAMES_CHRONICS_TO_BACKEND,\n",
    "                agentClass=MLAgent,\n",
    "               rewardClass=L2RPNReward,\n",
    "               actionClass=PowerLineSet)\n",
    "# initialize it\n",
    "res = runner.run(nb_episode=1)\n",
    "print(\"The results for the DoNothing agent are:\")\n",
    "for chron_name, cum_reward, nb_time_step, max_ts in res:\n",
    "    msg_tmp = \"\\tFor chronics located at {}\\n\".format(chron_name)\n",
    "    msg_tmp += \"\\t\\t - cumulative reward: {:.6f}\\n\".format(cum_reward)\n",
    "    msg_tmp += \"\\t\\t - number of time steps completed: {:.0f} / {:.0f}\".format(nb_time_step, max_ts)\n",
    "    print(msg_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that is it. It is as simple as changing the \"agentClass\". Now we will provide an example on how this class can be overidden to train a \"real\" Agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II) Training an Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial, we will expose to built a Q-learning Agent. Most of the code (except the neural network architecture) are inspired from this blog post: [https://towardsdatascience.com/deep-reinforcement-learning-tutorial-with-open-ai-gym-c0de4471f368](https://towardsdatascience.com/deep-reinforcement-learning-tutorial-with-open-ai-gym-c0de4471f368).\n",
    "\n",
    "**Requirements** This notebook require to have `keras` installed on your machine.\n",
    "\n",
    "The Agent we will train will:\n",
    "* only act on the topology\n",
    "* only try to set the status of the powerline (he won't try to modify the \"change\" them, nor try to perform any kind of bus splitting merging)\n",
    "\n",
    "This is achieved by passing the \"*PowerLineSet*\" class in the actionClass of the Runner. For more information, one can consult the [PowerLineSet](../documentation/html/action.html#grid2op.Action.PowerLineSet) help page (if built locally, which is recommended) or the [PowerLineSet](../grid2op/Action.py) class definition in the [Action.py](../grid2op/Action.py) file.\n",
    "\n",
    "Note that this agent is unlikely to perform well in a L2RPN competition, as it uses only a small subset of the action. It is exposed here as an example.\n",
    "\n",
    "Also, note that we use a specific class of `Action` in this notebook. This is unlikely this class will be use in a competition. The exercise in this notebook is then purely for demonstrating \"how to\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.A) Defining some \"helpers\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type of Agent were are using require a bit of set up, independantly of Grid2Op. We will reuse the code showed in \n",
    "[https://towardsdatascience.com/deep-reinforcement-learning-tutorial-with-open-ai-gym-c0de4471f368](https://towardsdatascience.com/deep-reinforcement-learning-tutorial-with-open-ai-gym-c0de4471f368) and in [Reinforcement-Learning-Tutorial](https://github.com/abhinavsagar/Reinforcement-Learning-Tutorial) from Abhinav Sagar code under a *MIT license* found here: [MIT License](https://github.com/abhinavsagar/Reinforcement-Learning-Tutorial/blob/master/LICENSE).\n",
    "\n",
    "This first section is here to define these classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first let's import the necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/donnotben/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/donnotben/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/donnotben/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/donnotben/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/donnotben/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/donnotben/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.models import load_model, Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.core import Activation, Dropout, Flatten, Dense\n",
    "from keras.layers import Input, Lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Replay buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " First we define a \"replay buffer\" necessary to train the Agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit Abhinav Sagar: \n",
    "# https://github.com/abhinavsagar/Reinforcement-Learning-Tutorial\n",
    "# Code under MIT license, available at:\n",
    "# https://github.com/abhinavsagar/Reinforcement-Learning-Tutorial/blob/master/LICENSE\n",
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Constructs a buffer object that stores the past moves\n",
    "    and samples a set of subsamples\"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.count = 0\n",
    "        self.buffer = deque()\n",
    "\n",
    "    def add(self, s, a, r, d, s2):\n",
    "        \"\"\"Add an experience to the buffer\"\"\"\n",
    "        # S represents current state, a is action,\n",
    "        # r is reward, d is whether it is the end, \n",
    "        # and s2 is next state\n",
    "        experience = (s, a, r, d, s2)\n",
    "        if self.count < self.buffer_size:\n",
    "            self.buffer.append(experience)\n",
    "            self.count += 1\n",
    "        else:\n",
    "            self.buffer.popleft()\n",
    "            self.buffer.append(experience)\n",
    "\n",
    "    def size(self):\n",
    "        return self.count\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Samples a total of elements equal to batch_size from buffer\n",
    "        if buffer contains enough elements. Otherwise return all elements\"\"\"\n",
    "\n",
    "        batch = []\n",
    "\n",
    "        if self.count < batch_size:\n",
    "            batch = random.sample(self.buffer, self.count)\n",
    "        else:\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        # Maps each experience in batch in batches of states, actions, rewards\n",
    "        # and new states\n",
    "        s_batch, a_batch, r_batch, d_batch, s2_batch = list(map(np.array, list(zip(*batch))))\n",
    "\n",
    "        return s_batch, a_batch, r_batch, d_batch, s2_batch\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "        self.count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Meta parameters of the methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we re-use the default parameters, note that these can be optimized. Nothing has been changed for this example.\n",
    "\n",
    "For more information about them, please refer to the blog post of Abhinav Sagar [available here](https://towardsdatascience.com/deep-reinforcement-learning-tutorial-with-open-ai-gym-c0de4471f368)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DECAY_RATE = 0.99\n",
    "BUFFER_SIZE = 40000\n",
    "MINIBATCH_SIZE = 64\n",
    "TOT_FRAME = 3000000\n",
    "EPSILON_DECAY = 1000000\n",
    "MIN_OBSERVATION = 1000 #5000\n",
    "FINAL_EPSILON = 1/300  # have on average 1 random action per scenario of approx 287 time steps\n",
    "INITIAL_EPSILON = 0.1\n",
    "TAU = 0.01\n",
    "# Number of frames to \"throw\" into network\n",
    "NUM_FRAMES = 1 ## this has been changed compared to the original implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.B) Adapatation of the inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original code, the models were used to play an Atari game and the inputs were images. For our system, the inputs are \"Observation\" converted as vector.\n",
    "\n",
    "For a more detailed description of the code used, please check:\n",
    "* [https://towardsdatascience.com/deep-reinforcement-learning-tutorial-with-open-ai-gym-c0de4471f368](https://towardsdatascience.com/deep-reinforcement-learning-tutorial-with-open-ai-gym-c0de4471f368)\n",
    "* and [Reinforcement-Learning-Tutorial](https://github.com/abhinavsagar/Reinforcement-Learning-Tutorial)\n",
    "\n",
    "\n",
    "This is why we adapted the original code from Abhinav Sagar:\n",
    "* We replaced convolutional layers with fully connected (dense) layers\n",
    "* We made sure not to look at all the observations, but rather at only some part of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) extracting relevant information of observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we extract relevant information about the dimension of the observation space, and the action space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_size_init = runner.env.observation_space.size()\n",
    "topo_vect_size = runner.env.observation_space.dim_topo\n",
    "action_size = runner.env.action_space.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Agent we will train will be able to act one only one powerline, and to either reconnect it, or disconnect it. It has a choice then bewteen 2 * `number of powerlines`.\n",
    "\n",
    "To train our agent more easily, we will only use part of the ***observation space***. We will not inform the agent about the state of the loads or generators, nor about the flows on the powerline, or the topology vector. Only the part of the observation that will concerned:\n",
    "* the relative powerflow $\\rho$ (current flow divided by thermal limit)\n",
    "* the powerline status (1 powerline is connected, 0 it's disconnected)\n",
    "\n",
    "The ***action space*** will be represented by a \"one hot\" vector, as followed:\n",
    "* if the first component (index 0) is set to 1, then the Agent does nothing\n",
    "* if the component equal to \"1\" has its index `i` between 1 and the `number of powerlines`, it will consist in reconnecting powerline `i-1`\n",
    "* if the component equal to \"1\" has its index `i` between the `number of powerlines` + 1 and twice \"the `number of powerlines`\", it will consist in disconnecting powerline `i-nb_powerline-1`\n",
    "\n",
    "It has then a dimension of 2 * `number of powerlines` + 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the subspace (represented as index) of the action space used by the agent.\n",
    "obs_to_model = np.full(observation_size_init, fill_value=False, dtype=np.bool)\n",
    "obs_to_model[(-3*action_size-topo_vect_size):(-action_size-topo_vect_size)] = True\n",
    "observation_size = np.sum(obs_to_model)\n",
    "\n",
    "# define the size of the action space\n",
    "NUM_ACTIONS = 2* action_size + 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Code the neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code of the neural networks used have been impacted only slightly to adapt them to our problem. The biggest changes comes from removing the convolutional layers, as well as adapting the input and output size.\n",
    "\n",
    "For each of the method bellow, we specify what have been adapted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit Abhinav Sagar: \n",
    "# https://github.com/abhinavsagar/Reinforcement-Learning-Tutorial\n",
    "# Code under MIT license, available at:\n",
    "# https://github.com/abhinavsagar/Reinforcement-Learning-Tutorial/blob/master/LICENSE\n",
    "\n",
    "class DeepQ(object):\n",
    "    \"\"\"Constructs the desired deep q learning network\"\"\"\n",
    "    def __init__(self, action_size, lr=0.00001):\n",
    "        # It is not modified from  Abhinav Sagar's code, except for adding the possibility to change the learning rate\n",
    "        # in parameter is also present the size of the action space\n",
    "        # (it used to be a global variable in the original code)\n",
    "        self.action_size = action_size\n",
    "        self.model = None\n",
    "        self.target_model = None\n",
    "        self.lr_ = lr\n",
    "        self.construct_q_network()\n",
    "    \n",
    "    def construct_q_network(self):\n",
    "        # replacement of the Convolution layers by Dense layers, and change the size of the input space and output space\n",
    "        \n",
    "        # Uses the network architecture found in DeepMind paper\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(observation_size*NUM_FRAMES))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Dense(observation_size))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Dense(observation_size))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Dense(2*NUM_ACTIONS))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Dense(NUM_ACTIONS))\n",
    "        self.model.compile(loss='mse', optimizer=Adam(lr=self.lr_))\n",
    "\n",
    "        # Creates a target network as described in DeepMind paper\n",
    "        self.target_model = Sequential()\n",
    "        self.target_model.add(Dense(observation_size*NUM_FRAMES))\n",
    "        self.target_model.add(Activation('relu'))\n",
    "        self.target_model.add(Dense(observation_size))\n",
    "        self.target_model.add(Activation('relu'))\n",
    "        self.target_model.add(Dense(observation_size))\n",
    "        self.target_model.add(Activation('relu'))\n",
    "        self.target_model.add(Dense(2*NUM_ACTIONS))\n",
    "        self.target_model.add(Activation('relu'))\n",
    "        self.target_model.add(Dense(NUM_ACTIONS))\n",
    "        self.target_model.compile(loss='mse', optimizer=Adam(lr=self.lr_))\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "    def predict_movement(self, data, epsilon):\n",
    "        \"\"\"Predict movement of game controler where is epsilon\n",
    "        probability randomly move.\"\"\"\n",
    "        # nothing has changed from the original implementation\n",
    "        rand_val = np.random.random()\n",
    "        q_actions = self.model.predict(data.reshape(1, observation_size*NUM_FRAMES), batch_size = 1)\n",
    "        \n",
    "        if rand_val < epsilon:\n",
    "            opt_policy = np.random.randint(0, NUM_ACTIONS)\n",
    "        else:\n",
    "            opt_policy = np.argmax(np.abs(q_actions))\n",
    "        return opt_policy, q_actions[0, opt_policy]\n",
    "\n",
    "    def train(self, s_batch, a_batch, r_batch, d_batch, s2_batch, observation_num):\n",
    "        \"\"\"Trains network to fit given parameters\"\"\"\n",
    "        # nothing has changed from the original implementation, except for changing the input dimension 'reshape'\n",
    "        batch_size = s_batch.shape[0]\n",
    "        targets = np.zeros((batch_size, NUM_ACTIONS))\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            targets[i] = self.model.predict(s_batch[i].reshape(1, observation_size*NUM_FRAMES), batch_size = 1)\n",
    "            fut_action = self.target_model.predict(s2_batch[i].reshape(1, observation_size*NUM_FRAMES), batch_size = 1)\n",
    "            targets[i, a_batch[i]] = r_batch[i]\n",
    "            if d_batch[i] == False:\n",
    "                targets[i, a_batch[i]] += DECAY_RATE * np.max(fut_action)\n",
    "        loss = self.model.train_on_batch(s_batch, targets)\n",
    "        # Print the loss every 100 iterations.\n",
    "        if observation_num % 100 == 0:\n",
    "            print(\"We had a loss equal to \", loss)\n",
    "\n",
    "    def save_network(self, path):\n",
    "        # Saves model at specified path as h5 file\n",
    "        # nothing has changed\n",
    "        self.model.save(path)\n",
    "        print(\"Successfully saved network.\")\n",
    "\n",
    "    def load_network(self, path):\n",
    "        # nothing has changed\n",
    "        self.model = load_model(path)\n",
    "        print(\"Succesfully loaded network.\")\n",
    "\n",
    "    def target_train(self):\n",
    "        # nothing has changed from the original implementation\n",
    "        model_weights = self.model.get_weights()\n",
    "        target_model_weights = self.target_model.get_weights()\n",
    "        for i in range(len(model_weights)):\n",
    "            target_model_weights[i] = TAU * model_weights[i] + (1 - TAU) * target_model_weights[i]\n",
    "        self.target_model.set_weights(target_model_weights)\n",
    "        \n",
    "class DuelQ(object):\n",
    "    \"\"\"Constructs the desired deep q learning network\"\"\"\n",
    "    def __init__(self, action_size, lr=0.00001):\n",
    "        # It is not modified from  Abhinav Sagar's code, except for adding the possibility to change the learning rate\n",
    "        # in parameter is also present the size of the action space\n",
    "        # (it used to be a global variable in the original code)\n",
    "        self.action_size = action_size\n",
    "        self.lr_ = lr\n",
    "        self.model = None\n",
    "        self.construct_q_network()\n",
    "\n",
    "    def construct_q_network(self):\n",
    "        # Uses the network architecture found in DeepMind paper\n",
    "        # The inputs and outputs size have changed, as well as replacing the convolution by dense layers.\n",
    "        self.model = Sequential()\n",
    "        \n",
    "        input_layer = Input(shape = (observation_size*NUM_FRAMES,))\n",
    "        lay1 = Dense(observation_size*NUM_FRAMES)(input_layer)\n",
    "        lay1 = Activation('relu')(lay1)\n",
    "        \n",
    "        lay2 = Dense(observation_size)(lay1)\n",
    "        lay2 = Activation('relu')(lay2)\n",
    "        \n",
    "        lay3 = Dense(2*NUM_ACTIONS)(lay2)\n",
    "        lay3 = Activation('relu')(lay3)\n",
    "        \n",
    "        fc1 = Dense(NUM_ACTIONS)(lay3)\n",
    "        advantage = Dense(NUM_ACTIONS)(fc1)\n",
    "        fc2 = Dense(NUM_ACTIONS)(lay3)\n",
    "        value = Dense(1)(fc2)\n",
    "        \n",
    "        meaner = Lambda(lambda x: K.mean(x, axis=1) )\n",
    "        mn_ = meaner(advantage)  \n",
    "        tmp = keras.layers.subtract([advantage, mn_])  # keras doesn't like this part...\n",
    "        policy = keras.layers.add([tmp, value])\n",
    "\n",
    "        self.model = Model(inputs=[input_layer], outputs=[policy])\n",
    "        self.model.compile(loss='mse', optimizer=Adam(lr=self.lr_))\n",
    "\n",
    "        self.target_model = Model(inputs=[input_layer], outputs=[policy])\n",
    "        self.target_model.compile(loss='mse', optimizer=Adam(lr=self.lr_))\n",
    "        print(\"Successfully constructed networks.\")\n",
    "    \n",
    "    def predict_movement(self, data, epsilon):\n",
    "        \"\"\"Predict movement of game controler where is epsilon\n",
    "        probability randomly move.\"\"\"\n",
    "        # only changes lie in adapting the input shape\n",
    "        q_actions = self.model.predict(data.reshape(1, observation_size*NUM_FRAMES), batch_size = 1)\n",
    "        opt_policy = np.argmax(q_actions)\n",
    "        rand_val = np.random.random()\n",
    "        if rand_val < epsilon:\n",
    "            opt_policy = np.random.randint(0, NUM_ACTIONS)\n",
    "        return opt_policy, q_actions[0, opt_policy]\n",
    "\n",
    "    def train(self, s_batch, a_batch, r_batch, d_batch, s2_batch, observation_num):\n",
    "        \"\"\"Trains network to fit given parameters\"\"\"\n",
    "        # nothing has changed except adapting the input shapes\n",
    "        batch_size = s_batch.shape[0]\n",
    "        targets = np.zeros((batch_size, NUM_ACTIONS))\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            targets[i] = self.model.predict(s_batch[i].reshape(1, observation_size*NUM_FRAMES), batch_size = 1)\n",
    "            fut_action = self.target_model.predict(s2_batch[i].reshape(1, observation_size*NUM_FRAMES), batch_size = 1)\n",
    "            targets[i, a_batch[i]] = r_batch[i]\n",
    "            if d_batch[i] == False:\n",
    "                targets[i, a_batch[i]] += DECAY_RATE * np.max(fut_action)\n",
    "\n",
    "        loss = self.model.train_on_batch(s_batch, targets)\n",
    "\n",
    "        # Print the loss every 100 iterations.\n",
    "        if observation_num % 100 == 0:\n",
    "            print(\"We had a loss equal to \", loss)\n",
    "\n",
    "    def save_network(self, path):\n",
    "        # Saves model at specified path as h5 file\n",
    "        # nothing has changed\n",
    "        self.model.save(path)\n",
    "        print(\"Successfully saved network.\")\n",
    "\n",
    "    def load_network(self, path):\n",
    "        # nothing has changed\n",
    "        self.model.load_weights(path)\n",
    "        self.target_model.load_weights(path)\n",
    "        print(\"Succesfully loaded network.\")\n",
    "\n",
    "    def target_train(self):\n",
    "        # nothing has changed\n",
    "        model_weights = self.model.get_weights()\n",
    "        self.target_model.set_weights(model_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.C) Making the code of the Agent and train it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the \"reference\" article [https://towardsdatascience.com/deep-reinforcement-learning-tutorial-with-open-ai-gym-c0de4471f368](https://towardsdatascience.com/deep-reinforcement-learning-tutorial-with-open-ai-gym-c0de4471f368), the author Abhinav Sagar made a dedicated environment based on SpaceInvader in the gym repository. We proceed here on a similar way, but with a the grid2op environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Adapated code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first expose the modify code, for each function we highlight what has changed and what has not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit Abhinav Sagar: \n",
    "# https://github.com/abhinavsagar/Reinforcement-Learning-Tutorial\n",
    "# Code under MIT license, available at:\n",
    "# https://github.com/abhinavsagar/Reinforcement-Learning-Tutorial/blob/master/LICENSE\n",
    "\n",
    "from grid2op.Parameters import Parameters\n",
    "from grid2op.BackendPandaPower import PandaPowerBackend\n",
    "from grid2op.ChronicsHandler import ChronicsHandler, Multifolder, GridStateFromFileWithForecasts\n",
    "from grid2op.Environment import Environment\n",
    "from grid2op.Action import PowerLineSet\n",
    "from grid2op.Reward import L2RPNReward\n",
    "from grid2op.Agent import MLAgent\n",
    "import pdb\n",
    "\n",
    "class DeepQAgent(MLAgent):\n",
    "    # first change: An Agent must derived from grid2op.Agent (in this case MLAgent, because we manipulate vector instead\n",
    "    # of classes)\n",
    "    \n",
    "    def __init__(self, action_space, mode=\"DDQN\"):\n",
    "        # this function has been adapted.\n",
    "        # no environment is created here, but it's rather created in \"train\" method.\n",
    "        \n",
    "        # to built a MLAgent, we need an action_space. No problem, we add it in the constructor.\n",
    "        MLAgent.__init__(self, action_space)\n",
    "        \n",
    "        # easier to access\n",
    "        self.action_space = action_space\n",
    "        self.action_size = action_space.size()\n",
    "        self.do_nothing_act = action_space({})\n",
    "        \n",
    "        # the scales of my inputs varies. I add a \"scaling vector\"\n",
    "        self.scale_vect = np.ones(observation_size)\n",
    "        self.scale_vect[:action_size] = 150\n",
    "        \n",
    "        # and now back to the origin implementation\n",
    "        self.replay_buffer = ReplayBuffer(BUFFER_SIZE)\n",
    "        \n",
    "        # Construct appropriate network based on flags\n",
    "        if mode == \"DDQN\":\n",
    "            self.deep_q = DeepQ(action_space.size())\n",
    "        elif mode == \"DQN\":\n",
    "#             raise RuntimeError(\"Implementation does not work yet\")\n",
    "            self.deep_q = DuelQ(action_space.size())\n",
    "\n",
    "    def _ml_act(self, observation, reward, done=False):\n",
    "        # method dedicated to grid2op, to use the MLAgent.\n",
    "        # i predict the action using the internal neural network\n",
    "        predict_movement_int, *_ = self.deep_q.predict_movement(observation[obs_to_model]*self.scale_vect, epsilon=0.)\n",
    "        # in the line above, we set the \"exploration\" parameter epsilon to \"0\" when using the agent.\n",
    "        # then convert it to a proper vector with the dedicated method (see bellow)\n",
    "        predict_movement_vect = self.opt_policy_to_action_vect(predict_movement_int)\n",
    "        return predict_movement_vect\n",
    "\n",
    "    def opt_policy_to_action_vect(self, opt_policy):\n",
    "        # helper to convert action, as returned as integer, to vector.\n",
    "        # DQN or DDQN will output a number (which is the choosen action)\n",
    "        # i need to convert it to a proper representation of an action.\n",
    "        # please refer to cell where  \"action space\" is defined, section II.B.a\n",
    "        res = np.zeros(self.action_size)\n",
    "        if opt_policy == 0:\n",
    "            # hard encode \"do nothing\"\n",
    "            pass\n",
    "        elif opt_policy < self.action_space.size():\n",
    "            # reconnect a powerline\n",
    "            res[opt_policy-1] = 1\n",
    "        else:\n",
    "            # disconnect a powerline\n",
    "            res[opt_policy-self.action_space.size()-1] = -1\n",
    "        return res\n",
    "    \n",
    "    def load_network(self, path):\n",
    "        # not modified compare to original implementation\n",
    "        self.deep_q.load_network(path)\n",
    "    \n",
    "    def convert_process_buffer(self):\n",
    "        \"\"\"Converts the list of NUM_FRAMES images in the process buffer\n",
    "        into one training sample\"\"\"\n",
    "        # here i simply concatenate the action in case of multiple action in the \"buffer\"\n",
    "        # this function existed in the original implementation, bus has been adapted.\n",
    "        return np.concatenate(self.process_buffer)\n",
    "    \n",
    "    def _build_valid_env(self, env=None):\n",
    "        # now we are creating a valid Environment\n",
    "        # it's mandatory because no environment are created when the agent is \n",
    "        # an Agent should not get direct access to the environment, but can interact with it only by:\n",
    "        # * receiving reward\n",
    "        # * receiving observation\n",
    "        # * sending action\n",
    "        if env is None:\n",
    "            param = Parameters()\n",
    "            backend = PandaPowerBackend()\n",
    "            data_feeding = ChronicsHandler(chronicsClass=Multifolder,\n",
    "                                           path=grid2op.CHRONICS_MLUTIEPISODE,\n",
    "                                           gridvalueClass=GridStateFromFileWithForecasts)\n",
    "\n",
    "            env = Environment(init_grid_path=grid2op.CASE_14_FILE,\n",
    "                                   chronics_handler=data_feeding,\n",
    "                                   backend=backend,\n",
    "                                   parameters=param,\n",
    "                                   names_chronics_to_backend=grid2op.NAMES_CHRONICS_TO_BACKEND,\n",
    "                                  actionClass=PowerLineSet,\n",
    "                                  rewardClass=L2RPNReward) \n",
    "        # I make sure the action space of the user and \n",
    "        if not isinstance(self.action_space, type(env.action_space)):\n",
    "            raise RuntimeError(\"Imposssible to build an agent with 2 different action space\")\n",
    "        if not isinstance(env.action_space, type(self.action_space)):\n",
    "            raise RuntimeError(\"Imposssible to build an agent with 2 different action space\")\n",
    "        # make sure the environment is reset\n",
    "        env.reset() \n",
    "        \n",
    "        # A buffer that keeps the last 3 images\n",
    "        self.replay_buffer.clear()\n",
    "        self.process_buffer = []\n",
    "        for _ in range(NUM_FRAMES):\n",
    "            # Initialize buffer with the first frames\n",
    "            s1, r1, _, _ = env.step(self.do_nothing_act)\n",
    "            s1_vect = s1.to_vect()\n",
    "            # all observation will that will be used by the agent will be\n",
    "            # of the shape vect_[obs_to_modl]*self.scale_vect\n",
    "            self.process_buffer.append(s1_vect[obs_to_model]*self.scale_vect)\n",
    "            \n",
    "        return env\n",
    "    \n",
    "    def train(self, num_frames, env=None):\n",
    "        # this function existed in the original implementation, but has been slightly adapted.\n",
    "        \n",
    "        close_env = env is None\n",
    "        # first we create an environment or make sure the given environment is valid\n",
    "        env = self._build_valid_env(env)\n",
    "        \n",
    "        # bellow that, only slight modification has been made. They are highlighted\n",
    "        observation_num = 0\n",
    "        curr_state = self.convert_process_buffer()\n",
    "        epsilon = INITIAL_EPSILON\n",
    "        alive_frame = 0\n",
    "        total_reward = 0\n",
    "\n",
    "        while observation_num < num_frames:\n",
    "            if observation_num % 1000 == 999:\n",
    "                print((\"Executing loop %d\" %observation_num))\n",
    "\n",
    "            # Slowly decay the learning rate\n",
    "            if epsilon > FINAL_EPSILON:\n",
    "                epsilon -= (INITIAL_EPSILON-FINAL_EPSILON)/EPSILON_DECAY\n",
    "\n",
    "            initial_state = self.convert_process_buffer()\n",
    "            self.process_buffer = []\n",
    "\n",
    "            # it's a bit less convenient that using the SpaceInvader environment.\n",
    "            # first we need to predict which actions to do (represented as an integer)\n",
    "            predict_movement_int, predict_q_value = self.deep_q.predict_movement(curr_state, epsilon)\n",
    "            # then we need to convert it to a valid vector that can represent a grid2op action\n",
    "            predict_movement_vect = self.opt_policy_to_action_vect(predict_movement_int)\n",
    "            # then we need to convert it to a proper action\n",
    "            predict_movement = self.convert_from_vect(predict_movement_vect)\n",
    "            \n",
    "            reward, done = 0, False\n",
    "            for i in range(NUM_FRAMES):\n",
    "                temp_observation_obj, temp_reward, temp_done, _ = env.step(predict_movement)\n",
    "                # here it has been adapted too. The observation get from the environment is\n",
    "                # first converted to vector\n",
    "                temp_observation = temp_observation_obj.to_vect()\n",
    "                # then only a subpart of it is used, and it is scaled to have proper values\n",
    "                temp_observation = temp_observation[obs_to_model]*self.scale_vect\n",
    "                \n",
    "                # below this line no changed have been made to the original implementation.\n",
    "                reward += temp_reward\n",
    "                self.process_buffer.append(temp_observation)\n",
    "                done = done | temp_done\n",
    "\n",
    "            if done:\n",
    "                print(\"Lived with maximum time \", alive_frame)\n",
    "                print(\"Earned a total of reward equal to \", total_reward)\n",
    "                # reset the environment\n",
    "                env.reset()\n",
    "                \n",
    "                alive_frame = 0\n",
    "                total_reward = 0\n",
    "\n",
    "            new_state = self.convert_process_buffer()\n",
    "            self.replay_buffer.add(initial_state, predict_movement_int, reward, done, new_state)\n",
    "            total_reward += reward\n",
    "            if self.replay_buffer.size() > MIN_OBSERVATION:\n",
    "                s_batch, a_batch, r_batch, d_batch, s2_batch = self.replay_buffer.sample(MINIBATCH_SIZE)\n",
    "                self.deep_q.train(s_batch, a_batch, r_batch, d_batch, s2_batch, observation_num)\n",
    "                self.deep_q.target_train()\n",
    "\n",
    "            # Save the network every 100000 iterations\n",
    "            if observation_num % 10000 == 9999 or observation_num == num_frames-1:\n",
    "                print(\"Saving Network\")\n",
    "                self.deep_q.save_network(\"saved.h5\")\n",
    "\n",
    "            alive_frame += 1\n",
    "            observation_num += 1\n",
    "        if close_env:\n",
    "            env.close()\n",
    "            \n",
    "    def calculate_mean(self, num_episode = 100, env=None):\n",
    "        # this method has been only slightly adapted from the original implementation\n",
    "        \n",
    "        close_env = env is None\n",
    "        \n",
    "        # Note that it is NOT the recommended method to evaluate an Agent. Please use \"Grid2Op.Runner\" instead\n",
    "        \n",
    "        # first we create an environment or make sure the given environment is valid\n",
    "        env = self._build_valid_env(env)\n",
    "        \n",
    "        reward_list = []\n",
    "        print(\"Printing scores of each trial\")\n",
    "        for i in range(num_episode):\n",
    "            done = False\n",
    "            tot_award = 0\n",
    "            self.env.reset()\n",
    "            while not done:\n",
    "                state = self.convert_process_buffer()\n",
    "                \n",
    "                # same adapation as in \"train\" function. \n",
    "                predict_movement_int = self.deep_q.predict_movement(state, 0.0)[0]\n",
    "                predict_movement_vect = self.opt_policy_to_action_vect(predict_movement_int)\n",
    "                predict_movement = self.convert_from_vect(predict_movement_vect)\n",
    "                \n",
    "                # same adapation as in the \"train\" funciton\n",
    "                observation_obj, reward, done, _ = self.env.step(predict_movement)\n",
    "                observation_vect_full = observation_obj.to_vect()\n",
    "                observation = observation_vect_full[obs_to_model]*self.scale_vect\n",
    "                \n",
    "                tot_award += reward\n",
    "                self.process_buffer.append(observation)\n",
    "                self.process_buffer = self.process_buffer[1:]\n",
    "            print(tot_award)\n",
    "            reward_list.append(tot_award)\n",
    "            \n",
    "        if close_env:\n",
    "            env.close()\n",
    "        return np.mean(reward_list), np.std(reward_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define the model (agent), and then train it.\n",
    "\n",
    "This is done exactly the same way as in the Abhinav Sagar implementation.\n",
    "\n",
    "**NB** The code bellow can take a few minutes to run. It's training a Deep Reinforcement Learning Agent afterall. It this takes too long on your machine, you can always decrease the \"nb_frame\", and set it to 1000 for example. In this case, the Agent will probably not be really good.\n",
    "\n",
    "**NB** For a real Agent, it would take much longer to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/donnotben/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Successfully constructed networks.\n",
      "Lived with maximum time  125\n",
      "Earned a total of reward equal to  2499.970171451901\n",
      "Lived with maximum time  61\n",
      "Earned a total of reward equal to  1199.9878237724658\n",
      "Lived with maximum time  173\n",
      "Earned a total of reward equal to  3439.929580285706\n",
      "Lived with maximum time  194\n",
      "Earned a total of reward equal to  3859.940037430744\n",
      "Lived with maximum time  99\n",
      "Earned a total of reward equal to  1959.9754147254969\n",
      "Lived with maximum time  64\n",
      "Earned a total of reward equal to  1259.9902110672824\n",
      "Lived with maximum time  135\n",
      "Earned a total of reward equal to  2679.9678879327025\n",
      "Lived with maximum time  119\n",
      "Earned a total of reward equal to  2359.976150465088\n",
      "Executing loop 999\n",
      "WARNING:tensorflow:From /home/donnotben/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "We had a loss equal to  9.828829\n",
      "Lived with maximum time  64\n",
      "Earned a total of reward equal to  1259.987669164445\n",
      "Lived with maximum time  19\n",
      "Earned a total of reward equal to  359.9963633466135\n",
      "We had a loss equal to  9.69742\n",
      "We had a loss equal to  9.684363\n",
      "Lived with maximum time  208\n",
      "Earned a total of reward equal to  4139.932547550526\n",
      "We had a loss equal to  9.789543\n",
      "Lived with maximum time  98\n",
      "Earned a total of reward equal to  1939.9780174583925\n",
      "We had a loss equal to  9.790991\n",
      "We had a loss equal to  9.767244\n",
      "Lived with maximum time  226\n",
      "Earned a total of reward equal to  4499.930645245781\n",
      "We had a loss equal to  9.961723\n",
      "Lived with maximum time  95\n",
      "Earned a total of reward equal to  1879.9793267491393\n",
      "We had a loss equal to  9.885097\n",
      "We had a loss equal to  9.661528\n",
      "We had a loss equal to  10.154638\n",
      "Lived with maximum time  230\n",
      "Earned a total of reward equal to  4579.95759460665\n",
      "Executing loop 1999\n",
      "We had a loss equal to  9.731823\n",
      "Lived with maximum time  189\n",
      "Earned a total of reward equal to  3759.958038852733\n",
      "We had a loss equal to  10.341629\n",
      "We had a loss equal to  10.144154\n",
      "Lived with maximum time  198\n",
      "Earned a total of reward equal to  3939.9608140714545\n",
      "We had a loss equal to  10.826862\n",
      "We had a loss equal to  10.88633\n",
      "We had a loss equal to  10.994096\n",
      "Lived with maximum time  212\n",
      "Earned a total of reward equal to  4219.940512581315\n",
      "Lived with maximum time  53\n",
      "Earned a total of reward equal to  1039.9884980105076\n",
      "We had a loss equal to  11.432173\n",
      "We had a loss equal to  10.775183\n",
      "Lived with maximum time  158\n",
      "Earned a total of reward equal to  3139.9673486079805\n",
      "We had a loss equal to  13.657351\n",
      "Lived with maximum time  144\n",
      "Earned a total of reward equal to  2859.9739022412155\n",
      "We had a loss equal to  11.627981\n",
      "Executing loop 2999\n",
      "We had a loss equal to  13.02446\n",
      "Lived with maximum time  139\n",
      "Earned a total of reward equal to  2759.961833769994\n",
      "Lived with maximum time  84\n",
      "Earned a total of reward equal to  1659.98574579795\n",
      "We had a loss equal to  13.40057\n",
      "We had a loss equal to  14.801549\n",
      "Lived with maximum time  175\n",
      "Earned a total of reward equal to  3479.952459904398\n",
      "We had a loss equal to  20.389349\n",
      "We had a loss equal to  25.228336\n",
      "We had a loss equal to  25.335625\n",
      "Lived with maximum time  267\n",
      "Earned a total of reward equal to  5319.90863875203\n",
      "We had a loss equal to  26.291199\n",
      "Lived with maximum time  119\n",
      "Earned a total of reward equal to  2359.963747883004\n",
      "We had a loss equal to  39.84664\n",
      "Lived with maximum time  138\n",
      "Earned a total of reward equal to  2739.974608668652\n",
      "We had a loss equal to  66.83873\n",
      "Lived with maximum time  33\n",
      "Earned a total of reward equal to  639.9944661996352\n",
      "We had a loss equal to  35.471992\n",
      "Executing loop 3999\n",
      "We had a loss equal to  78.90253\n",
      "We had a loss equal to  44.950745\n",
      "Lived with maximum time  287\n",
      "Earned a total of reward equal to  5719.92323414546\n",
      "Lived with maximum time  67\n",
      "Earned a total of reward equal to  1339.9856069631107\n",
      "We had a loss equal to  97.589874\n",
      "Lived with maximum time  63\n",
      "Earned a total of reward equal to  1239.990317304687\n",
      "We had a loss equal to  131.38708\n",
      "Lived with maximum time  96\n",
      "Earned a total of reward equal to  1899.9827645895598\n",
      "Lived with maximum time  4\n",
      "Earned a total of reward equal to  59.999353274506\n",
      "Lived with maximum time  8\n",
      "Earned a total of reward equal to  139.99854706877377\n",
      "We had a loss equal to  112.21759\n",
      "We had a loss equal to  122.86586\n",
      "Lived with maximum time  216\n",
      "Earned a total of reward equal to  4299.9620568031905\n",
      "We had a loss equal to  233.75197\n",
      "We had a loss equal to  157.24373\n",
      "Lived with maximum time  237\n",
      "Earned a total of reward equal to  4719.918176372806\n",
      "We had a loss equal to  424.8252\n",
      "We had a loss equal to  623.22095\n",
      "Lived with maximum time  168\n",
      "Earned a total of reward equal to  3339.9450387152783\n",
      "Executing loop 4999\n",
      "We had a loss equal to  174.47708\n",
      "Lived with maximum time  45\n",
      "Earned a total of reward equal to  879.9909738520446\n",
      "Lived with maximum time  62\n",
      "Earned a total of reward equal to  1219.9884485351276\n",
      "Lived with maximum time  5\n",
      "Earned a total of reward equal to  79.99914568116208\n",
      "We had a loss equal to  436.45047\n",
      "We had a loss equal to  491.39075\n",
      "Lived with maximum time  187\n",
      "Earned a total of reward equal to  3719.9558979119743\n",
      "Lived with maximum time  32\n",
      "Earned a total of reward equal to  619.9946187865266\n",
      "We had a loss equal to  757.5478\n",
      "Lived with maximum time  35\n",
      "Earned a total of reward equal to  679.9941723747406\n",
      "We had a loss equal to  328.7059\n",
      "We had a loss equal to  651.5939\n",
      "Lived with maximum time  174\n",
      "Earned a total of reward equal to  3459.956471517897\n",
      "Lived with maximum time  84\n",
      "Earned a total of reward equal to  1659.974233759262\n",
      "We had a loss equal to  865.89233\n",
      "Lived with maximum time  16\n",
      "Earned a total of reward equal to  299.9971602563954\n",
      "Lived with maximum time  50\n",
      "Earned a total of reward equal to  979.9913470364712\n",
      "We had a loss equal to  872.39954\n",
      "We had a loss equal to  1293.645\n",
      "We had a loss equal to  920.9718\n",
      "Lived with maximum time  287\n",
      "Earned a total of reward equal to  5719.900167177057\n",
      "Executing loop 5999\n",
      "We had a loss equal to  2300.422\n",
      "Lived with maximum time  116\n",
      "Earned a total of reward equal to  2319.974825415157\n",
      "We had a loss equal to  1734.6188\n",
      "Lived with maximum time  99\n",
      "Earned a total of reward equal to  1959.9697580924396\n",
      "We had a loss equal to  2877.4373\n",
      "Lived with maximum time  111\n",
      "Earned a total of reward equal to  2199.978512824354\n",
      "We had a loss equal to  4234.2056\n",
      "Lived with maximum time  88\n",
      "Earned a total of reward equal to  1739.986141139957\n",
      "We had a loss equal to  3551.1406\n",
      "Lived with maximum time  95\n",
      "Earned a total of reward equal to  1879.9735606306497\n",
      "We had a loss equal to  2342.0847\n",
      "Lived with maximum time  96\n",
      "Earned a total of reward equal to  1899.9835968278583\n",
      "We had a loss equal to  3202.7002\n",
      "Lived with maximum time  140\n",
      "Earned a total of reward equal to  2779.958686058893\n",
      "We had a loss equal to  1105.7781\n",
      "Lived with maximum time  96\n",
      "Earned a total of reward equal to  1899.9790080373032\n",
      "We had a loss equal to  3915.1626\n",
      "We had a loss equal to  4732.919\n",
      "Executing loop 6999\n",
      "Lived with maximum time  216\n",
      "Earned a total of reward equal to  4299.947281657801\n",
      "We had a loss equal to  5388.656\n",
      "We had a loss equal to  3604.0645\n",
      "We had a loss equal to  4061.0444\n",
      "Lived with maximum time  212\n",
      "Earned a total of reward equal to  4219.960967335936\n",
      "We had a loss equal to  5248.313\n",
      "We had a loss equal to  3766.7075\n",
      "Lived with maximum time  202\n",
      "Earned a total of reward equal to  4019.948225280083\n",
      "We had a loss equal to  8532.006\n",
      "We had a loss equal to  1328.27\n",
      "Lived with maximum time  209\n",
      "Earned a total of reward equal to  4159.9604322056675\n",
      "We had a loss equal to  4053.4202\n",
      "Lived with maximum time  97\n",
      "Earned a total of reward equal to  1919.982863697518\n",
      "We had a loss equal to  4154.7656\n",
      "Lived with maximum time  87\n",
      "Earned a total of reward equal to  1719.9860288487373\n",
      "We had a loss equal to  5192.744\n",
      "Lived with maximum time  187\n",
      "Earned a total of reward equal to  3719.9499838331394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing loop 7999\n",
      "We had a loss equal to  6671.618\n",
      "Lived with maximum time  10\n",
      "Earned a total of reward equal to  179.99796826156629\n",
      "Lived with maximum time  95\n",
      "Earned a total of reward equal to  1879.9815305760528\n",
      "We had a loss equal to  5807.167\n",
      "Lived with maximum time  59\n",
      "Earned a total of reward equal to  1159.9881978100568\n",
      "We had a loss equal to  6422.2705\n",
      "Lived with maximum time  70\n",
      "Earned a total of reward equal to  1379.9879966035892\n",
      "We had a loss equal to  5534.8604\n",
      "Lived with maximum time  106\n",
      "Earned a total of reward equal to  2099.9826749762196\n",
      "Lived with maximum time  59\n",
      "Earned a total of reward equal to  1159.9912314095448\n",
      "We had a loss equal to  3258.7783\n",
      "We had a loss equal to  8522.704\n",
      "Lived with maximum time  137\n",
      "Earned a total of reward equal to  2719.9727119623044\n",
      "We had a loss equal to  8625.047\n",
      "Lived with maximum time  116\n",
      "Earned a total of reward equal to  2299.974704627136\n",
      "We had a loss equal to  3646.3096\n",
      "We had a loss equal to  2476.2727\n",
      "Lived with maximum time  219\n",
      "Earned a total of reward equal to  4359.934246285315\n",
      "We had a loss equal to  9017.887\n",
      "Lived with maximum time  122\n",
      "Earned a total of reward equal to  2419.9770684608943\n",
      "Executing loop 8999\n",
      "We had a loss equal to  7234.896\n",
      "We had a loss equal to  1958.104\n",
      "Lived with maximum time  133\n",
      "Earned a total of reward equal to  2639.9583834104337\n",
      "We had a loss equal to  2249.0046\n",
      "Lived with maximum time  85\n",
      "Earned a total of reward equal to  1679.97626768511\n",
      "We had a loss equal to  2873.57\n",
      "Lived with maximum time  108\n",
      "Earned a total of reward equal to  2139.982288434917\n",
      "Lived with maximum time  9\n",
      "Earned a total of reward equal to  159.99826503643249\n",
      "We had a loss equal to  2496.422\n",
      "We had a loss equal to  11144.66\n",
      "Lived with maximum time  201\n",
      "Earned a total of reward equal to  3999.95277378978\n",
      "We had a loss equal to  8154.528\n",
      "Lived with maximum time  103\n",
      "Earned a total of reward equal to  2039.9803988872864\n",
      "We had a loss equal to  13656.809\n",
      "Lived with maximum time  136\n",
      "Earned a total of reward equal to  2699.9645212362157\n",
      "We had a loss equal to  3773.955\n",
      "Lived with maximum time  44\n",
      "Earned a total of reward equal to  859.9928325266757\n",
      "We had a loss equal to  4242.793\n",
      "Lived with maximum time  125\n",
      "Earned a total of reward equal to  2479.97038431825\n",
      "Executing loop 9999\n",
      "Saving Network\n",
      "Successfully saved network.\n"
     ]
    }
   ],
   "source": [
    "nb_frame = 10000\n",
    "my_agent = DeepQAgent(runner.env.action_space, mode=\"DQN\")\n",
    "my_agent.train(nb_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III) Evaluating the Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, time to test this trained agent.\n",
    "\n",
    "To do that, we have multiple choices.\n",
    "\n",
    "Either we recode the \"DeepQAgent\" class to load the stored weights (that have been saved during trainig) when it is initialized (not covered in this notebook), or we can also directly specified the \"instance\" of the Agent to use in the Grid2Op Runner.\n",
    "\n",
    "To do that, it's fairly simple. First, you need to specify that you won't use the \"*agentClass*\" argument, by setting it to ``None``, and secondly you simply provide the agent to use in the *agentInstance* argument.\n",
    "\n",
    "**NB** If you don't do that, the Runner will be created (the constructor will raise an exception). And if you choose to use the \"*agentClass*\" argument, your agent will be reloaded from scratch. So **if it doesn't load the weights** it will behave as a non trained agent, unlikely to perform well on the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.A) Evaluate the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a runner\n",
    "runner = Runner(init_grid_path=grid2op.CASE_14_FILE, # this should be the same grid as the one the agent is trained one\n",
    "                path_chron=grid2op.CHRONICS_MLUTIEPISODE,  # chronics can changed of course\n",
    "                gridStateclass=Multifolder, # the class of chronics can changed too\n",
    "                gridStateclass_kwargs={\"gridvalueClass\": GridStateFromFileWithForecasts},  # so this can changed too\n",
    "                names_chronics_to_backend = grid2op.NAMES_CHRONICS_TO_BACKEND,  # this also can changed\n",
    "                agentInstance=my_agent,  # here i pass a trained agent, no need to read it from the \n",
    "                agentClass=None,  # if i use an instance of Agent, i cannot provide a class\n",
    "                rewardClass=L2RPNReward,  # this can be anything, not necessarily the same for training\n",
    "                actionClass=PowerLineSet  # this should be the same as the one used for training.\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the Agent and save the results. As opposed to the multiple times we exposed the \"runner.run\" call, we never really dive into the \"path_save\" argument. This path allows you to save lots of information about your Agent behaviour. Please All the informations present are shown on the documentation [here](file:///home/donnotben/Documents/Grid2Op/documentation/html/runner.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the trained agent are:\n",
      "\tFor chronics located at /home/donnotben/.local/lib/python3.6/site-packages/grid2op/data/test_multi_chronics/1\n",
      "\t\t - cumulative reward: 5739.951023\n",
      "\t\t - number of time steps completed: 287 / 287\n"
     ]
    }
   ],
   "source": [
    "# initialize it\n",
    "res = runner.run(nb_episode=1, path_save=\"trained_agent_log\")\n",
    "print(\"The results for the trained agent are:\")\n",
    "for chron_name, cum_reward, nb_time_step, max_ts in res:\n",
    "    msg_tmp = \"\\tFor chronics located at {}\\n\".format(chron_name)\n",
    "    msg_tmp += \"\\t\\t - cumulative reward: {:.6f}\\n\".format(cum_reward)\n",
    "    msg_tmp += \"\\t\\t - number of time steps completed: {:.0f} / {:.0f}\".format(nb_time_step, max_ts)\n",
    "    print(msg_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.B) Inspect the Agent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please refer to the official document for more information about the content of the directory where the data are saved. Note that the saving of the information is triggered by the \"path_save\" argument sent to the \"runner.run\" function.\n",
    "\n",
    "Some information that will be present in this repository are:\n",
    "If enabled, the :class:`Runner` will save the information in a structured way. For each episode there will be a folder\n",
    "with:\n",
    "\n",
    "  - \"episode_meta.json\" that represents some meta information about:\n",
    "\n",
    "    - \"backend_type\": the name of the `grid2op.Backend` class used\n",
    "    - \"chronics_max_timestep\": the **maximum** number of timestep for the chronics used\n",
    "    - \"chronics_path\": the path where the temporal data (chronics) are located\n",
    "    - \"env_type\": the name of the `grid2op.Environment` class used.\n",
    "    - \"grid_path\": the path where the powergrid has been loaded from\n",
    "\n",
    "  - \"episode_times.json\": gives some information about the total time spend in multiple part of the runner, mainly the\n",
    "    `grid2op.Agent` (and especially its method `grid2op.Agent.act`) and amount of time spent in the\n",
    "    `grid2op.Environment`\n",
    "\n",
    "  - \"_parameters.json\": is a representation as json of a the `grid2op.Parameters` used for this episode\n",
    "  - \"rewards.npy\" is a numpy 1d array giving the rewards at each time step. We adopted the convention that the stored\n",
    "    reward at index `i` is the one observed by the agent at time `i` and **NOT** the reward sent by the\n",
    "    `grid2op.Environment` after the action has been implemented.\n",
    "  - \"exec_times.npy\" is a numpy 1d array giving the execution time of each time step of the episode\n",
    "  - \"actions.npy\" gives the actions that has been taken by the `grid2op.Agent`. At row `i` of \"actions.npy\" is a\n",
    "    vectorized representation of the action performed by the agent at timestep `i` *ie.* **after** having observed\n",
    "    the observation present at row `i` of \"observation.npy\" and the reward showed in row `i` of \"rewards.npy\".\n",
    "  - \"disc_lines.npy\" gives which lines have been disconnected during the simulation of the cascading failure at each\n",
    "    time step. The same convention as for \"rewards.npy\" has been adopted. This means that the powerlines are\n",
    "    disconnected when the `grid2op.Agent` takes the `grid2op.Action` at time step `i`.\n",
    "  - \"observations.npy\" is a numpy 2d array reprensenting the `grid2op.Observation` at the disposal of the\n",
    "    `grid2op.Agent` when he took his action.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can first look at the repository were the data are stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\r\n"
     ]
    }
   ],
   "source": [
    "!ls trained_agent_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there is only one folder there. It's named \"1\" because, in the original data, this came from the folder named \"1\" (the original data are located at \"/home/donnotben/.local/lib/python3.6/site-packages/grid2op/data/test_multi_chronics/\")\n",
    "\n",
    "If there were multiple episode, each episode would have it's own folder, with a name as resemblant as possible to the origin name of the data. This is done to ease the studying of the results.\n",
    "\n",
    "Now let's see what is inside this folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions.npy\t\t\t  episode_meta.json   _parameters.json\r\n",
      "agent_exec_times.npy\t\t  episode_times.json  rewards.npy\r\n",
      "disc_lines_cascading_failure.npy  observations.npy\r\n"
     ]
    }
   ],
   "source": [
    "!ls trained_agent_log/1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can for example load the \"actions\" performed by the Agent, and have a look at them.\n",
    "\n",
    "To do that we will load the action array (represented as vector) and use the action_space to convert it back into valid action class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_actions = np.load(os.path.join(\"trained_agent_log\", \"1\", \"actions.npy\"))\n",
    "li_actions = []\n",
    "for i in range(all_actions.shape[0]):\n",
    "    tmp = runner.env.action_space.from_vect(all_actions[i,:])\n",
    "    li_actions.append(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows us to have a deeper look at the action, and their effect. Note that here, we used action that can only **set** the line status, so looking at their effect is pretty straightforward.\n",
    "\n",
    "Also, note that as oppose to \"change\", if a powerline is already connected, trying to **set** it as connected has absolutely no impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "287"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_disc = 0\n",
    "line_reco = 0\n",
    "for act in li_actions:\n",
    "    dict_ = act.as_dict()\n",
    "    if \"set_line_status\" in dict_:\n",
    "        line_reco +=  dict_[\"set_line_status\"][\"nb_connected\"]\n",
    "        line_disc +=  dict_[\"set_line_status\"][\"nb_disconnected\"]\n",
    "line_reco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As wa can see for our event, the agent always try to reconnect a powerline. As all lines are alway reconnected, this Agent does basically nothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do the same kind of post analysis for the observation, even though here, as the observations come from files, it's probably not particularly intersting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_observations = np.load(os.path.join(\"trained_agent_log\", \"1\", \"observations.npy\"))\n",
    "li_observations = []\n",
    "nb_real_disc = 0\n",
    "for i in range(all_observations.shape[0]):\n",
    "    tmp = runner.env.observation_space.from_vect(all_observations[i,:])\n",
    "    li_observations.append(tmp)\n",
    "    nb_real_disc += (np.sum(tmp.line_status) - tmp.line_status.shape[0])\n",
    "nb_real_disc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
