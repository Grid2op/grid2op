{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actions as vector, and RL agent training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Disclaimer***: This file referenced some files in other directories. In order to have working cross referencing it's recommended to start the notebook server from the root directory (`Grid2Op`) of the package and __not__ in the `getting_started` sub directory:\n",
    "```bash\n",
    "cd Grid2Op\n",
    "jupyter notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***NB*** For more information about how to use the package, a general help can be built locally (provided that sphinx is installed on the machine) with:\n",
    "```bash\n",
    "cd Grid2Op\n",
    "make html\n",
    "```\n",
    "from the top directory of the package (usually `Grid2Op`).\n",
    "\n",
    "Once build, the help can be access from [here](../documentation/html/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is recommended to have a look at the [0_basic_functionalities](0_basic_functionalities.ipynb), [1_Observation_Agents](1_Observation_Agents.ipynb) and [2_Action_GridManipulation](2_Action_GridManipulation.ipynb) notebooks before getting into this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objectives**\n",
    "\n",
    "In this notebook we will expose :\n",
    "* how to use the \"vectorized\" verions of Observations and Actions\n",
    "* how to train a (stupid) Agent using reinforcement learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import grid2op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"my_id_menu_nb\">run previous cell, wait for 2 seconds</div>\n",
       "<script>\n",
       "function repeat_indent_string(n){\n",
       "    var a = \"\" ;\n",
       "    for ( ; n > 0 ; --n)\n",
       "        a += \"    \";\n",
       "    return a;\n",
       "}\n",
       "// look up into all sections and builds an automated menu //\n",
       "var update_menu_string = function(begin, lfirst, llast, sformat, send, keep_item, begin_format, end_format) {\n",
       "    var anchors = document.getElementsByClassName(\"section\");\n",
       "    if (anchors.length == 0) {\n",
       "        anchors = document.getElementsByClassName(\"text_cell_render rendered_html\");\n",
       "    }\n",
       "    var i,t;\n",
       "    var text_menu = begin;\n",
       "    var text_memo = \"<pre>\\nlength:\" + anchors.length + \"\\n\";\n",
       "    var ind = \"\";\n",
       "    var memo_level = 1;\n",
       "    var href;\n",
       "    var tags = [];\n",
       "    var main_item = 0;\n",
       "    var format_open = 0;\n",
       "    for (i = 0; i <= llast; i++)\n",
       "        tags.push(\"h\" + i);\n",
       "\n",
       "    for (i = 0; i < anchors.length; i++) {\n",
       "        text_memo += \"**\" + anchors[i].id + \"--\\n\";\n",
       "\n",
       "        var child = null;\n",
       "        for(t = 0; t < tags.length; t++) {\n",
       "            var r = anchors[i].getElementsByTagName(tags[t]);\n",
       "            if (r.length > 0) {\n",
       "child = r[0];\n",
       "break;\n",
       "            }\n",
       "        }\n",
       "        if (child == null) {\n",
       "            text_memo += \"null\\n\";\n",
       "            continue;\n",
       "        }\n",
       "        if (anchors[i].hasAttribute(\"id\")) {\n",
       "            // when converted in RST\n",
       "            href = anchors[i].id;\n",
       "            text_memo += \"#1-\" + href;\n",
       "            // passer à child suivant (le chercher)\n",
       "        }\n",
       "        else if (child.hasAttribute(\"id\")) {\n",
       "            // in a notebook\n",
       "            href = child.id;\n",
       "            text_memo += \"#2-\" + href;\n",
       "        }\n",
       "        else {\n",
       "            text_memo += \"#3-\" + \"*\" + \"\\n\";\n",
       "            continue;\n",
       "        }\n",
       "        var title = child.textContent;\n",
       "        var level = parseInt(child.tagName.substring(1,2));\n",
       "\n",
       "        text_memo += \"--\" + level + \"?\" + lfirst + \"--\" + title + \"\\n\";\n",
       "\n",
       "        if ((level < lfirst) || (level > llast)) {\n",
       "            continue ;\n",
       "        }\n",
       "        if (title.endsWith('¶')) {\n",
       "            title = title.substring(0,title.length-1).replace(\"<\", \"&lt;\")\n",
       "         .replace(\">\", \"&gt;\").replace(\"&\", \"&amp;\");\n",
       "        }\n",
       "        if (title.length == 0) {\n",
       "            continue;\n",
       "        }\n",
       "\n",
       "        while (level < memo_level) {\n",
       "            text_menu += end_format + \"</ul>\\n\";\n",
       "            format_open -= 1;\n",
       "            memo_level -= 1;\n",
       "        }\n",
       "        if (level == lfirst) {\n",
       "            main_item += 1;\n",
       "        }\n",
       "        if (keep_item != -1 && main_item != keep_item + 1) {\n",
       "            // alert(main_item + \" - \" + level + \" - \" + keep_item);\n",
       "            continue;\n",
       "        }\n",
       "        while (level > memo_level) {\n",
       "            text_menu += \"<ul>\\n\";\n",
       "            memo_level += 1;\n",
       "        }\n",
       "        text_menu += repeat_indent_string(level-2);\n",
       "        text_menu += begin_format + sformat.replace(\"__HREF__\", href).replace(\"__TITLE__\", title);\n",
       "        format_open += 1;\n",
       "    }\n",
       "    while (1 < memo_level) {\n",
       "        text_menu += end_format + \"</ul>\\n\";\n",
       "        memo_level -= 1;\n",
       "        format_open -= 1;\n",
       "    }\n",
       "    text_menu += send;\n",
       "    //text_menu += \"\\n\" + text_memo;\n",
       "\n",
       "    while (format_open > 0) {\n",
       "        text_menu += end_format;\n",
       "        format_open -= 1;\n",
       "    }\n",
       "    return text_menu;\n",
       "};\n",
       "var update_menu = function() {\n",
       "    var sbegin = \"\";\n",
       "    var sformat = '<a href=\"#__HREF__\">__TITLE__</a>';\n",
       "    var send = \"\";\n",
       "    var begin_format = '<li>';\n",
       "    var end_format = '</li>';\n",
       "    var keep_item = -1;\n",
       "    var text_menu = update_menu_string(sbegin, 2, 4, sformat, send, keep_item,\n",
       "       begin_format, end_format);\n",
       "    var menu = document.getElementById(\"my_id_menu_nb\");\n",
       "    menu.innerHTML=text_menu;\n",
       "};\n",
       "window.setTimeout(update_menu,2000);\n",
       "            </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = None\n",
    "try:\n",
    "    from jyquickhelper import add_notebook_menu\n",
    "    res = add_notebook_menu()\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Impossible to automatically add a menu / table of content to this notebook.\\nYou can download \\\"jyquickhelper\\\" package with: \\n\\\"pip install jyquickhelper\\\"\")\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I) Manipulating vectors instead of class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid2op package has been built with an \"object oriented\" perspective: almost everything is encapsulated in a dedicated `class`. This allows for more customization of the plateform.\n",
    "\n",
    "The downside of this approach is that machine learning method, and especially deep learning, often prefers to deal with vectors rather than with `complex` objects. \n",
    "\n",
    "To have the best of both worlds, we provided a `MLAgent` class that handles the convertion to / from these classes to numpy vector. In this notebook, we will see how this class can be used and overidden to develop a Agent that learns how to perform action on the grid. By default, this class does nothing, and it's possible (and encouraged) to override the `MLAgent._ml_act` method to build smarter Agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the DoNothing agent are:\n",
      "\tFor chronics located at /home/donnotben/.local/lib/python3.6/site-packages/grid2op/data/test_multi_chronics/1\n",
      "\t\t - cumulative reward: 5739.951023\n",
      "\t\t - number of time steps completed: 287 / 287\n"
     ]
    }
   ],
   "source": [
    "# import the usefull class\n",
    "import numpy as np\n",
    "\n",
    "from grid2op.Runner import Runner\n",
    "from grid2op.ChronicsHandler import Multifolder, GridStateFromFileWithForecasts\n",
    "from grid2op.Agent import MLAgent\n",
    "from grid2op.Reward import L2RPNReward\n",
    "from grid2op.Action import PowerLineSet\n",
    "# make a runner\n",
    "runner = Runner(init_grid_path=grid2op.CASE_14_FILE,\n",
    "               path_chron=grid2op.CHRONICS_MLUTIEPISODE,\n",
    "               gridStateclass=Multifolder,\n",
    "               gridStateclass_kwargs={\"gridvalueClass\": GridStateFromFileWithForecasts},\n",
    "               names_chronics_to_backend = grid2op.NAMES_CHRONICS_TO_BACKEND,\n",
    "                agentClass=MLAgent,\n",
    "               rewardClass=L2RPNReward,\n",
    "               actionClass=PowerLineSet)\n",
    "# initialize it\n",
    "res = runner.run(nb_episode=1)\n",
    "print(\"The results for the DoNothing agent are:\")\n",
    "for chron_name, cum_reward, nb_time_step, max_ts in res:\n",
    "    msg_tmp = \"\\tFor chronics located at {}\\n\".format(chron_name)\n",
    "    msg_tmp += \"\\t\\t - cumulative reward: {:.6f}\\n\".format(cum_reward)\n",
    "    msg_tmp += \"\\t\\t - number of time steps completed: {:.0f} / {:.0f}\".format(nb_time_step, max_ts)\n",
    "    print(msg_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that is it. It is as simple as changing the \"agentClass\". Now we will provide an example on how this class can be overidden to train a \"real\" Agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II) Training an Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial, we will expose to built a Q-learning Agent. Most of the code (except the neural network architecture) are inspired from this blog post: [https://towardsdatascience.com/deep-reinforcement-learning-tutorial-with-open-ai-gym-c0de4471f368](https://towardsdatascience.com/deep-reinforcement-learning-tutorial-with-open-ai-gym-c0de4471f368).\n",
    "\n",
    "**Requirements** This notebook require to have `keras` and `gym` installed on your machine.\n",
    "\n",
    "The Agent we will train will:\n",
    "* only act on the topology\n",
    "* only try to set the status of the powerline (he won't try to modify the \"change\" them, nor try to perform any kind of bus splitting merging)\n",
    "\n",
    "This is achieved by passing the \"*PowerLineSet*\" class in the actionClass of the Runner. For more information, one can consult the [PowerLineSet](../documentation/html/action.html#grid2op.Action.PowerLineSet) help page (if built locally, which is recommended) or the [PowerLineSet](../grid2op/Action.py) class definition in the [Action.py](../grid2op/Action.py) file.\n",
    "\n",
    "First we define a \"replay buffer\" necessary to train the Agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Constructs a buffer object that stores the past moves\n",
    "    and samples a set of subsamples\"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.count = 0\n",
    "        self.buffer = deque()\n",
    "\n",
    "    def add(self, s, a, r, d, s2):\n",
    "        \"\"\"Add an experience to the buffer\"\"\"\n",
    "        # S represents current state, a is action,\n",
    "        # r is reward, d is whether it is the end, \n",
    "        # and s2 is next state\n",
    "        experience = (s, a, r, d, s2)\n",
    "        if self.count < self.buffer_size:\n",
    "            self.buffer.append(experience)\n",
    "            self.count += 1\n",
    "        else:\n",
    "            self.buffer.popleft()\n",
    "            self.buffer.append(experience)\n",
    "\n",
    "    def size(self):\n",
    "        return self.count\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Samples a total of elements equal to batch_size from buffer\n",
    "        if buffer contains enough elements. Otherwise return all elements\"\"\"\n",
    "\n",
    "        batch = []\n",
    "\n",
    "        if self.count < batch_size:\n",
    "            batch = random.sample(self.buffer, self.count)\n",
    "        else:\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        # Maps each experience in batch in batches of states, actions, rewards\n",
    "        # and new states\n",
    "        s_batch, a_batch, r_batch, d_batch, s2_batch = list(map(np.array, list(zip(*batch))))\n",
    "\n",
    "        return s_batch, a_batch, r_batch, d_batch, s2_batch\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "        self.count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we import the necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/donnotben/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/donnotben/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/donnotben/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/donnotben/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/donnotben/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/donnotben/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.models import load_model, Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.core import Activation, Dropout, Flatten, Dense\n",
    "from keras.layers import Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we re-use the default parameters, note that these can be optimized. Nothing has been changed for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DECAY_RATE = 0.99\n",
    "BUFFER_SIZE = 40000\n",
    "MINIBATCH_SIZE = 64\n",
    "TOT_FRAME = 3000000\n",
    "EPSILON_DECAY = 1000000\n",
    "MIN_OBSERVATION = 100 #5000\n",
    "FINAL_EPSILON = 0.05\n",
    "INITIAL_EPSILON = 0.1\n",
    "TAU = 0.01\n",
    "# Number of frames to \"throw\" into network\n",
    "NUM_FRAMES = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the model is built. Please check:\n",
    "* [https://towardsdatascience.com/deep-reinforcement-learning-tutorial-with-open-ai-gym-c0de4471f368](https://towardsdatascience.com/deep-reinforcement-learning-tutorial-with-open-ai-gym-c0de4471f368)\n",
    "* and [ddqn_space/deep_Q.py](https://github.com/abhinavsagar/Reinforcement-Learning-Tutorial/blob/master/ddqn&#160;space/deep_Q.py)\n",
    "\n",
    "for more information.\n",
    "\n",
    "Note that we replaced all Convolutional layer with Fully connected layers, and we changed the input size and output size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_size_init = runner.env.observation_space.size()\n",
    "topo_vect_size = runner.env.observation_space.dim_topo\n",
    "action_size = runner.env.action_space.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Agent we will train will be able to act one only one powerline, and to either reconnect it, or disconnect it. It has a choice then bewteen 2 * `number of powerlines`.\n",
    "\n",
    "This `action_space` will be represented by a \"one hot\" vector, as followed:\n",
    "* if the first component (index 0) is set to 1, then the Agent does nothing\n",
    "* if the component equal to \"1\" has its index `i` between 1 and the number of powerline, it will consist in reconnecting powerline `i-1`\n",
    "* if the component equal to \"1\" has its index `i` between the number of powerline + 1 and twice \"the number of powerline\", it will consist in disconnecting powerline `i-nb_powerline-1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ACTIONS = 2* action_size + 1\n",
    "obs_to_model = np.full(observation_size_init, fill_value=False, dtype=np.bool)\n",
    "obs_to_model[(-3*action_size-topo_vect_size):(-action_size-topo_vect_size)] = True\n",
    "observation_size = np.sum(obs_to_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grid2op.Agent import MLAgent\n",
    "\n",
    "class DeepQ(object):\n",
    "    \"\"\"Constructs the desired deep q learning network\"\"\"\n",
    "    def __init__(self, action_size, lr=0.00001):\n",
    "        self.action_size = action_size\n",
    "        self.model = None\n",
    "        self.target_model = None\n",
    "        self.lr_ = lr\n",
    "        self.construct_q_network()\n",
    "    \n",
    "    def construct_q_network(self):\n",
    "        # Uses the network architecture found in DeepMind paper\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(observation_size*NUM_FRAMES))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Dense(observation_size))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Dense(observation_size))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Dense(2*NUM_ACTIONS))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Dense(NUM_ACTIONS))\n",
    "        self.model.compile(loss='mse', optimizer=Adam(lr=self.lr_))\n",
    "\n",
    "        # Creates a target network as described in DeepMind paper\n",
    "        self.target_model = Sequential()\n",
    "        self.target_model.add(Dense(observation_size*NUM_FRAMES))\n",
    "        self.target_model.add(Activation('relu'))\n",
    "        self.target_model.add(Dense(observation_size))\n",
    "        self.target_model.add(Activation('relu'))\n",
    "        self.target_model.add(Dense(observation_size))\n",
    "        self.target_model.add(Activation('relu'))\n",
    "        self.target_model.add(Dense(2*NUM_ACTIONS))\n",
    "        self.target_model.add(Activation('relu'))\n",
    "        self.target_model.add(Dense(NUM_ACTIONS))\n",
    "        self.target_model.compile(loss='mse', optimizer=Adam(lr=self.lr_))\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "    def predict_movement(self, data, epsilon):\n",
    "        \"\"\"Predict movement of game controler where is epsilon\n",
    "        probability randomly move.\"\"\"\n",
    "        rand_val = np.random.random()\n",
    "        q_actions = self.model.predict(data.reshape(1, observation_size*NUM_FRAMES), batch_size = 1)\n",
    "        \n",
    "        if rand_val < epsilon:\n",
    "            opt_policy = np.random.randint(0, NUM_ACTIONS)\n",
    "        else:\n",
    "            opt_policy = np.argmax(np.abs(q_actions))\n",
    "            \n",
    "\n",
    "        return opt_policy, q_actions[0, opt_policy]\n",
    "\n",
    "    def train(self, s_batch, a_batch, r_batch, d_batch, s2_batch, observation_num):\n",
    "        \"\"\"Trains network to fit given parameters\"\"\"\n",
    "        batch_size = s_batch.shape[0]\n",
    "        targets = np.zeros((batch_size, NUM_ACTIONS))\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            targets[i] = self.model.predict(s_batch[i].reshape(1, observation_size*NUM_FRAMES), batch_size = 1)\n",
    "            fut_action = self.target_model.predict(s2_batch[i].reshape(1, observation_size*NUM_FRAMES), batch_size = 1)\n",
    "#             pdb.set_trace()\n",
    "            targets[i, a_batch[i]] = r_batch[i]\n",
    "            if d_batch[i] == False:\n",
    "                targets[i, a_batch[i]] += DECAY_RATE * np.max(fut_action)\n",
    "        loss = self.model.train_on_batch(s_batch, targets)\n",
    "#         print(\"init loss: {}\".format(loss))\n",
    "#         for i in range(100):\n",
    "#             loss = self.model.train_on_batch(s_batch, targets)\n",
    "#         print(\"init loss after 100 training: {}\".format(loss))\n",
    "#         pdb.set_trace()\n",
    "        # Print the loss every 10 iterations.\n",
    "        if observation_num % 10 == 0:\n",
    "            print(\"We had a loss equal to \", loss)\n",
    "\n",
    "    def save_network(self, path):\n",
    "        # Saves model at specified path as h5 file\n",
    "        self.model.save(path)\n",
    "        print(\"Successfully saved network.\")\n",
    "\n",
    "    def load_network(self, path):\n",
    "        self.model = load_model(path)\n",
    "        print(\"Succesfully loaded network.\")\n",
    "\n",
    "    def target_train(self):\n",
    "        model_weights = self.model.get_weights()\n",
    "        target_model_weights = self.target_model.get_weights()\n",
    "        for i in range(len(model_weights)):\n",
    "            target_model_weights[i] = TAU * model_weights[i] + (1 - TAU) * target_model_weights[i]\n",
    "        self.target_model.set_weights(target_model_weights)\n",
    "        \n",
    "class DuelQ(object):\n",
    "    \"\"\"Constructs the desired deep q learning network\"\"\"\n",
    "    def __init__(self, lr=0.00001):\n",
    "        self.lr_ = lr\n",
    "        self.model = None\n",
    "        self.construct_q_network()\n",
    "\n",
    "    def construct_q_network(self):\n",
    "        # Uses the network architecture found in DeepMind paper\n",
    "        self.model = Sequential()\n",
    "        \n",
    "        input_layer = Input(shape = (observation_size*NUM_FRAMES,))\n",
    "        lay1 = Dense(observation_size*NUM_FRAMES)(input_layer)\n",
    "        lay1 = Activation('relu')(lay1)\n",
    "        \n",
    "        lay2 = Dense(observation_size)(lay1)\n",
    "        lay2 = Activation('relu')(lay2)\n",
    "        \n",
    "        lay3 = Dense(2*NUM_ACTIONS)(lay2)\n",
    "        lay3 = Activation('relu')(lay3)\n",
    "        \n",
    "        fc1 = Dense(NUM_ACTIONS)(lay3)\n",
    "        advantage = Dense(NUM_ACTIONS)(fc1)\n",
    "        fc2 = Dense(NUM_ACTIONS)(lay3)\n",
    "        value = Dense(1)(fc2)\n",
    "        \n",
    "#         policy = merge([advantage, value], mode = lambda x: x[0]-K.mean(x[0])+x[1], output_shape = (NUM_ACTIONS,))\n",
    "        #tmp =  keras.layers.Subtract()([advantage, K.mean(advantage)])\n",
    "#         tmp = advantage - K.mean(advantage)\n",
    "        mn_ = K.mean(advantage)\n",
    "        tmp = keras.layers.subtract([advantage, mn_])\n",
    "        policy = keras.layers.add([tmp, value])\n",
    "#         policy = Dense(NUM_ACTIONS)(merge_layer)\n",
    "\n",
    "        self.model = Model(inputs=[input_layer], outputs=[policy])\n",
    "        self.model.compile(loss='mse', optimizer=Adam(lr=self.lr_))\n",
    "\n",
    "        self.target_model = Model(inputs=[input_layer], outputs=[policy])\n",
    "        self.target_model.compile(loss='mse', optimizer=Adam(lr=self.lr_))\n",
    "        print(\"Successfully constructed networks.\")\n",
    "    \n",
    "    def predict_movement(self, data, epsilon):\n",
    "        \"\"\"Predict movement of game controler where is epsilon\n",
    "        probability randomly move.\"\"\"\n",
    "        q_actions = self.model.predict(data.reshape(1, observation_size*NUM_FRAMES), batch_size = 1)\n",
    "        opt_policy = np.argmax(q_actions)\n",
    "        rand_val = np.random.random()\n",
    "        if rand_val < epsilon:\n",
    "            opt_policy = np.random.randint(0, NUM_ACTIONS)\n",
    "        return opt_policy, q_actions[0, opt_policy]\n",
    "\n",
    "    def train(self, s_batch, a_batch, r_batch, d_batch, s2_batch, observation_num):\n",
    "        \"\"\"Trains network to fit given parameters\"\"\"\n",
    "        batch_size = s_batch.shape[0]\n",
    "        targets = np.zeros((batch_size, NUM_ACTIONS))\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            targets[i] = self.model.predict(s_batch[i].reshape(1, observation_size*NUM_FRAMES), batch_size = 1)\n",
    "            fut_action = self.target_model.predict(s2_batch[i].reshape(1, observation_size*NUM_FRAMES), batch_size = 1)\n",
    "            targets[i, a_batch[i]] = r_batch[i]\n",
    "            if d_batch[i] == False:\n",
    "                targets[i, a_batch[i]] += DECAY_RATE * np.max(fut_action)\n",
    "\n",
    "        loss = self.model.train_on_batch(s_batch, targets)\n",
    "\n",
    "        # Print the loss every 10 iterations.\n",
    "        if observation_num % 10 == 0:\n",
    "            print(\"We had a loss equal to \", loss)\n",
    "\n",
    "    def save_network(self, path):\n",
    "        # Saves model at specified path as h5 file\n",
    "        self.model.save(path)\n",
    "        print(\"Successfully saved network.\")\n",
    "\n",
    "    def load_network(self, path):\n",
    "        self.model.load_weights(path)\n",
    "        self.target_model.load_weights(path)\n",
    "        print(\"Succesfully loaded network.\")\n",
    "\n",
    "    def target_train(self):\n",
    "        model_weights = self.model.get_weights()\n",
    "        self.target_model.set_weights(model_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the \"reference\" article [https://towardsdatascience.com/deep-reinforcement-learning-tutorial-with-open-ai-gym-c0de4471f368](https://towardsdatascience.com/deep-reinforcement-learning-tutorial-with-open-ai-gym-c0de4471f368), the author Abhinav Sagar made a dedicated environment based on SpaceInvader in the gym repository. We proceed here on a similar way, but with a the grid2op environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grid2op.Parameters import Parameters\n",
    "from grid2op.BackendPandaPower import PandaPowerBackend\n",
    "from grid2op.ChronicsHandler import ChronicsHandler, Multifolder, GridStateFromFileWithForecasts\n",
    "from grid2op.Environment import Environment\n",
    "from grid2op.Action import PowerLineSet\n",
    "from grid2op.Reward import L2RPNReward\n",
    "import pdb\n",
    "\n",
    "class Grid2Op(MLAgent):\n",
    "    def __init__(self, action_space, mode=\"DDQN\"):\n",
    "        MLAgent.__init__(self, action_space)\n",
    "        self.action_size = action_space.size()\n",
    "        param = Parameters()\n",
    "        backend = PandaPowerBackend()\n",
    "        data_feeding = ChronicsHandler(chronicsClass=Multifolder,\n",
    "                                       path=grid2op.CHRONICS_MLUTIEPISODE,\n",
    "                                       gridvalueClass=GridStateFromFileWithForecasts)\n",
    "        \n",
    "        self.env = Environment(init_grid_path=grid2op.CASE_14_FILE,\n",
    "                               chronics_handler=data_feeding,\n",
    "                               backend=backend,\n",
    "                               parameters=param,\n",
    "                               names_chronics_to_backend=grid2op.NAMES_CHRONICS_TO_BACKEND,\n",
    "                              actionClass=PowerLineSet,\n",
    "                              rewardClass=L2RPNReward)\n",
    "        self.env.reset()\n",
    "        if not isinstance(action_space, type(self.env.action_space)):\n",
    "            raise RuntimeError(\"Imposssible to build an agent with 2 different action space\")\n",
    "        if not isinstance(self.env.action_space, type(action_space)):\n",
    "            raise RuntimeError(\"Imposssible to build an agent with 2 different action space\")\n",
    "            \n",
    "        self.replay_buffer = ReplayBuffer(BUFFER_SIZE)\n",
    "\n",
    "        self.do_nothing_act = self.env.action_space({})\n",
    "        \n",
    "        self.scale_vect = np.ones(observation_size)\n",
    "        self.scale_vect[:action_size] = 150\n",
    "        \n",
    "        # Construct appropriate network based on flags\n",
    "        if mode == \"DDQN\":\n",
    "            self.deep_q = DeepQ(self.env.action_space.size())\n",
    "        elif mode == \"DQN\":\n",
    "            raise RuntimeError(\"Implementation does not work yet\")\n",
    "            self.deep_q = DuelQ(self.env.action_space)\n",
    "\n",
    "        # A buffer that keeps the last 3 images\n",
    "        self.process_buffer = []\n",
    "        for _ in range(NUM_FRAMES):\n",
    "            # Initialize buffer with the first frames\n",
    "            s1, r1, _, _ = self.env.step(self.do_nothing_act)\n",
    "            s1_vect = s1.to_vect()\n",
    "            self.process_buffer.append(s1_vect[obs_to_model]*self.scale_vect)\n",
    "\n",
    "    def opt_policy_to_action_vect(self, opt_policy):\n",
    "        res = np.zeros(self.action_size)\n",
    "        if opt_policy == 0:\n",
    "            # hard encode \"do nothing\"\n",
    "            pass\n",
    "        elif opt_policy < self.action_space.size():\n",
    "            # reconnect a powerline\n",
    "            res[opt_policy-1] = 1\n",
    "        else:\n",
    "            # disconnect a powerline\n",
    "            res[opt_policy-self.action_space.size()-1] = -1\n",
    "        return res\n",
    "    \n",
    "    def load_network(self, path):\n",
    "        self.deep_q.load_network(path)\n",
    "\n",
    "    def _ml_act(self, observation, reward, done=False):\n",
    "        predict_movement_int, *_ = self.deep_q.predict_movement(observation[obs_to_model], epsilon=0)\n",
    "        predict_movement_vect = self.opt_policy_to_action_vect(predict_movement_int)\n",
    "#         predict_movement = self.deep_q.convert_from_vect(predict_movement_vect)\n",
    "        return predict_movement_vect\n",
    "    \n",
    "    def convert_process_buffer(self):\n",
    "        \"\"\"Converts the list of NUM_FRAMES images in the process buffer\n",
    "        into one training sample\"\"\"\n",
    "        return np.concatenate(self.process_buffer)\n",
    "    \n",
    "    def train(self, num_frames):\n",
    "        observation_num = 0\n",
    "        curr_state = self.convert_process_buffer()\n",
    "        epsilon = INITIAL_EPSILON\n",
    "        alive_frame = 0\n",
    "        total_reward = 0\n",
    "\n",
    "        while observation_num < num_frames:\n",
    "            if observation_num % 1000 == 999:\n",
    "                print((\"Executing loop %d\" %observation_num))\n",
    "\n",
    "            # Slowly decay the learning rate\n",
    "            if epsilon > FINAL_EPSILON:\n",
    "                epsilon -= (INITIAL_EPSILON-FINAL_EPSILON)/EPSILON_DECAY\n",
    "\n",
    "            initial_state = self.convert_process_buffer()\n",
    "            self.process_buffer = []\n",
    "\n",
    "            predict_movement_int, predict_q_value = self.deep_q.predict_movement(curr_state, epsilon)\n",
    "            predict_movement_vect = self.opt_policy_to_action_vect(predict_movement_int)\n",
    "            predict_movement = self.convert_from_vect(predict_movement_vect)\n",
    "            \n",
    "            reward, done = 0, False\n",
    "            for i in range(NUM_FRAMES):\n",
    "                temp_observation_obj, temp_reward, temp_done, _ = self.env.step(predict_movement)\n",
    "                temp_observation = temp_observation_obj.to_vect()\n",
    "                temp_observation = temp_observation[obs_to_model]*self.scale_vect\n",
    "                reward += temp_reward\n",
    "                self.process_buffer.append(temp_observation)\n",
    "                done = done | temp_done\n",
    "\n",
    "#             if observation_num % 10 == 0:\n",
    "#                 print(\"We predicted a q value of \", predict_q_value)\n",
    "\n",
    "            if done:\n",
    "                print(\"Lived with maximum time \", alive_frame)\n",
    "                print(\"Earned a total of reward equal to \", total_reward)\n",
    "                self.env.reset()\n",
    "                alive_frame = 0\n",
    "                total_reward = 0\n",
    "\n",
    "            new_state = self.convert_process_buffer()\n",
    "            self.replay_buffer.add(initial_state, predict_movement_int, reward, done, new_state)\n",
    "            total_reward += reward\n",
    "\n",
    "            if self.replay_buffer.size() > MIN_OBSERVATION:\n",
    "                s_batch, a_batch, r_batch, d_batch, s2_batch = self.replay_buffer.sample(MINIBATCH_SIZE)\n",
    "                self.deep_q.train(s_batch, a_batch, r_batch, d_batch, s2_batch, observation_num)\n",
    "                self.deep_q.target_train()\n",
    "\n",
    "            # Save the network every 100000 iterations\n",
    "            if observation_num % 10000 == 9999 or observation_num == num_frames-1:\n",
    "                print(\"Saving Network\")\n",
    "                self.deep_q.save_network(\"saved.h5\")\n",
    "\n",
    "            alive_frame += 1\n",
    "            observation_num += 1\n",
    "            \n",
    "    def calculate_mean(self, num_episode = 100):\n",
    "        reward_list = []\n",
    "        print(\"Printing scores of each trial\")\n",
    "        for i in range(num_episode):\n",
    "            done = False\n",
    "            tot_award = 0\n",
    "            self.env.reset()\n",
    "            while not done:\n",
    "                state = self.convert_process_buffer()\n",
    "                predict_movement_int = self.deep_q.predict_movement(state, 0.0)[0]\n",
    "                predict_movement_vect = self.opt_policy_to_action_vect(predict_movement_int)\n",
    "                predict_movement = self.convert_from_vect(predict_movement_vect)\n",
    "                \n",
    "                observation, reward, done, _ = self.env.step(predict_movement)\n",
    "                tot_award += reward\n",
    "                self.process_buffer.append(observation)\n",
    "                self.process_buffer = self.process_buffer[1:]\n",
    "            print(tot_award)\n",
    "            reward_list.append(tot_award)\n",
    "        return np.mean(reward_list), np.std(reward_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define the model (agent), and then train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lived with maximum time  92\n",
      "Earned a total of reward equal to  1839.9644994319299\n",
      "We had a loss equal to  10.302423\n",
      "We had a loss equal to  10.060549\n",
      "We had a loss equal to  10.055819\n",
      "We had a loss equal to  9.978113\n",
      "We had a loss equal to  9.79144\n",
      "We had a loss equal to  9.92123\n",
      "We had a loss equal to  9.756811\n",
      "We had a loss equal to  9.896887\n",
      "Lived with maximum time  80\n",
      "Earned a total of reward equal to  1579.975641080914\n",
      "We had a loss equal to  9.734239\n",
      "We had a loss equal to  9.7110405\n",
      "Saving Network\n",
      "Successfully saved network.\n"
     ]
    }
   ],
   "source": [
    "my_agent = Grid2Op(runner.env.action_space, mode=\"DDQN\")\n",
    "my_agent.train(1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, time to test this trained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results for the DoNothing agent are:\n",
      "\tFor chronics located at /home/donnotben/.local/lib/python3.6/site-packages/grid2op/data/test_multi_chronics/1\n",
      "\t\t - cumulative reward: 5739.939937\n",
      "\t\t - number of time steps completed: 287 / 287\n"
     ]
    }
   ],
   "source": [
    "# make a runner\n",
    "runner = Runner(init_grid_path=grid2op.CASE_14_FILE, # this should be the same grid as the one the agent is trained one\n",
    "               path_chron=grid2op.CHRONICS_MLUTIEPISODE,  # chronics can changed of course\n",
    "               gridStateclass=Multifolder, # the class of chronics can changed too\n",
    "               gridStateclass_kwargs={\"gridvalueClass\": GridStateFromFileWithForecasts},  # so this can changed too\n",
    "               names_chronics_to_backend = grid2op.NAMES_CHRONICS_TO_BACKEND,  # this also can changed\n",
    "                agentInstance=my_agent,  # here i pass a trained agent, no need to read it from the \n",
    "                agentClass=None,  # if i use an instance of Agent, i cannot provide a class\n",
    "               rewardClass=L2RPNReward,  # this can be anything, not necessarily the same for training\n",
    "               actionClass=PowerLineSet  # this should be the same as the one used for training.\n",
    "               )\n",
    "# initialize it\n",
    "res = runner.run(nb_episode=1)\n",
    "print(\"The results for the DoNothing agent are:\")\n",
    "for chron_name, cum_reward, nb_time_step, max_ts in res:\n",
    "    msg_tmp = \"\\tFor chronics located at {}\\n\".format(chron_name)\n",
    "    msg_tmp += \"\\t\\t - cumulative reward: {:.6f}\\n\".format(cum_reward)\n",
    "    msg_tmp += \"\\t\\t - number of time steps completed: {:.0f} / {:.0f}\".format(nb_time_step, max_ts)\n",
    "    print(msg_tmp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
